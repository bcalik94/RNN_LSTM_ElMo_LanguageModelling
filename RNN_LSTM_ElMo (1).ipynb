{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Recurrent models\n",
    "\n",
    "This lab is supposed to give you some initial practice with neural models in NLP.\n",
    "\n",
    "**This is the complete Lab 4, in two parts.** The purpose of the first part of the lab is to get you started with using neural models. The second part of the lab contains exercises on ELMo embeddings, applying them to the task of word sense disambuiguation following the approach from the original paper by Peters et al.\n",
    "\n",
    "\n",
    "## Part 1 (50 points)\n",
    "\n",
    "In the first part of lab 4, we will play with training a recurrent model for part of speech tagging. As an easy exercise, you will observe what happens when you plug in pretrained word embeddings into an neural NLP model and will experiment with different sizes of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: prepare the data (5 points)\n",
    "\n",
    "Linguistic data come in a variety of formats. You already had a chance to play with POS-annotated corpus data in Lab 1.\n",
    "\n",
    "In the first exercise, you will access POS-annotated data in one format (NLTK) and save it on the disk in a text format. Start with the tagged sentences from the Brown corpus, which can be retrieved as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.10 |Anaconda, Inc.| (default, May  7 2020, 23:06:31) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import nltk\n",
    "nltk.corpus.brown.tagged_sents()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57340"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(nltk.corpus.brown.tagged_sents())\n",
    "#randomsents = random.shuffle(nltk.corpus.brown.sents())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now randomize the order of all sentences in the corpus using <code>random.shuffle()</code> function and split it into 50K sentences for training, 5K for validation, and the rest for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here\n",
    "tagged_sents = list(nltk.corpus.brown.tagged_sents())\n",
    "random.shuffle(tagged_sents)\n",
    "training_brown= tagged_sents[:50000]\n",
    "validation_brown=tagged_sents[50000:55000]\n",
    "testing_brown=tagged_sents[55000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for saving your datasets to a text file in the following format:\n",
    "* one sentence per line\n",
    "* tokens separated by spaces\n",
    "* POS tag separated from the token by \"###\", for example <code>said###VBD</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He###PPS had###HVD to###TO write###VB very###QL small###JJ to###TO get###VB it###PPO on###IN the###AT bottom###NN of###IN the###AT scrap###NN of###IN paper###NN .###. \n",
      "but###CC ,###, as###CS he###PPS had###HVD a###AT special###JJ fondness###NN for###IN magic###NN and###CC divination###NN ,###, he###PPS ordered###VBD that###CS books###NNS on###IN these###DTS subjects###NNS should###MD be###BE spared###VBN .###. \n",
      "For###IN large###JJ letters###NNS ,###, e.g.###RB thermoformed###VBN of###IN acrylic###NN or###CC butyrate###NN ,###, there###EX are###BER other###AP techniques###NNS .###. \n",
      "Congress###NP reacted###VBD with###IN a###AT series###NN of###IN measures###NNS modifying###VBG in###IN various###AP ways###NNS what###WDT it###PPS had###HVD granted###VBN in###IN 1875###CD .###. \n",
      "Fixed###VBN monthly###JJ allowances###NNS are###BER reimbursements###NNS for###IN the###AT same###AP purpose###NN except###IN on###IN a###AT non-itemized###JJ basis###NN .###. \n",
      "Education###NN must###MD not###* be###BE limited###VBN to###IN our###PP$ youth###NN but###CC must###MD be###BE a###AT continuing###VBG process###NN through###IN our###PP$ entire###JJ lives###NNS ,###, for###CS it###PPS is###BEZ only###QL through###IN knowledge###NN that###CS we###PPSS ,###, as###CS a###AT nation###NN ,###, can###MD cope###VB with###IN the###AT dangers###NNS that###WPS threaten###VB our###PP$ society###NN .###. \n",
      "You###PPSS might###MD say###VB we###PPSS are###BER in###IN the###AT nation-building###JJ business###NN ''###'' .###. \n",
      "The###AT almost###RB six###CD million###CD persons###NNS without###IN jobs###NNS and###CC the###AT two###CD million###CD working###VBG part-time###RB do###DO not###* consider###VB themselves###PPLS and###CC their###PP$ plight###NN as###CS statistical###JJ .###. \n",
      "Why###WRB ,###, in###IN the###AT first###OD place###NN ,###, call###VB himself###PPL a###AT liberal###JJ if###CS he###PPS is###BEZ against###IN laissez-faire###NN and###CC favors###VBZ an###AT authoritarian###JJ central###JJ government###NN with###IN womb-to-tomb###JJ controls###NNS over###IN everybody###PN ?###. ?###. \n",
      "It###PPS occurred###VBD to###IN me###PPO that###CS you###PPSS might###MD be###BE interested###VBN in###IN some###DTI thoughts###NNS which###WDT I###PPSS expressed###VBD privately###RB in###IN recent###JJ years###NNS ,###, in###IN the###AT hope###NN of###IN clearing###VBG up###RP a###AT certain###JJ confusion###NN in###IN the###AT public###JJ mind###NN about###IN what###WDT foreign###JJ policy###NN is###BEZ all###QL about###IN and###CC what###WDT it###PPS means###VBZ ,###, and###CC of###IN developing###VBG a###AT certain###JJ compassion###NN for###IN those###DTS who###WPS are###BER carrying###VBG such###JJ responsibilities###NNS inside###IN Government###NN-TL .###. \n",
      "It###PPS would###MD seem###VB that###CS the###AT wheel###NN had###HVD turned###VBN full###JJ circle###NN .###. \n",
      "The###AT plight###NN of###IN a###AT small###JJ community###NN library###NN is###BEZ proportionately###QL worse###JJR .###. \n",
      "Factors###NNS involved###VBN in###IN the###AT displacement###NN formula###NN are###BER the###AT bore###NN diameter###NN of###IN the###AT engine's###NN$ cylinders###NNS ,###, the###AT length###NN of###IN the###AT piston###NN stroke###NN ,###, the###AT number###NN of###IN cylinders###NNS in###IN the###AT engine###NN ,###, and###CC a###AT constant###NN .###. \n",
      "With###IN Lizzie###NP in###IN the###AT barn###NN ,###, the###AT screen###NN door###NN unlocked###VBN and###CC Bridget###NP upstairs###RB in###IN her###PP$ attic###NN room###NN ,###, he###PPS would###MD have###HV had###HVN free###JJ and###CC easy###JJ access###NN to###IN the###AT house###NN .###. \n",
      "Part###NN of###IN this###DT headquarters###NN staff###NN ,###, however###RB ,###, are###BER engineering###VBG managers###NNS who###WPS work###VB between###IN divisional###JJ chief###JJS engineers###NNS and###CC headquarters###NN management###NN .###. \n",
      "But###CC now###RB apparently###RB the###AT job###NN of###IN Secretary###NN-TL of###IN-TL Labor###NN-TL requires###VBZ that###CS he###PPS be###BE willing###JJ to###TO risk###VB his###PP$ reputation###NN as###CS a###AT prognosticator###NN of###IN unemployment###NN trends###NNS .###. \n",
      "The###AT villains###NNS of###IN the###AT piece###NN are###BER those###DTS who###WPS deny###VB job###NN opportunities###NNS to###IN these###DTS youngsters###NNS ,###, and###CC Dr.###NN-TL Conant###NP accuses###VBZ employers###NNS and###CC labor###NN unions###NNS alike###RB .###. \n",
      "Ten###CD years###NNS ago###RB they###PPSS blew###VBD up###RP some###DTI of###IN our###PP$ ditches###NNS .###. \n",
      "``###`` It's###PPS+BEZ a###AT kind###NN of###IN agreement###NN in###IN which###WDT each###DT party###NN gives###VBZ something###PN to###IN the###AT other###AP ''###'' ,###, Jack###NP said###VBD .###. \n",
      "I###PPSS shuddered###VBD and###CC backed###VBD out###IN of###IN the###AT room###NN .###. \n",
      "``###`` Also###RB ,###, that###DT Mr.###NP Ferguson###NP was###BEDZ here###RB .###. \n",
      "He###PPS stopped###VBD stone-still###JJ .###. \n",
      "U###NP-HL Thant###NP-HL of###IN-HL Burma###NP-HL \n",
      "At###IN least###AP I###PPSS had###HVD been###BEN unable###JJ to###TO lay###VB hold###NN on###IN the###AT experience###NN of###IN conversion###NN .###. \n",
      "--###-- Topography###NN is###BEZ very###QL important###JJ .###. \n",
      "Sam###NP Spade###NP joins###VBZ forces###NNS with###IN a###AT band###NN of###IN adventurers###NNS in###IN search###NN of###IN a###AT priceless###JJ jeweled###JJ statue###NN of###IN a###AT falcon###NN ;###. ;###. \n",
      "The###AT fear###NN of###IN punishment###NN just###RB didn't###DOD* bother###VB him###PPO .###. \n",
      "One###PN has###HVZ only###RB ,###, for###IN example###NN ,###, to###TO walk###VB through###IN Harlem###NP and###CC ask###VB oneself###PPL two###CD questions###NNS .###. \n",
      "The###AT-HL need###NN-HL of###IN-HL the###AT-HL new###JJ-HL birth###NN-HL \n",
      "He###PPS took###VBD advantage###NN of###IN the###AT antagonism###NN between###IN aggressive###JJ assertiveness###NN and###CC anxiety###NN and###CC found###VBD a###AT relatively###QL rapid###JJ disappearance###NN of###IN anxiety###NN when###WRB the###AT former###AP attitude###NN was###BEDZ established###VBN .###. \n",
      "Bisque###NN fired###VBN to###IN cone###NN 05###CD .###. \n",
      "Leadership###NN is###BEZ lacking###VBG in###IN our###PP$ society###NN because###CS it###PPS has###HVZ no###AT legitimate###JJ place###NN to###TO develop###VB .###. \n",
      "``###`` How###WRB do###DO you###PPO do###DO ''###'' ?###. ?###. \n",
      "you###PPSS may###MD be###BE baptized###VBN ,###, confirmed###VBN ,###, reverent###JJ and###CC worshipful###JJ ;###. ;###. \n",
      "I###PPSS think###VB all###ABN this###DT could###MD apply###VB to###IN Parker###NP just###QL as###QL well###RB ,###, although###CS ,###, because###RB of###IN the###AT nature###NN of###IN music###NN ,###, it###PPS is###BEZ not###* demonstrable###JJ --###-- at###IN least###AP not###* conclusively###RB .###. \n",
      "Also###RB count###VB as###CS an###AT expense###NN a###AT charge###NN for###IN the###AT labor###NN to###TO be###BE contributed###VBN by###IN the###AT family###NN .###. \n",
      "Most###AP members###NNS of###IN the###AT U.S.###NP-TL Senate###NN-TL ,###, because###CS they###PPSS are###BER human###JJ ,###, like###VB to###TO eat###VB as###QL high###RB on###IN the###AT hog###NN as###CS they###PPSS can###MD .###. \n",
      "In###IN fact###NN ,###, however###RB ,###, the###AT problem###NN is###BEZ not###* so###QL simple###JJ .###. \n",
      "I###PPSS have###HV helped###VBN him###PPO .###. \n",
      "Sometimes###RB ,###, although###CS by###IN no###AT means###NNS always###RB ,###, these###DTS are###BER indeed###RB alkaline###JJ .###. \n",
      "In###IN any###DTI event###NN ,###, it###PPS is###BEZ an###AT irreversible###JJ step###NN ,###, and###CC if###CS we###PPSS are###BER at###IN all###ABN honest###JJ with###IN ourselves###PPLS ,###, we###PPSS will###MD know###VB we###PPSS have###HV no###AT other###AP alternative###NN than###CS to###TO live###VB in###IN the###AT world###NN in###IN which###WDT God###NP has###HVZ seen###VBN fit###VBN to###TO place###VB us###PPO .###. \n",
      "Out###IN in###IN the###AT hall###NN ,###, the###AT upstairs###JJ phone###NN shrilled###VBD ,###, and###CC the###AT small###JJ ghost###NN vanished###VBD .###. \n",
      "You###PPSS still###RB have###HV time###NN to###TO drop###VB a###AT few###AP hints###NNS about###IN the###AT gifts###NNS you'd###PPSS+MD appreciate###VB most###RBT ;###. ;###. \n",
      "``###`` This###DT is###BEZ the###AT first###OD time###NN in###IN 100###CD years###NNS that###CS a###AT candidate###NN for###IN the###AT presidency###NN announced###VBD the###AT result###NN of###IN an###AT election###NN in###IN which###WDT he###PPS was###BEDZ defeated###VBN ''###'' ,###, he###PPS said###VBD .###. \n",
      "She###PPS hoped###VBD they###PPSS were###BED well###RB .###. \n",
      "And###CC last###AP week###NN at###IN the###AT Masters###NNS-TL Palmer###NP and###CC Player###NP did###DOD .###. \n",
      "Have###HV you###PPSS checked###VBN the###AT cost###NN of###IN subcontracting###VBG your###PP$ cafeteria###NN operation###NN in###IN order###NN to###TO save###VB administrative###JJ costs###NNS ?###. ?###. \n",
      "If###CS this###DT is###BEZ the###AT case###NN ,###, one###PN would###MD expect###VB that###CS not###* only###RB the###AT various###JJ procedures###NNS just###RB mentioned###VBN which###WDT alter###VB the###AT hypothalamic###JJ balance###NN would###MD influence###VB emotional###JJ state###NN and###CC behavior###NN but###CC that###DT emotion###NN itself###PPL would###MD act###VB likewise###RB .###. \n",
      "The###AT old###JJ man###NN in###IN the###AT baggy###JJ clothes###NNS waited###VBD at###IN the###AT foot###NN of###IN the###AT steps###NNS .###. \n",
      "The###AT direct###JJ mail###NN campaign###NN consisted###VBD of###IN 3###CD intra-state###JJ mailings###NNS of###IN 1680###CD letters###NNS each###DT and###CC 6###CD out-of-state###JJ directed###VBN to###IN electronics###NN ,###, plastics###NNS ,###, pharmaceutical###JJ ,###, and###CC business###NN machine###NN manufacturers###NNS ,###, and###CC to###IN publishers###NNS .###. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def write_posdata(sentences,outfile):\n",
    "    f = open(outfile, \"w\")\n",
    "    for sentence in sentences:\n",
    "        for word, tag in sentence:\n",
    "            f.write(\"\"+word+\"###\"+tag+\" \")\n",
    "        f.write(\"\\n\")\n",
    "    f.close()       \n",
    "\n",
    "write_posdata(training_brown[:50],\"train_brown_50.txt\")  \n",
    "\n",
    "f = open(\"train_brown_50.txt\", \"r\")\n",
    "print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now save your data partitions in different sizes. We will start with small data samples since training on a large dataset may be very slow depending on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_posdata(training_brown,\"train_brown.txt\")\n",
    "write_posdata(testing_brown,\"test_brown.txt\")\n",
    "write_posdata(validation_brown,\"validation_brown.txt\")\n",
    "write_posdata(training_brown[:50],\"train_brown_50.txt\")\n",
    "write_posdata(validation_brown[:50],\"validation_brown_50.txt\")\n",
    "write_posdata(training_brown[:500],\"train_brown_500.txt\")\n",
    "write_posdata(validation_brown[:500],\"validation_brown_500.txt\")\n",
    "write_posdata(training_brown[:5000],\"train_brown_5000.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you have now saved the POS tagged data for model training purposes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: train neural POS tagger models (35 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now play with a neural model. First of all, install <code>allennlp</code>. The LSTM model we will train follows the AllenNLP tutorial https://allennlp.org/tutorials which contains ample explanations of the underlying code. Let us start by loading the model code and data, starting with a tiny sample for demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.10 |Anaconda, Inc.| (default, May  7 2020, 23:06:31) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:00, 9074.65it/s]\n",
      "50it [00:00, 13997.81it/s]\n"
     ]
    }
   ],
   "source": [
    "from lstm_tutorial import *\n",
    "\n",
    "train_dataset_tiny = reader.read(\"train_brown_50.txt\")\n",
    "validation_dataset_tiny = reader.read(\"validation_brown_50.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fist of all we need to initialize the vocabulary and define an embedding (vector) for each token. We set the embedding size at 300, common in realistic applications. By default, the embeddings are initialized randomly and updated during trining (this can be changed but we start with a standard configuration). We also need to specify the <code>HIDDEN_DIM</code> parameter: the dimensionality of the hidden vector representations in the LSTM cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 33681.07it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab_tiny = Vocabulary.from_instances(train_dataset_tiny + validation_dataset_tiny)\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 20\n",
    "\n",
    "token_embedding_tiny = Embedding(num_embeddings=vocab_tiny.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the smallest pretrained word vector model from https://nlp.stanford.edu/projects/glove/, unzip it, and extract the relevant file <code>'glove.6B.300d.txt'</code> in your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:01, 304314.64it/s]\n"
     ]
    }
   ],
   "source": [
    "glove_token_embedding_tiny = Embedding.from_params(vocab=vocab_tiny,\n",
    "                            params=Params({'pretrained_file':'glove.6B.300d.txt',\n",
    "                                           'embedding_dim' : EMBEDDING_DIM}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now from embedding a single word with <code>token_embedding_tiny</code> we can proceed to mapping a word sequence into a sequence of vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings_tiny = BasicTextFieldEmbedder({\"tokens\": token_embedding_tiny})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following initializes parameters of an LSTM model using <code>word_embeddings_tiny</code> input encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
    "\n",
    "model_tiny = LstmTagger(word_embeddings_tiny, lstm, vocab_tiny)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define an LSTM model called <code>glove_model_tiny</code> that uses <code>glove_token_embedding_tiny</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write your code here\n",
    "glove_word_embeddings_tiny = BasicTextFieldEmbedder({\"tokens\": glove_token_embedding_tiny})\n",
    "glove_model_tiny = LstmTagger(glove_word_embeddings_tiny,lstm,vocab_tiny)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the basic model for the tiny dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "basic_trainer_tiny=initialize_trainer(model_tiny,vocab_tiny,train_dataset_tiny,validation_dataset_tiny,batch_size=50)\n",
    "basic_trainer_tiny.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have trained an LSTM POS tagger for the basic model. Now train the <code>glove_model_tiny</code>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "basic_trainer_tiny=initialize_trainer(glove_model_tiny,vocab_tiny,train_dataset_tiny,validation_dataset_tiny,batch_size=50)\n",
    "basic_trainer_tiny.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Explore training parameters (10 points)\n",
    "\n",
    "Create separate models on the basis of bigger datasets: the 500 sentence training and 500 sentence validation and 5000 sentence training and 5000 sentence validation. Using the full training set (50K sentences) is optional (your machine might be too slow). Initialize and train the basic model on 500 sentence training and 500 sentence validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset_500 = reader.read(\"train_brown_500.txt\")\n",
    "validation_dataset_500 = reader.read(\"validation_brown_500.txt\")\n",
    "vocab_500 = Vocabulary.from_instances(train_dataset_500 + validation_dataset_500)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 20\n",
    "lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
    "token_embedding_500 = Embedding(num_embeddings=vocab_500.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)\n",
    "word_embeddings_500 = BasicTextFieldEmbedder({\"tokens\": token_embedding_500})\n",
    "model_500 = LstmTagger(word_embeddings_500, lstm, vocab_500)\n",
    "basic_trainer_500 = initialize_trainer(model_500, vocab_500, train_dataset_500, validation_dataset_500, batch_size=50)\n",
    "basic_trainer_500.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same training (500 sentence training and 500 sentence validation sets) with GloVE embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset_500 = reader.read(\"train_brown_500.txt\")\n",
    "validation_dataset_500 = reader.read(\"validation_brown_500.txt\")\n",
    "vocab_500 = Vocabulary.from_instances(train_dataset_500 + validation_dataset_500)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 20\n",
    "lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
    "\n",
    "glove_token_embedding_500 = Embedding.from_params(vocab=vocab_500,\n",
    "                            params=Params({'pretrained_file':'glove.6B.300d.txt',\n",
    "                                           'embedding_dim' : EMBEDDING_DIM}))\n",
    "\n",
    "glove_word_embeddings_500 = BasicTextFieldEmbedder({\"tokens\": glove_token_embedding_500})\n",
    "glove_model_500 = LstmTagger(glove_word_embeddings_500, lstm, vocab_500)\n",
    "glove_trainer_500 = initialize_trainer(glove_model_500, vocab_500, train_dataset_500, validation_dataset_500, batch_size=50)\n",
    "glove_trainer_500.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a bigger training set now with 5K sentence training and 5K sentence validation sets and random initial embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset_5k = reader.read(\"train_brown_5000.txt\")\n",
    "validation_dataset_5k = reader.read(\"validation_brown.txt\")\n",
    "vocab_5k = Vocabulary.from_instances(train_dataset_5k + validation_dataset_5k)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 20\n",
    "lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
    "token_embedding_5k = Embedding(num_embeddings=vocab_5k.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)\n",
    "word_embeddings_5k = BasicTextFieldEmbedder({\"tokens\": token_embedding_5k})\n",
    "model_5k = LstmTagger(word_embeddings_5k, lstm, vocab_5k)\n",
    "basic_trainer_5k = initialize_trainer(model_5k, vocab_5k, train_dataset_5k, validation_dataset_5k, batch_size=50)\n",
    "basic_trainer_5k.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same training (5K sentence training and 5K sentence validation sets) with GloVE embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset_5k = reader.read(\"train_brown_5000.txt\")\n",
    "validation_dataset_5k = reader.read(\"validation_brown.txt\")\n",
    "vocab_5k = Vocabulary.from_instances(train_dataset_5k + validation_dataset_5k)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 20\n",
    "lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
    "\n",
    "glove_token_embedding_5k = Embedding.from_params(vocab=vocab_5k,\n",
    "                            params=Params({'pretrained_file':'glove.6B.300d.txt',\n",
    "                                           'embedding_dim' : EMBEDDING_DIM}))\n",
    "\n",
    "glove_word_embeddings_5k = BasicTextFieldEmbedder({\"tokens\": glove_token_embedding_5k})\n",
    "glove_model_5k = LstmTagger(glove_word_embeddings_5k, lstm, vocab_5k)\n",
    "glove_trainer_5k = initialize_trainer(glove_model_5k, vocab_5k, train_dataset_5k, validation_dataset_5k, batch_size=50)\n",
    "glove_trainer_5k.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each trained model, record validation accuracy and training duration (they are returned along with other training stats after training a model) and accuracy on the training set. Fill in the numbers in the table below:\n",
    "\n",
    "| model | validation accuracy | training accuracy | training duration|\n",
    "|-------|---------------------|---------------|-------------------------------------------\n",
    "| basic model on 50 sentences|0.38638454461821525|0.41870350690754515|0:01:28.96586|    \n",
    "| glove model on 50 sentences|0.5087396504139834|0.6801275239107333|0:01:31.767727|\n",
    "| basic model on 500 sentences|0.7383440514469454|0.9234220135628586|0:07:46.176068|\n",
    "| glove model on 500 sentences|0.7901929260450161|0.9209181011997913|0:08:14.402893|\n",
    "| basic model on 5000 sentences|0.8557469889128181|0.856524686926697|0:28:55.007167|\n",
    "| glove model on 5000 sentences|0.871809444045625|0.9148496445064127|0:35:22.978479|\n",
    "\n",
    "**Question.** What do you conclude from these comparisons? when can it be especially beneficial to initialize a model with pretrained embeddings?\n",
    "\n",
    "**Answer.** \n",
    "\n",
    "Pretrained word embeddings capture the semantic and syntactic meaning of a word as they are trained on large datasets. They are capable of boosting the performance of LSTM model, accordingly we can see from the accuracy results that the words trained with GLoVe Embeddings stands with higher accuracy results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, data is processed in batches so that the model performs computation for multiple examples simultaneously. How does batching affect model training? Modify the training to have smaller batches of data - let's use batches of 5 or 500 instead of 50. How does this affect the results? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define your trainers with alternative batching here: batches of 5, 50 sentences\n",
    "\n",
    "train_dataset_50 = reader.read(\"train_brown_50.txt\")\n",
    "validation_dataset_50 = reader.read(\"validation_brown_50.txt\")\n",
    "vocab_50 = Vocabulary.from_instances(train_dataset_50 + validation_dataset_50)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 20\n",
    "lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
    "token_embedding_50 = Embedding(num_embeddings=vocab_50.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)\n",
    "word_embeddings_50 = BasicTextFieldEmbedder({\"tokens\": token_embedding_50})\n",
    "model_50 = LstmTagger(word_embeddings_50, lstm, vocab_50)\n",
    "basic_trainer_50_b5 = initialize_trainer(model_50, vocab_50, train_dataset_50, validation_dataset_50, batch_size=5)\n",
    "basic_trainer_50_b5.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# batches of 5, 500 sentences\n",
    "\n",
    "train_dataset_500 = reader.read(\"train_brown_500.txt\")\n",
    "validation_dataset_500 = reader.read(\"validation_brown_500.txt\")\n",
    "vocab_500 = Vocabulary.from_instances(train_dataset_500 + validation_dataset_500)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 20\n",
    "lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
    "token_embedding_500 = Embedding(num_embeddings=vocab_500.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)\n",
    "word_embeddings_500 = BasicTextFieldEmbedder({\"tokens\": token_embedding_500})\n",
    "model_500 = LstmTagger(word_embeddings_500, lstm, vocab_500)\n",
    "basic_trainer_500_b5 = initialize_trainer(model_500, vocab_500, train_dataset_500, validation_dataset_500, batch_size=5)\n",
    "basic_trainer_500_b5.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#batches of 500, 50 sentences\n",
    "\n",
    "train_dataset_50 = reader.read(\"train_brown_50.txt\")\n",
    "validation_dataset_50 = reader.read(\"validation_brown_50.txt\")\n",
    "vocab_50 = Vocabulary.from_instances(train_dataset_50 + validation_dataset_50)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 20\n",
    "lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
    "token_embedding_50 = Embedding(num_embeddings=vocab_50.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)\n",
    "word_embeddings_50 = BasicTextFieldEmbedder({\"tokens\": token_embedding_50})\n",
    "model_50 = LstmTagger(word_embeddings_50, lstm, vocab_50)\n",
    "basic_trainer_50_b500 = initialize_trainer(model_50, vocab_50, train_dataset_50, validation_dataset_50, batch_size=500)\n",
    "basic_trainer_50_b500.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#batches of 500, 500 sentences\n",
    "\n",
    "train_dataset_500 = reader.read(\"train_brown_500.txt\")\n",
    "validation_dataset_500 = reader.read(\"validation_brown_500.txt\")\n",
    "vocab_500 = Vocabulary.from_instances(train_dataset_500 + validation_dataset_500)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 20\n",
    "lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
    "token_embedding_500 = Embedding(num_embeddings=vocab_500.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)\n",
    "word_embeddings_500 = BasicTextFieldEmbedder({\"tokens\": token_embedding_500})\n",
    "model_500 = LstmTagger(word_embeddings_500, lstm, vocab_500)\n",
    "basic_trainer_500_b500 = initialize_trainer(model_500, vocab_500, train_dataset_500, validation_dataset_500, batch_size=500)\n",
    "basic_trainer_500_b500.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report your results below:\n",
    "\n",
    "**batches of 5**:\n",
    "\n",
    "| model | validation accuracy | training accuracy | training duration|\n",
    "|-------|---------------------|---------------|-------------------------------------------\n",
    "| basic model on 50 sentences|0.5915363385464582|0.926673751328374|0:00:56.758152|\n",
    "| basic model on 500 sentences|0.7405546623794212|0.9348982785602504|'0:03:01.415810|\n",
    "\n",
    "**batches of 500**:\n",
    "\n",
    "| model | validation accuracy | training accuracy | training duration|\n",
    "|-------|---------------------|---------------|-------------------------------------------\n",
    "| basic model on 50 sentences|0.40754369825206993|0.4357066950053135|0:01:39.887773|\n",
    "| basic model on 500 sentences|0.37932073954983925|0.38935837245696403|0:13:36.935827|\n",
    "\n",
    "**Question.** What do these results tell you?\n",
    "**Answer.** WRITE YOUR ANSWER HERE\n",
    "\n",
    "\n",
    "After each batch size has been passed into the model the networks parameters are updated.\n",
    "\n",
    "The batch size is set to a value of 5 and the network weights are updated after each 5 training example.This can have the effect of faster learning, but also adds instability to the learning process as the weights widely vary with each 5 batches.\n",
    "\n",
    "\n",
    "Another solution is to make all predictions at once in a batch (500 sentences with batches = 500).We adapted the model for batch forecasting by predicting with a batch size equal to the training batch size.This would mean that we could be very limited in the way the model is used. Therefore, too large of a batch size will lead to poor generalization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment \n",
    "In this lab we used pretrained GloVe embeddings in a model for part of speech tagging. GloVe in its turn is also a neural word embedding model, but it had been trained on a completely different objective. GloVe vectors had been optimised on word cooccurrence matrix decomposition, i.e. on the task of predicting which words tend to occur with which other words. Part of speech certainly plays a role in determining statistical cooccurrence of words, but this role is indirect, and explicit part of speech information has not been used in training GloVe.\n",
    "\n",
    "This makes our application an example of **transfer learning**, whereby a learned model trained on one objective (e.g. word cooccurrence) can benefit a different application (e.g. POS tagging), because some information is shared between them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - ELMo vectors (50 points)\n",
    "\n",
    "In the second part of this lab we will reproduce the word sense disambiguation strategy that the authors of the ELMo vectors explored. The strategy consists in the following:\n",
    "\n",
    "- create ELMo embeddings for all tokens in a sense-annotated corpus\n",
    "- calculate mean sense vectors for each word sense in the training partition of the corpus\n",
    "- for each sense-annotated token in the test partition of the corpus, assign it to the sense of the word to which its ELMo vector is the closest according to the cosine distance metric\n",
    "- as a backup strategy, use the 1st sense of the word by default.\n",
    "\n",
    "As a sense annotated corpus, we can use SemCor, conveniently available within NLTK. <code>semcor.sents()</code> iterates over all sentences represented as lists of tokens, while <code>semcor.tagged_sents()</code> iterates over the same sentences with additional annotation including WordNet lemma identifiers (lemmas in WordNet stand for a word taken in a specific sense)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['The'], Tree(Lemma('group.n.01.group'), [Tree('NE', ['Fulton', 'County', 'Grand', 'Jury'])]), Tree(Lemma('state.v.01.say'), ['said']), Tree(Lemma('friday.n.01.Friday'), ['Friday']), ['an'], Tree(Lemma('probe.n.01.investigation'), ['investigation']), ['of'], Tree(Lemma('atlanta.n.01.Atlanta'), ['Atlanta']), [\"'s\"], Tree(Lemma('late.s.03.recent'), ['recent']), Tree(Lemma('primary.n.01.primary_election'), ['primary', 'election']), Tree(Lemma('produce.v.04.produce'), ['produced']), ['``'], ['no'], Tree(Lemma('evidence.n.01.evidence'), ['evidence']), [\"''\"], ['that'], ['any'], Tree(Lemma('abnormality.n.04.irregularity'), ['irregularities']), Tree(Lemma('happen.v.01.take_place'), ['took', 'place']), ['.']], [['The'], Tree(Lemma('jury.n.01.jury'), ['jury']), Tree(Lemma('far.r.02.far'), ['further']), Tree(Lemma('state.v.01.say'), ['said']), ['in'], Tree(Lemma('term.n.02.term'), ['term']), Tree(Lemma('end.n.02.end'), ['end']), Tree(Lemma('presentment.n.01.presentment'), ['presentments']), ['that'], ['the'], Tree(Lemma('group.n.01.group'), [Tree('NE', ['City', 'Executive', 'Committee'])]), [','], ['which'], Tree(Lemma('own.v.01.have'), ['had']), Tree(Lemma('overall.s.02.overall'), ['over-all']), Tree(Lemma('mission.n.03.charge'), ['charge']), ['of'], ['the'], Tree(Lemma('election.n.01.election'), ['election']), [','], ['``'], Tree(Lemma('deserve.v.01.deserve'), ['deserves']), ['the'], Tree(Lemma('praise.n.01.praise'), ['praise']), ['and'], Tree(Lemma('thanks.n.01.thanks'), ['thanks']), ['of'], ['the'], Tree(Lemma('location.n.01.location'), [Tree('NE', ['City', 'of', 'Atlanta'])]), [\"''\"], ['for'], ['the'], Tree(Lemma('manner.n.01.manner'), ['manner']), ['in'], ['which'], ['the'], Tree(Lemma('election.n.01.election'), ['election']), ['was'], Tree(Lemma('conduct.v.01.conduct'), ['conducted']), ['.']], ...]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import semcor\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tree import Tree\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict\n",
    "import numpy\n",
    "import torch\n",
    "\n",
    "semcor.sents()\n",
    "semcor.tagged_sents(tag=\"sem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1. Extract relevant data from SemCor (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, split all the sentences in SemCor randomly into 90% training and 10% testing partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semcor_tagged_sents: 37176\n",
      "semcor_train: 33458\n",
      "semcor_test: 3718\n"
     ]
    }
   ],
   "source": [
    "semcor_tagged_sents = list(nltk.corpus.semcor.tagged_sents(tag=\"sem\"))\n",
    "random.shuffle(semcor_tagged_sents)\n",
    "\n",
    "semcor_train= semcor_tagged_sents[:int(0.9 * len(semcor_tagged_sents))]\n",
    "semcor_test= semcor_tagged_sents[int(0.9 * len(semcor_tagged_sents)):]\n",
    "\n",
    "print(\"semcor_tagged_sents:\", len(semcor_tagged_sents))\n",
    "print(\"semcor_train:\", len(semcor_train))\n",
    "print(\"semcor_test:\", len(semcor_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that takes as input a sentence from SemCor and extracts a list which contains, for each token of the sentence, either the corresponding WordNet Lemma (e.g. <code>Lemma('friday.n.01.Friday')</code>) or <code>None</code>. <code>None</code> corresponds to tokens that are either 1) not annotated for word senses (e.g. articles); 2) are marked up as (part of) a named entity (e.g. \"City of Atlanta\" or placename \"Fulton\" annotated as  <code>Tree(Lemma('location.n.01.location'), [Tree('NE', ['Fulton'])])</code>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmas(input_tagged_sentence): \n",
    "    temp_list = []\n",
    "    \n",
    "    # check for Lemma's or None's\n",
    "    for token in range(0, len(input_tagged_sentence)):\n",
    "        if (type(input_tagged_sentence[token]) != nltk.tree.Tree):\n",
    "            # print(1, \"-\", input_tagged_sentence[token])\n",
    "            temp_list.append(\"None\")\n",
    "        elif type(input_tagged_sentence[token][0]) == str:\n",
    "            # print(2, \"-\", input_tagged_sentence[token])\n",
    "            temp_list.append(input_tagged_sentence[token])    \n",
    "        elif input_tagged_sentence[token][0].pos()[0][1] == 'NE':\n",
    "            # print(3, \"-\", input_tagged_sentence[token])\n",
    "            temp_list.append(\"None\")   \n",
    "        else:\n",
    "            print(\"Error at index:\\n\", token, \"\\n\")\n",
    "            \n",
    "    return temp_list   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['None', 'None', Tree(Lemma('state.v.01.say'), ['said']), Tree(Lemma('friday.n.01.Friday'), ['Friday']), 'None', Tree(Lemma('probe.n.01.investigation'), ['investigation']), 'None', Tree(Lemma('atlanta.n.01.Atlanta'), ['Atlanta']), 'None', Tree(Lemma('late.s.03.recent'), ['recent']), Tree(Lemma('primary.n.01.primary_election'), ['primary', 'election']), Tree(Lemma('produce.v.04.produce'), ['produced']), 'None', 'None', Tree(Lemma('evidence.n.01.evidence'), ['evidence']), 'None', 'None', 'None', Tree(Lemma('abnormality.n.04.irregularity'), ['irregularities']), Tree(Lemma('happen.v.01.take_place'), ['took', 'place']), 'None']\n"
     ]
    }
   ],
   "source": [
    "# test function \n",
    "semcor_tagged_sents = semcor.tagged_sents(tag=\"sem\")\n",
    "print(get_lemmas(semcor_tagged_sents[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now able to extract word senses (instantiated by WordNet lemmas) from the corpus. The next step is to associate senses with ELMo vectors. Create a dictionary of contextualized token embeddings from the training corpus grouped by the WordNet sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_embeddings=defaultdict(list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create contextualized ELMo word embeddings for the tokens in this corpus. We can load the pretrained ELMo model and define a function <code>sentences_to_elmo()</code> that receives a list of tokenized sentences as input and produces their ELMo vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "\n",
    "options_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "weight_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "elmo = Elmo(options_file, weight_file, 1, dropout=0)\n",
    "\n",
    "def sentences_to_elmo(sentences): \n",
    "    character_ids = batch_to_ids(sentences)\n",
    "    embeddings = elmo(character_ids)\n",
    "    return embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can process the corpus sentences and produce their ELMo vectors. It is recommended to pass the input to ELMo encoder in batches. A suggested batch size is 50 sentences. For example, the code below processes the first 50 sentences from the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=semcor.sents()[:50]\n",
    "\n",
    "embeddings=sentences_to_elmo(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>embeddings</code> that we obtained is a dictionary that contains a list of ELMo embeddings and a list of masks. The mask tells us which embeddings correspond to tokens in the original input sentences and which correspond to the padding (introduced to give all sentences in the batch the same length).\n",
    "In principle all embeddings are stored in PyTorch tensors so that they can be used in bigger neural models, but we are not going to do it now. For our purposes, PyTorch tensors can be converted to numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-6.46188855e-03,  6.02140278e-03, -3.55983436e-01, ...,\n",
       "         -1.17147304e-02,  7.04263002e-02, -4.18728709e-01],\n",
       "        [-3.77808213e-01,  2.81414628e-01, -2.58361459e-01, ...,\n",
       "         -4.85472798e-01,  2.55084008e-01,  3.63812745e-02],\n",
       "        [ 9.11907077e-01,  1.17794800e+00, -8.48333716e-01, ...,\n",
       "          9.84723091e-01,  3.36747169e-01,  1.61717817e-01],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "       [[-6.46188855e-03,  6.02140278e-03, -3.55983436e-01, ...,\n",
       "         -4.48764861e-02,  1.13127291e-01, -9.96282995e-02],\n",
       "        [ 1.37208998e-01, -2.00027555e-01, -1.30738422e-01, ...,\n",
       "          5.94822645e-01,  9.33864832e-01, -2.67571092e-01],\n",
       "        [ 1.72800213e-01,  1.08008480e+00, -5.45395315e-01, ...,\n",
       "          3.19658905e-01, -5.64084053e-01,  3.24607313e-01],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "       [[-6.46188855e-03,  6.02140278e-03, -3.55983436e-01, ...,\n",
       "         -3.62002775e-02, -9.36144516e-02, -3.22369874e-01],\n",
       "        [ 6.31842673e-01,  1.43846914e-01,  3.53618234e-01, ...,\n",
       "         -2.50883490e-01,  8.24538693e-02, -1.12004310e-01],\n",
       "        [ 4.68477368e-01,  4.06336814e-01,  1.03118807e-01, ...,\n",
       "         -2.13102400e-02,  1.60498604e-01,  2.13640824e-01],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 6.91312402e-02,  2.75958836e-01,  3.09307903e-01, ...,\n",
       "         -3.17321092e-01, -8.14944386e-01,  1.41807050e-01],\n",
       "        [-2.66639665e-02, -6.02102518e-01,  4.08914268e-01, ...,\n",
       "         -2.87962496e-01, -5.98325193e-01,  2.63062596e-01],\n",
       "        [-4.58681583e-01, -2.99257517e-01,  3.24779928e-01, ...,\n",
       "         -7.64576346e-02,  8.64553601e-02,  8.22831511e-01],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "       [[-1.29072413e-01, -5.11559784e-01,  3.25434566e-01, ...,\n",
       "         -2.02451468e-01,  5.12931496e-03, -3.78726184e-01],\n",
       "        [-8.06154370e-01, -3.47683430e-01,  1.56529605e-01, ...,\n",
       "         -4.41656053e-01,  2.10902572e-01, -7.48497918e-02],\n",
       "        [-5.46395779e-04, -3.70894223e-01,  4.03376341e-01, ...,\n",
       "          3.76692504e-01, -3.58357608e-01,  3.45836669e-01],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "       [[-1.34372234e+00, -2.05630019e-01,  1.07977167e-01, ...,\n",
       "         -3.15490425e-01,  7.20490962e-02, -4.48259294e-01],\n",
       "        [ 3.22957486e-02,  3.81020337e-01,  1.89836651e-01, ...,\n",
       "          5.85263252e-01,  2.99542516e-01, -1.47112608e-01],\n",
       "        [-5.09661555e-01, -7.00836256e-02,  1.13597736e-01, ...,\n",
       "         -1.85098737e-01,  5.74855030e-01,  3.52964923e-02],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['elmo_representations'][0].detach().numpy() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the size of the embeddings we got. It has three dimensions: 1) the number of sentences 2) the number of tokens (corresponds to the tokens in the longest original sentence of the batch; shorter ones were padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 59, 1024])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['elmo_representations'][0].detach().size() # 3D: num sents, num tokens, masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing contained in the <code>embeddings</code> is the mask, a tensor encoding which tokens vectors correspond to original tokens and which are paddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['mask'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2. Extract ELMo encoding of sentences using a mask (5 points)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a function <code>get_masked_vectors(embeddings)</code> that takes embeddings as input and returns a list of ELMo sentence encodings to which the mask has been applied, i.e. where the padding vectors have been removed so the representation of each sentence contains as many vectors as there were tokens in the original sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_vectors(embeddings): # dict of embeddings \n",
    "    temp_list_sentences = [] \n",
    "    \n",
    "    for sentence in range(0, (len(embeddings['elmo_representations'][0]))):\n",
    "        temp_list_tokens = [] \n",
    "            \n",
    "        for token in range(0, (len(embeddings['elmo_representations'][0][0]))):\n",
    "            \n",
    "            # check if the mask is false at the corresponding location\n",
    "            if embeddings['mask'].data[sentence][token] == True:\n",
    "                \n",
    "                # add the token with encoding to the adjusted sentence\n",
    "                temp_list_tokens.append(embeddings['elmo_representations'][0][sentence][token])\n",
    "                \n",
    "        temp_list_sentences.append(temp_list_tokens)\n",
    "        \n",
    "    return temp_list_sentences # list of ELMo sentence encodings, where paddings are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len sent 1: 26\n",
      "len sent 2: 44\n",
      "len sent 3: 36\n",
      "len sent 4: 37\n",
      "len sent 5: 25\n",
      "len sent 6: 24\n",
      "len sent 7: 43\n",
      "len sent 8: 26\n",
      "len sent 9: 25\n",
      "len sent 10: 14\n",
      "len sent 11: 15\n",
      "len sent 12: 28\n",
      "len sent 13: 25\n",
      "len sent 14: 59\n",
      "len sent 15: 23\n",
      "len sent 16: 25\n",
      "len sent 17: 17\n",
      "len sent 18: 35\n",
      "len sent 19: 33\n",
      "len sent 20: 34\n",
      "len sent 21: 35\n",
      "len sent 22: 33\n",
      "len sent 23: 9\n",
      "len sent 24: 31\n",
      "len sent 25: 28\n",
      "len sent 26: 35\n",
      "len sent 27: 22\n",
      "len sent 28: 6\n",
      "len sent 29: 9\n",
      "len sent 30: 20\n",
      "len sent 31: 15\n",
      "len sent 32: 17\n",
      "len sent 33: 17\n",
      "len sent 34: 20\n",
      "len sent 35: 11\n",
      "len sent 36: 14\n",
      "len sent 37: 17\n",
      "len sent 38: 14\n",
      "len sent 39: 11\n",
      "len sent 40: 30\n",
      "len sent 41: 23\n",
      "len sent 42: 23\n",
      "len sent 43: 37\n",
      "len sent 44: 34\n",
      "len sent 45: 20\n",
      "len sent 46: 28\n",
      "len sent 47: 31\n",
      "len sent 48: 52\n",
      "len sent 49: 22\n",
      "len sent 50: 10\n"
     ]
    }
   ],
   "source": [
    "# test functions\n",
    "semcor_sents = nltk.corpus.semcor.sents()\n",
    "sentences=semcor.sents()[:50] \n",
    "embeddings=sentences_to_elmo(sentences)\n",
    "embeddings['elmo_representations'][0].detach().numpy()\n",
    "new_embeddings = get_masked_vectors(embeddings)\n",
    "\n",
    "\n",
    "# check if length of the 50 sentences are variable\n",
    "for s in range(0, 50):\n",
    "    print(\"len sent {}: {}\".format(s+1, len(new_embeddings[s])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3. Collect ELMo vectors from the training corpus (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the corpus updating your train word sense vectors. Iterate over the all the train sentences in the corpus, and retrieve for each lemma-annotated token (where lemma is not <code>None</code>) the corresponding ELMo vector. Store the ELMo sense embeddings that correspond to each lemma in the dictionary <code>Train_embeddings</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma_annotated_tokens(sentence): \n",
    "    list_lemmas_nones = get_lemmas(sentence)\n",
    "    list_lemmas = []\n",
    "\n",
    "    # select lemmas and create list of lemma-annotated tokens\n",
    "    for token in range(0, len(list_lemmas_nones)):\n",
    "        if list_lemmas_nones[token] != 'None':\n",
    "            list_lemmas.append(list_lemmas_nones[token])\n",
    "    \n",
    "    return list_lemmas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def store_training_set(training_set):\n",
    "    lemma_annotated_token_sents = []\n",
    "    token_sents = []\n",
    "    \n",
    "    # create list of lists of lemma-annotated tokens\n",
    "    for sentence in range(0, len(training_set)):\n",
    "        print(\"sentence\", sentence+1, ': \\n', training_set[sentence], \"\\n\")\n",
    "        \n",
    "        sent = get_lemma_annotated_tokens(training_set[sentence])\n",
    "        lemma_annotated_token_sents.append(sent)\n",
    "        print(len(sent), \"lemma-annotated tokens :\\n\", sent, \"\\n\\n\")\n",
    "        \n",
    "    # get embeddings for each sentence\n",
    "    for s in range(0, len(lemma_annotated_token_sents)):\n",
    "        senses_of_sent = []\n",
    "        tokens_of_sent = []\n",
    "        idx = 0\n",
    "        \n",
    "        for token in range(0, len(lemma_annotated_token_sents[s])):\n",
    "            senses_of_sent.append(lemma_annotated_token_sents[s][token].label())\n",
    "            tokens_of_sent.append(lemma_annotated_token_sents[s][token][:]) \n",
    "          \n",
    "        # remove paddings     \n",
    "        embeddings_elmo = sentences_to_elmo(tokens_of_sent)\n",
    "        embeddings_elmo['elmo_representations'][0].detach().numpy() \n",
    "        embeddings_without_paddings = get_masked_vectors(embeddings_elmo)\n",
    "\n",
    "        # make tuples of sense and embeddings per token\n",
    "        sense_embeddings = list(zip(senses_of_sent, embeddings_without_paddings))\n",
    "        \n",
    "        # store embeddings in Train_embeddings grouped by WordNet sense\n",
    "        for sense, embeddings in sense_embeddings:  # list of tuples\n",
    "            \n",
    "            # ignore multiword expressions\n",
    "            if len(embeddings) == 1:\n",
    "    \n",
    "                if sense in Train_embeddings:\n",
    "                    current_vector = Train_embeddings.get(sense)\n",
    "    \n",
    "                    # Method1: sum vectors, and count\n",
    "                    # updated_vector = [sum(x) for x in zip(current_vector, embeddings)]\n",
    "                    # Train_embeddings[sense] = updated_vector\n",
    "                    # current_count = Train_counter.get(sense)\n",
    "                    # Train_counter[sense] += 1\n",
    "                    \n",
    "                    # Method 2:\n",
    "                    Train_embeddings[sense].append(sense_embeddings[idx][1])\n",
    "                    idx += 1\n",
    "      \n",
    "                # adding new sense # METHOD 1\n",
    "                else:\n",
    "                    # Method 1: add vectors, and count\n",
    "                    # Train_embeddings.update( {sense : embeddings}) # METHOD 1\n",
    "                    # Train_counter.update( {sense : 1}) # METHOD 1\n",
    "                    \n",
    "                    # Method 2:\n",
    "                    Train_embeddings.update( {sense : sense_embeddings[idx][1]})\n",
    "                    idx += 1\n",
    "            \n",
    "    return Train_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 1 : \n",
      " [['A'], ['new'], ['radial'], ['drill'], ['press'], ['with'], ['a'], ['16'], ['inch'], ['capacity'], Tree(Lemma('have.v.02.have'), ['has']), ['a'], Tree(Lemma('lean.v.01.tilt'), ['tilting']), ['head'], ['that'], Tree(Lemma('let.v.01.allow'), ['allows']), ['drilling'], ['to'], ['be'], Tree(Lemma('make.v.01.do'), ['done']), ['at'], ['any'], ['angle'], ['.']] \n",
      "\n",
      "4 lemma-annotated tokens :\n",
      " [Tree(Lemma('have.v.02.have'), ['has']), Tree(Lemma('lean.v.01.tilt'), ['tilting']), Tree(Lemma('let.v.01.allow'), ['allows']), Tree(Lemma('make.v.01.do'), ['done'])] \n",
      "\n",
      "\n",
      "sentence 2 : \n",
      " [['I'], ['used', 'to'], Tree(Lemma('love.v.01.love'), ['love']), ['this'], Tree(Lemma('country.n.02.country'), ['country']), ['and'], Tree(Lemma('believe.v.03.believe'), ['believe']), ['that'], Tree(Lemma('someday.r.01.someday'), ['someday']), ['we'], [\"'d\"], Tree(Lemma('win.v.01.win'), ['win']), ['our'], Tree(Lemma('struggle.n.01.battle'), ['battle']), ['for'], Tree(Lemma('equality.n.01.equality'), ['equality']), ['.']] \n",
      "\n",
      "7 lemma-annotated tokens :\n",
      " [Tree(Lemma('love.v.01.love'), ['love']), Tree(Lemma('country.n.02.country'), ['country']), Tree(Lemma('believe.v.03.believe'), ['believe']), Tree(Lemma('someday.r.01.someday'), ['someday']), Tree(Lemma('win.v.01.win'), ['win']), Tree(Lemma('struggle.n.01.battle'), ['battle']), Tree(Lemma('equality.n.01.equality'), ['equality'])] \n",
      "\n",
      "\n",
      "sentence 3 : \n",
      " [['``'], ['Thanks'], [\"''\"], [','], ['Owen'], Tree(Lemma('state.v.01.say'), ['said']), [','], ['``'], ['but'], ['Friday'], Tree(Lemma('be.v.01.be'), ['is']), ['a'], ['long'], ['way'], ['off'], ['and'], ['anything'], ['can'], Tree(Lemma('happen.v.01.happen'), ['happen']), [\"''\"], ['.']] \n",
      "\n",
      "3 lemma-annotated tokens :\n",
      " [Tree(Lemma('state.v.01.say'), ['said']), Tree(Lemma('be.v.01.be'), ['is']), Tree(Lemma('happen.v.01.happen'), ['happen'])] \n",
      "\n",
      "\n",
      "sentence 4 : \n",
      " [['One'], Tree(Lemma('wonder.v.02.wonder'), ['wonders']), ['about'], ['its'], Tree(Lemma('applicability.n.01.applicability'), ['applicability']), ['to'], Tree(Lemma('people.n.01.people'), ['people']), ['.']] \n",
      "\n",
      "3 lemma-annotated tokens :\n",
      " [Tree(Lemma('wonder.v.02.wonder'), ['wonders']), Tree(Lemma('applicability.n.01.applicability'), ['applicability']), Tree(Lemma('people.n.01.people'), ['people'])] \n",
      "\n",
      "\n",
      "sentence 5 : \n",
      " [['Richard', 'J.', 'Hughes'], Tree(Lemma('make.v.01.make'), ['made']), ['his'], ['Morris', 'County'], ['debut'], ['in'], ['his'], ['bid'], ['for'], ['the'], ['Democratic'], ['gubernatorial'], ['nomination'], ['here'], ['last'], ['night'], ['with'], ['a'], ['pledge'], ['``'], ['to'], Tree(Lemma('transport.v.02.carry'), ['carry']), ['the'], ['issues'], ['to'], ['every'], ['corner'], ['of'], ['the'], ['state'], [\"''\"], ['.']] \n",
      "\n",
      "2 lemma-annotated tokens :\n",
      " [Tree(Lemma('make.v.01.make'), ['made']), Tree(Lemma('transport.v.02.carry'), ['carry'])] \n",
      "\n",
      "\n",
      "sentence 6 : \n",
      " [['But'], Tree(Lemma('person.n.01.person'), [Tree('NE', ['Duclos'])]), Tree('think.v.1;2', ['thought']), ['he'], Tree(Lemma('visualize.v.01.see'), ['saw']), ['a'], Tree(Lemma('exit.n.01.way_out'), ['way', 'out']), ['.']] \n",
      "\n",
      "3 lemma-annotated tokens :\n",
      " [Tree('think.v.1;2', ['thought']), Tree(Lemma('visualize.v.01.see'), ['saw']), Tree(Lemma('exit.n.01.way_out'), ['way', 'out'])] \n",
      "\n",
      "\n",
      "sentence 7 : \n",
      " [['Now'], ['he'], ['is'], ['apparently'], Tree(Lemma('ask.v.04.expect'), ['expected']), ['to'], Tree(Lemma('spare.v.03.give_up'), ['give', 'up']), ['his'], ['evenings'], ['-'], ['and'], ['Sundays'], [','], ['too'], [','], ['for'], ['this'], ['is'], Tree(Lemma('come.v.03.come'), ['coming']), ['.']] \n",
      "\n",
      "3 lemma-annotated tokens :\n",
      " [Tree(Lemma('ask.v.04.expect'), ['expected']), Tree(Lemma('spare.v.03.give_up'), ['give', 'up']), Tree(Lemma('come.v.03.come'), ['coming'])] \n",
      "\n",
      "\n",
      "sentence 8 : \n",
      " [['The'], ['striptease'], Tree(Lemma('be.v.01.be'), ['is']), ['crass'], [';'], ['the'], ['belly', 'dance'], Tree(Lemma('leave.v.06.leave'), ['leaves']), ['more'], ['to'], ['the'], ['imagination'], ['.']] \n",
      "\n",
      "2 lemma-annotated tokens :\n",
      " [Tree(Lemma('be.v.01.be'), ['is']), Tree(Lemma('leave.v.06.leave'), ['leaves'])] \n",
      "\n",
      "\n",
      "sentence 9 : \n",
      " [['But'], Tree(Lemma('person.n.01.person'), [Tree('NE', ['Michael', 'Sept'])]), ['had'], Tree(Lemma('unmask.v.01.unmask'), ['unmasked']), ['him'], [','], Tree(Lemma('uncover.v.01.reveal'), ['revealing']), ['he'], ['had'], Tree(Lemma('never.r.01.never'), ['never']), Tree(Lemma('be.v.02.be'), ['been']), ['a'], Tree(Lemma('bishop.n.01.bishop'), ['bishop']), [','], ['but'], Tree(Lemma('be.v.02.be'), ['was']), ['an'], Tree(Lemma('anabaptist.n.01.Anabaptist'), ['Anabaptist']), [','], Tree(Lemma('afraid.a.01.afraid'), ['afraid']), ['to'], Tree(Lemma('state.v.01.state'), ['state']), ['his'], Tree(Lemma('religion.n.01.faith'), ['faith']), [','], ['because'], ['he'], Tree(Lemma('know.v.01.know'), ['knew']), Tree(Lemma('person.n.01.person'), [Tree('NE', ['John', 'Calvin'])]), ['had'], Tree(Lemma('publish.v.03.write'), ['written']), ['a'], Tree(Lemma('book.n.01.book'), ['book']), ['against'], ['their'], Tree('belief.n.00', ['belief']), ['that'], ['the'], Tree(Lemma('soul.n.01.soul'), ['soul']), Tree(Lemma('sleep.v.01.sleep'), ['slept']), ['after'], Tree(Lemma('death.n.04.death'), ['death']), ['.']] \n",
      "\n",
      "17 lemma-annotated tokens :\n",
      " [Tree(Lemma('unmask.v.01.unmask'), ['unmasked']), Tree(Lemma('uncover.v.01.reveal'), ['revealing']), Tree(Lemma('never.r.01.never'), ['never']), Tree(Lemma('be.v.02.be'), ['been']), Tree(Lemma('bishop.n.01.bishop'), ['bishop']), Tree(Lemma('be.v.02.be'), ['was']), Tree(Lemma('anabaptist.n.01.Anabaptist'), ['Anabaptist']), Tree(Lemma('afraid.a.01.afraid'), ['afraid']), Tree(Lemma('state.v.01.state'), ['state']), Tree(Lemma('religion.n.01.faith'), ['faith']), Tree(Lemma('know.v.01.know'), ['knew']), Tree(Lemma('publish.v.03.write'), ['written']), Tree(Lemma('book.n.01.book'), ['book']), Tree('belief.n.00', ['belief']), Tree(Lemma('soul.n.01.soul'), ['soul']), Tree(Lemma('sleep.v.01.sleep'), ['slept']), Tree(Lemma('death.n.04.death'), ['death'])] \n",
      "\n",
      "\n",
      "sentence 10 : \n",
      " [['Since'], ['the'], Tree('circulating.s.00', ['circulating']), Tree(Lemma('thyroid_hormone.n.01.thyroid_hormone'), ['thyroid', 'hormones']), Tree(Lemma('be.v.02.be'), ['are']), ['the'], Tree(Lemma('amino_acid.n.01.amino_acid'), ['amino', 'acids']), Tree(Lemma('thyroxine.n.01.thyroxine'), ['thyroxine']), ['and'], Tree(Lemma('tri-iodothyronine.n.01.tri-iodothyronine'), ['tri-iodothyronine']), ['('], ['cf.'], Tree(Lemma('section.n.01.section'), ['Section']), ['C'], [')'], [','], ['it'], Tree(Lemma('be.v.01.be'), ['is']), Tree(Lemma('clear.a.01.clear'), ['clear']), ['that'], ['some'], Tree(Lemma('mechanism.n.03.mechanism'), ['mechanism']), ['must'], Tree(Lemma('exist.v.01.exist'), ['exist']), ['in'], ['the'], Tree(Lemma('thyroid_gland.n.01.thyroid_gland'), ['thyroid', 'gland']), ['for'], ['their'], Tree(Lemma('release.n.03.release'), ['release']), ['from'], Tree(Lemma('protein.n.01.protein'), ['proteins']), ['before'], Tree(Lemma('secretion.n.01.secretion'), ['secretion']), ['.']] \n",
      "\n",
      "15 lemma-annotated tokens :\n",
      " [Tree('circulating.s.00', ['circulating']), Tree(Lemma('thyroid_hormone.n.01.thyroid_hormone'), ['thyroid', 'hormones']), Tree(Lemma('be.v.02.be'), ['are']), Tree(Lemma('amino_acid.n.01.amino_acid'), ['amino', 'acids']), Tree(Lemma('thyroxine.n.01.thyroxine'), ['thyroxine']), Tree(Lemma('tri-iodothyronine.n.01.tri-iodothyronine'), ['tri-iodothyronine']), Tree(Lemma('section.n.01.section'), ['Section']), Tree(Lemma('be.v.01.be'), ['is']), Tree(Lemma('clear.a.01.clear'), ['clear']), Tree(Lemma('mechanism.n.03.mechanism'), ['mechanism']), Tree(Lemma('exist.v.01.exist'), ['exist']), Tree(Lemma('thyroid_gland.n.01.thyroid_gland'), ['thyroid', 'gland']), Tree(Lemma('release.n.03.release'), ['release']), Tree(Lemma('protein.n.01.protein'), ['proteins']), Tree(Lemma('secretion.n.01.secretion'), ['secretion'])] \n",
      "\n",
      "\n",
      "sentence 11 : \n",
      " [['Attention'], ['to'], ['details'], ['can'], Tree(Lemma('reduce.v.01.cut'), ['cut']), ['in'], ['half'], ['the'], ['size'], ['unit'], ['you'], Tree(Lemma('necessitate.v.01.need'), ['need']), ['and'], Tree(Lemma('pare.v.01.pare'), ['pare']), ['operating'], ['expense'], ['proportionately'], ['.']] \n",
      "\n",
      "3 lemma-annotated tokens :\n",
      " [Tree(Lemma('reduce.v.01.cut'), ['cut']), Tree(Lemma('necessitate.v.01.need'), ['need']), Tree(Lemma('pare.v.01.pare'), ['pare'])] \n",
      "\n",
      "\n",
      "sentence 12 : \n",
      " [['A'], Tree(Lemma('yankee.n.01.Yankee'), ['Yankee']), Tree(Lemma('sergeant.n.01.sergeant'), ['sergeant']), Tree(Lemma('yield.v.01.give'), ['gave']), ['the'], Tree('following.s.01', ['following']), Tree(Lemma('description.n.01.description'), ['description']), ['of'], ['his'], Tree(Lemma('sweetheart.n.01.sweetheart'), ['sweetheart']), [':'], ['``'], ['My'], Tree(Lemma('girlfriend.n.02.girl'), ['girl']), Tree(Lemma('be.v.01.be'), ['is']), ['none'], ['of'], ['your'], Tree(Lemma('jerkwater.s.01.one-horse'), ['one-horse']), Tree(Lemma('girl.n.01.girl'), ['girls']), ['.']] \n",
      "\n",
      "10 lemma-annotated tokens :\n",
      " [Tree(Lemma('yankee.n.01.Yankee'), ['Yankee']), Tree(Lemma('sergeant.n.01.sergeant'), ['sergeant']), Tree(Lemma('yield.v.01.give'), ['gave']), Tree('following.s.01', ['following']), Tree(Lemma('description.n.01.description'), ['description']), Tree(Lemma('sweetheart.n.01.sweetheart'), ['sweetheart']), Tree(Lemma('girlfriend.n.02.girl'), ['girl']), Tree(Lemma('be.v.01.be'), ['is']), Tree(Lemma('jerkwater.s.01.one-horse'), ['one-horse']), Tree(Lemma('girl.n.01.girl'), ['girls'])] \n",
      "\n",
      "\n",
      "sentence 13 : \n",
      " [['It'], ['would'], ['have'], Tree(Lemma('kill.v.01.kill'), ['killed']), ['you'], ['in'], ['the'], ['cabin'], ['.']] \n",
      "\n",
      "1 lemma-annotated tokens :\n",
      " [Tree(Lemma('kill.v.01.kill'), ['killed'])] \n",
      "\n",
      "\n",
      "sentence 14 : \n",
      " [['He'], Tree(Lemma('exist.v.02.live'), ['lived']), ['in'], ['a'], ['very'], ['dark'], ['world'], [','], ['but'], ['he'], Tree(Lemma('be.v.03.be'), ['was']), ['not'], ['in'], ['the'], ['dark'], ['.']] \n",
      "\n",
      "2 lemma-annotated tokens :\n",
      " [Tree(Lemma('exist.v.02.live'), ['lived']), Tree(Lemma('be.v.03.be'), ['was'])] \n",
      "\n",
      "\n",
      "sentence 15 : \n",
      " [['Her'], Tree(Lemma('name.n.01.name'), ['name']), Tree(Lemma('be.v.02.be'), ['was']), Tree(Lemma('person.n.01.person'), [Tree('NE', ['Mollie'])]), ['.']] \n",
      "\n",
      "2 lemma-annotated tokens :\n",
      " [Tree(Lemma('name.n.01.name'), ['name']), Tree(Lemma('be.v.02.be'), ['was'])] \n",
      "\n",
      "\n",
      "sentence 16 : \n",
      " [['The'], ['district'], ['office'], ['here'], Tree(Lemma('use.v.01.employ'), ['employs']), ['about'], ['65'], ['.']] \n",
      "\n",
      "1 lemma-annotated tokens :\n",
      " [Tree(Lemma('use.v.01.employ'), ['employs'])] \n",
      "\n",
      "\n",
      "sentence 17 : \n",
      " [['He'], ['never'], ['used', 'to'], ['like'], ['any'], ['hot', 'cereal'], [','], ['now'], ['that'], Tree(Lemma('be.v.02.be'), [\"'s\"]), ['the'], ['only'], ['kind'], ['he'], Tree(Lemma('desire.v.01.want'), ['wants']), ['.']] \n",
      "\n",
      "2 lemma-annotated tokens :\n",
      " [Tree(Lemma('be.v.02.be'), [\"'s\"]), Tree(Lemma('desire.v.01.want'), ['wants'])] \n",
      "\n",
      "\n",
      "sentence 18 : \n",
      " [['To'], Tree(Lemma('help.v.01.help'), ['help']), Tree(Lemma('prevent.v.01.prevent'), ['prevent']), ['orthodontic'], ['problems'], ['from'], Tree(Lemma('originate.v.01.arise'), ['arising']), [','], ['your'], ['dentist'], ['can'], Tree(Lemma('make.v.01.do'), ['do']), ['these'], ['things'], [':']] \n",
      "\n",
      "4 lemma-annotated tokens :\n",
      " [Tree(Lemma('help.v.01.help'), ['help']), Tree(Lemma('prevent.v.01.prevent'), ['prevent']), Tree(Lemma('originate.v.01.arise'), ['arising']), Tree(Lemma('make.v.01.do'), ['do'])] \n",
      "\n",
      "\n",
      "sentence 19 : \n",
      " [['Hotels'], [','], ['for', 'example'], [','], Tree(Lemma('be.v.01.be'), ['are']), ['ready'], ['to'], Tree(Lemma('lower.v.01.let_down'), ['let', 'down']), ['the'], ['bars'], ['.']] \n",
      "\n",
      "2 lemma-annotated tokens :\n",
      " [Tree(Lemma('be.v.01.be'), ['are']), Tree(Lemma('lower.v.01.let_down'), ['let', 'down'])] \n",
      "\n",
      "\n",
      "sentence 20 : \n",
      " [['Owen'], Tree(Lemma('be.v.01.be'), ['was']), ['surprised'], ['to'], Tree(Lemma('see.v.01.see'), ['see']), ['Mrs.', 'Gertrude', 'Parker'], Tree(Lemma('play.v.10.play'), ['playing']), ['the'], ['one-arm'], ['bandits'], ['that'], ['were'], ['cunningly'], Tree(Lemma('arrange.v.01.arrange'), ['arranged']), ['by'], ['the'], ['entrance'], ['.']] \n",
      "\n",
      "4 lemma-annotated tokens :\n",
      " [Tree(Lemma('be.v.01.be'), ['was']), Tree(Lemma('see.v.01.see'), ['see']), Tree(Lemma('play.v.10.play'), ['playing']), Tree(Lemma('arrange.v.01.arrange'), ['arranged'])] \n",
      "\n",
      "\n",
      "sentence 21 : \n",
      " [['You'], ['must'], Tree(Lemma('know.v.01.know'), ['know']), Tree(Lemma('earlier.r.01.before'), ['before']), ['you'], Tree(Lemma('start.v.06.start'), ['start']), ['what'], ['the'], Tree(Lemma('need.n.02.need'), ['needs']), ['and'], Tree(Lemma('aim.n.02.objective'), ['objectives']), ['of'], ['your'], Tree(Lemma('organization.n.01.organization'), ['organization']), Tree(Lemma('be.v.02.be'), ['are']), [';'], ['you'], ['must'], Tree(Lemma('have.v.01.have'), ['have']), ['a'], Tree(Lemma('list.n.01.list'), ['list']), ['of'], Tree(Lemma('necessity.n.02.requirement'), ['requirements']), ['on'], ['where'], [','], ['how'], ['many'], [','], ['and'], ['what'], Tree(Lemma('type.n.01.type'), ['type']), Tree(Lemma('site.n.01.site'), ['sites']), ['are'], Tree(Lemma('want.v.02.need'), ['needed']), ['.']] \n",
      "\n",
      "13 lemma-annotated tokens :\n",
      " [Tree(Lemma('know.v.01.know'), ['know']), Tree(Lemma('earlier.r.01.before'), ['before']), Tree(Lemma('start.v.06.start'), ['start']), Tree(Lemma('need.n.02.need'), ['needs']), Tree(Lemma('aim.n.02.objective'), ['objectives']), Tree(Lemma('organization.n.01.organization'), ['organization']), Tree(Lemma('be.v.02.be'), ['are']), Tree(Lemma('have.v.01.have'), ['have']), Tree(Lemma('list.n.01.list'), ['list']), Tree(Lemma('necessity.n.02.requirement'), ['requirements']), Tree(Lemma('type.n.01.type'), ['type']), Tree(Lemma('site.n.01.site'), ['sites']), Tree(Lemma('want.v.02.need'), ['needed'])] \n",
      "\n",
      "\n",
      "sentence 22 : \n",
      " [['While'], Tree(Lemma('most.a.01.most'), ['most']), ['of'], ['his'], Tree(Lemma('belief.n.01.belief'), ['beliefs']), Tree(Lemma('be.v.01.be'), ['were']), Tree(Lemma('still.r.01.still'), ['still']), Tree(Lemma('unsettled.a.01.unsettled'), ['unsettled']), [','], ['he'], Tree(Lemma('know.v.03.know'), ['knew']), ['that'], ['he'], ['did'], Tree(Lemma('not.r.01.not'), ['not']), Tree(Lemma('believe_in.v.01.believe_in'), ['believe', 'in']), Tree(Lemma('killing.n.02.killing'), ['killing']), ['.']] \n",
      "\n",
      "9 lemma-annotated tokens :\n",
      " [Tree(Lemma('most.a.01.most'), ['most']), Tree(Lemma('belief.n.01.belief'), ['beliefs']), Tree(Lemma('be.v.01.be'), ['were']), Tree(Lemma('still.r.01.still'), ['still']), Tree(Lemma('unsettled.a.01.unsettled'), ['unsettled']), Tree(Lemma('know.v.03.know'), ['knew']), Tree(Lemma('not.r.01.not'), ['not']), Tree(Lemma('believe_in.v.01.believe_in'), ['believe', 'in']), Tree(Lemma('killing.n.02.killing'), ['killing'])] \n",
      "\n",
      "\n",
      "sentence 23 : \n",
      " [['``'], ['He'], ['did'], Tree(\"n't.r.00\", [\"n't\"]), [\"''\"], ['!']] \n",
      "\n",
      "1 lemma-annotated tokens :\n",
      " [Tree(\"n't.r.00\", [\"n't\"])] \n",
      "\n",
      "\n",
      "sentence 24 : \n",
      " [['The'], Tree(Lemma('jury.n.01.jury'), ['jury']), Tree(Lemma('praise.v.01.praise'), ['praised']), ['the'], Tree(Lemma('administration.n.01.administration'), ['administration']), ['and'], Tree(Lemma('operation.n.01.operation'), ['operation']), ['of'], ['the'], Tree(Lemma('atlanta.n.01.Atlanta'), ['Atlanta']), Tree(Lemma('police_department.n.01.police_department'), ['Police', 'Department']), [','], ['the'], Tree(Lemma('group.n.01.group'), [Tree('NE', ['Fulton', 'Tax', 'Commissioner', \"'s\", 'Office'])]), [','], ['the'], Tree(Lemma('location.n.01.location'), [Tree('NE', ['Bellwood'])]), ['and'], Tree(Lemma('location.n.01.location'), [Tree('NE', ['Alpharetta'])]), Tree(Lemma('work_camp.n.01.prison_farm'), ['prison', 'farms']), [','], Tree(Lemma('group.n.01.group'), [Tree('NE', ['Grady', 'Hospital'])]), ['and'], ['the'], Tree(Lemma('group.n.01.group'), [Tree('NE', ['Fulton', 'Health', 'Department'])]), ['.']] \n",
      "\n",
      "7 lemma-annotated tokens :\n",
      " [Tree(Lemma('jury.n.01.jury'), ['jury']), Tree(Lemma('praise.v.01.praise'), ['praised']), Tree(Lemma('administration.n.01.administration'), ['administration']), Tree(Lemma('operation.n.01.operation'), ['operation']), Tree(Lemma('atlanta.n.01.Atlanta'), ['Atlanta']), Tree(Lemma('police_department.n.01.police_department'), ['Police', 'Department']), Tree(Lemma('work_camp.n.01.prison_farm'), ['prison', 'farms'])] \n",
      "\n",
      "\n",
      "sentence 25 : \n",
      " [Tree(Lemma('then.r.01.then'), ['Then']), [','], ['with'], ['the'], Tree(Lemma('new.a.01.new'), ['new']), Tree(Lemma('affluence.n.01.affluence'), ['affluence']), [','], ['there'], ['is'], Tree(Lemma('actually.r.01.actually'), ['actually']), ['a'], Tree(Lemma('sally.n.03.sallying_forth'), ['sallying', 'forth']), ['into'], ['the'], Tree(Lemma('wide.a.01.wide'), ['wide']), [','], Tree(Lemma('wide.a.01.wide'), ['wide']), Tree(Lemma('earth.n.01.world'), ['world']), ['beyond'], ['the'], Tree(Lemma('precinct.n.01.precinct'), ['precincts']), ['of'], Tree(Lemma('new_york.n.01.New_York'), ['New', 'York']), ['.']] \n",
      "\n",
      "10 lemma-annotated tokens :\n",
      " [Tree(Lemma('then.r.01.then'), ['Then']), Tree(Lemma('new.a.01.new'), ['new']), Tree(Lemma('affluence.n.01.affluence'), ['affluence']), Tree(Lemma('actually.r.01.actually'), ['actually']), Tree(Lemma('sally.n.03.sallying_forth'), ['sallying', 'forth']), Tree(Lemma('wide.a.01.wide'), ['wide']), Tree(Lemma('wide.a.01.wide'), ['wide']), Tree(Lemma('earth.n.01.world'), ['world']), Tree(Lemma('precinct.n.01.precinct'), ['precincts']), Tree(Lemma('new_york.n.01.New_York'), ['New', 'York'])] \n",
      "\n",
      "\n",
      "sentence 26 : \n",
      " [Tree(Lemma('slightly.r.01.somewhat'), ['Somewhat']), ['to'], ['his'], Tree(Lemma('surprise.n.01.surprise'), ['surprise']), ['he'], Tree(Lemma('detect.v.01.find'), ['found']), ['that'], Tree('one.s.00', ['one']), Tree(Lemma('girl.n.01.girl'), ['girl']), [','], ['whom'], ['he'], ['would'], Tree(Lemma('never.r.01.never'), ['never']), ['have'], Tree(Lemma('consider.v.03.consider'), ['considered']), ['for'], ['the'], Tree(Lemma('occupation.n.01.job'), ['job']), ['since'], ['she'], ['had'], Tree(Lemma('look.v.02.appear'), ['appeared']), Tree(Lemma('slightly.r.01.somewhat'), ['somewhat']), Tree(Lemma('mousy.s.01.mousy'), ['mousy']), ['and'], Tree(Lemma('besides.r.02.also'), ['also']), ['had'], Tree(Lemma('be.v.03.be'), ['been']), ['in'], ['the'], Tree(Lemma('office.n.01.office'), ['office']), ['a'], Tree(Lemma('relatively.r.01.relatively'), ['relatively']), Tree(Lemma('short.a.01.short'), ['short']), Tree(Lemma('time.n.02.time'), ['time']), [','], Tree(Lemma('do.v.03.do'), ['did']), ['the'], Tree(Lemma('most.r.01.most'), ['most']), Tree(Lemma('outstanding.s.01.outstanding'), ['outstanding']), Tree(Lemma('job.n.06.job'), ['job']), ['of'], Tree(Lemma('act.v.03.play'), ['playing']), ['the'], Tree('role.n.2;1', ['role']), ['of'], Tree(Lemma('receptionist.n.01.receptionist'), ['receptionist']), [','], Tree('show.v.6;4', ['showing']), Tree(Lemma('wit.n.01.wit'), ['wit']), [','], Tree(Lemma('sparkle.n.01.sparkle'), ['sparkle']), [','], ['and'], Tree(Lemma('aplomb.n.01.aplomb'), ['aplomb']), ['.']] \n",
      "\n",
      "28 lemma-annotated tokens :\n",
      " [Tree(Lemma('slightly.r.01.somewhat'), ['Somewhat']), Tree(Lemma('surprise.n.01.surprise'), ['surprise']), Tree(Lemma('detect.v.01.find'), ['found']), Tree('one.s.00', ['one']), Tree(Lemma('girl.n.01.girl'), ['girl']), Tree(Lemma('never.r.01.never'), ['never']), Tree(Lemma('consider.v.03.consider'), ['considered']), Tree(Lemma('occupation.n.01.job'), ['job']), Tree(Lemma('look.v.02.appear'), ['appeared']), Tree(Lemma('slightly.r.01.somewhat'), ['somewhat']), Tree(Lemma('mousy.s.01.mousy'), ['mousy']), Tree(Lemma('besides.r.02.also'), ['also']), Tree(Lemma('be.v.03.be'), ['been']), Tree(Lemma('office.n.01.office'), ['office']), Tree(Lemma('relatively.r.01.relatively'), ['relatively']), Tree(Lemma('short.a.01.short'), ['short']), Tree(Lemma('time.n.02.time'), ['time']), Tree(Lemma('do.v.03.do'), ['did']), Tree(Lemma('most.r.01.most'), ['most']), Tree(Lemma('outstanding.s.01.outstanding'), ['outstanding']), Tree(Lemma('job.n.06.job'), ['job']), Tree(Lemma('act.v.03.play'), ['playing']), Tree('role.n.2;1', ['role']), Tree(Lemma('receptionist.n.01.receptionist'), ['receptionist']), Tree('show.v.6;4', ['showing']), Tree(Lemma('wit.n.01.wit'), ['wit']), Tree(Lemma('sparkle.n.01.sparkle'), ['sparkle']), Tree(Lemma('aplomb.n.01.aplomb'), ['aplomb'])] \n",
      "\n",
      "\n",
      "sentence 27 : \n",
      " [['The'], Tree(Lemma('whole.a.01.whole'), ['whole']), ['fucken'], Tree(Lemma('sky.n.01.sky'), ['sky']), [\"'s\"], ['cavin'], ['in'], ['!']] \n",
      "\n",
      "2 lemma-annotated tokens :\n",
      " [Tree(Lemma('whole.a.01.whole'), ['whole']), Tree(Lemma('sky.n.01.sky'), ['sky'])] \n",
      "\n",
      "\n",
      "sentence 28 : \n",
      " [['Isfahan'], Tree(Lemma('become.v.02.become'), ['became']), ['more'], ['of'], ['a'], ['legend'], ['than'], ['a'], ['place'], [','], ['and'], ['now'], ['it'], Tree(Lemma('be.v.01.be'), ['is']), ['for'], ['many'], ['people'], ['simply'], ['a'], ['name'], ['to', 'which'], ['they'], Tree(Lemma('attach.v.03.attach'), ['attach']), ['their'], ['notions'], ['of'], ['old'], ['Persia'], ['and'], ['sometimes'], ['of'], ['the'], ['East'], ['.']] \n",
      "\n",
      "3 lemma-annotated tokens :\n",
      " [Tree(Lemma('become.v.02.become'), ['became']), Tree(Lemma('be.v.01.be'), ['is']), Tree(Lemma('attach.v.03.attach'), ['attach'])] \n",
      "\n",
      "\n",
      "sentence 29 : \n",
      " [Tree(Lemma('person.n.01.person'), [Tree('NE', ['Chandler'])]), Tree(Lemma('leave.v.02.leave'), ['left']), Tree(Lemma('person.n.01.person'), [Tree('NE', ['Carroll'])]), ['at'], ['the'], Tree(Lemma('bottom.n.02.bottom'), ['bottom']), ['of'], ['the'], Tree(Lemma('hill.n.01.hill'), ['hill']), ['to'], Tree(Lemma('conduct.v.02.direct'), ['direct']), ['any'], Tree(Lemma('support.n.04.reinforcement'), ['reinforcements']), ['he'], ['could'], Tree(Lemma('find.v.01.find'), ['find']), ['to'], ['the'], Tree(Lemma('battle.n.01.fight'), ['fight']), ['.']] \n",
      "\n",
      "7 lemma-annotated tokens :\n",
      " [Tree(Lemma('leave.v.02.leave'), ['left']), Tree(Lemma('bottom.n.02.bottom'), ['bottom']), Tree(Lemma('hill.n.01.hill'), ['hill']), Tree(Lemma('conduct.v.02.direct'), ['direct']), Tree(Lemma('support.n.04.reinforcement'), ['reinforcements']), Tree(Lemma('find.v.01.find'), ['find']), Tree(Lemma('battle.n.01.fight'), ['fight'])] \n",
      "\n",
      "\n",
      "sentence 30 : \n",
      " [['In'], ['an'], ['anonymous'], ['interview'], ['with'], ['a'], ['French'], ['newspaper'], ['the'], ['financier'], Tree(Lemma('tell.v.03.tell'), ['told']), ['of'], Tree(Lemma('spend.v.01.spend'), ['spending']), ['several'], ['months'], ['with'], ['her'], ['.']] \n",
      "\n",
      "2 lemma-annotated tokens :\n",
      " [Tree(Lemma('tell.v.03.tell'), ['told']), Tree(Lemma('spend.v.01.spend'), ['spending'])] \n",
      "\n",
      "\n",
      "sentence 31 : \n",
      " [['Where'], ['can'], ['we'], Tree(Lemma('get.v.01.get'), ['get']), ['this'], ['cereal'], ['he'], Tree(Lemma('like.v.02.like'), ['likes']), ['so', 'much'], [\"''\"], ['?']] \n",
      "\n",
      "2 lemma-annotated tokens :\n",
      " [Tree(Lemma('get.v.01.get'), ['get']), Tree(Lemma('like.v.02.like'), ['likes'])] \n",
      "\n",
      "\n",
      "sentence 32 : \n",
      " [['``'], ['I'], ['could'], Tree(\"n't.r.00\", [\"n't\"]), Tree(Lemma('compose.v.02.write'), ['write']), ['with'], ['them'], ['in'], ['the'], Tree(Lemma('same.a.01.same'), ['same']), Tree(Lemma('room.n.01.room'), ['room']), ['with'], ['me'], [','], ['but'], ['I'], ['could'], ['with'], Tree(Lemma('person.n.01.person'), [Tree('NE', ['Harold'])]), ['.']] \n",
      "\n",
      "4 lemma-annotated tokens :\n",
      " [Tree(\"n't.r.00\", [\"n't\"]), Tree(Lemma('compose.v.02.write'), ['write']), Tree(Lemma('same.a.01.same'), ['same']), Tree(Lemma('room.n.01.room'), ['room'])] \n",
      "\n",
      "\n",
      "sentence 33 : \n",
      " [Tree(Lemma('give.v.03.give'), ['Give']), ['generously'], ['when'], ['you'], Tree(Lemma('buy.v.01.buy'), ['buy']), ['candy'], ['today'], ['for'], ['the'], ['Brain'], ['Research', 'Foundation'], ['.']] \n",
      "\n",
      "2 lemma-annotated tokens :\n",
      " [Tree(Lemma('give.v.03.give'), ['Give']), Tree(Lemma('buy.v.01.buy'), ['buy'])] \n",
      "\n",
      "\n",
      "sentence 34 : \n",
      " [['``'], ['This'], Tree(Lemma('be.v.01.be'), ['is']), ['not'], ['a'], ['program'], ['of'], ['socialized'], ['medicine'], ['.']] \n",
      "\n",
      "1 lemma-annotated tokens :\n",
      " [Tree(Lemma('be.v.01.be'), ['is'])] \n",
      "\n",
      "\n",
      "sentence 35 : \n",
      " [Tree(Lemma('feel.v.01.feel'), ['Feeling']), ['protective'], ['toward'], ['this'], ['sleeping'], ['being'], [','], ['Henrietta'], Tree(Lemma('find.v.01.find'), ['found']), ['a'], ['yesterday'], ['bun'], ['and'], ['milk'], ['in'], ['a'], ['white'], ['jug'], [','], ['a'], ['breakfast'], ['which'], Tree(Lemma('be.v.02.be'), ['was']), ['somewhat'], ['the'], ['equivalent'], ['of'], Tree(Lemma('travel.v.01.go'), ['going']), ['barefoot'], ['.']] \n",
      "\n",
      "4 lemma-annotated tokens :\n",
      " [Tree(Lemma('feel.v.01.feel'), ['Feeling']), Tree(Lemma('find.v.01.find'), ['found']), Tree(Lemma('be.v.02.be'), ['was']), Tree(Lemma('travel.v.01.go'), ['going'])] \n",
      "\n",
      "\n",
      "sentence 36 : \n",
      " [['We'], ['should'], Tree(Lemma('avoid.v.01.avoid'), ['avoid']), ['these'], ['congestion'], ['points'], ['or'], [','], Tree(Lemma('frame.v.04.put'), ['putting']), ['it'], ['another'], ['way'], [','], Tree(Lemma('keep.v.01.keep'), ['keep']), ['ars'], Tree(Lemma('depart.v.03.start'), ['starting']), ['and'], Tree(Lemma('end.v.01.end'), ['ending']), ['on'], ['the'], ['East', 'side'], ['of'], ['the'], ['river'], ['-'], ['on'], ['the'], ['East', 'side'], ['.']] \n",
      "\n",
      "5 lemma-annotated tokens :\n",
      " [Tree(Lemma('avoid.v.01.avoid'), ['avoid']), Tree(Lemma('frame.v.04.put'), ['putting']), Tree(Lemma('keep.v.01.keep'), ['keep']), Tree(Lemma('depart.v.03.start'), ['starting']), Tree(Lemma('end.v.01.end'), ['ending'])] \n",
      "\n",
      "\n",
      "sentence 37 : \n",
      " [['``'], ['If'], ['I'], Tree(Lemma('catch.v.01.catch'), ['catch']), ['you'], Tree(Lemma('one.s.01.one'), ['one']), Tree('more.s.00', ['more']), Tree(Lemma('time.n.01.time'), ['time']), Tree(Lemma('down.a.01.down'), ['down']), Tree(Lemma('here.r.01.here'), ['here']), ['without'], Tree(Lemma('stocking.n.01.stocking'), ['stockings']), [\"''\"], ['-']] \n",
      "\n",
      "7 lemma-annotated tokens :\n",
      " [Tree(Lemma('catch.v.01.catch'), ['catch']), Tree(Lemma('one.s.01.one'), ['one']), Tree('more.s.00', ['more']), Tree(Lemma('time.n.01.time'), ['time']), Tree(Lemma('down.a.01.down'), ['down']), Tree(Lemma('here.r.01.here'), ['here']), Tree(Lemma('stocking.n.01.stocking'), ['stockings'])] \n",
      "\n",
      "\n",
      "sentence 38 : \n",
      " [['Under'], Tree(Lemma('modern.a.01.modern'), ['modern']), Tree(Lemma('condition.n.01.condition'), ['conditions']), [','], ['this'], Tree(Lemma('be.v.01.be'), ['is']), Tree(Lemma('particularly.r.01.especially'), ['especially']), Tree(Lemma('true.a.01.true'), ['true']), ['of'], ['the'], Tree(Lemma('ready.a.01.ready'), ['ready']), Tree(Lemma('militia.n.01.reserves'), ['reserve']), ['.']] \n",
      "\n",
      "7 lemma-annotated tokens :\n",
      " [Tree(Lemma('modern.a.01.modern'), ['modern']), Tree(Lemma('condition.n.01.condition'), ['conditions']), Tree(Lemma('be.v.01.be'), ['is']), Tree(Lemma('particularly.r.01.especially'), ['especially']), Tree(Lemma('true.a.01.true'), ['true']), Tree(Lemma('ready.a.01.ready'), ['ready']), Tree(Lemma('militia.n.01.reserves'), ['reserve'])] \n",
      "\n",
      "\n",
      "sentence 39 : \n",
      " [['If'], ['the'], Tree(Lemma('department_of_state.n.01.Department_of_State'), ['Department', 'of', 'State']), ['is'], ['to'], Tree(Lemma('assume.v.05.take'), ['take']), Tree(Lemma('primary.a.01.primary'), ['primary']), Tree(Lemma('duty.n.01.responsibility'), ['responsibility']), ['for'], Tree(Lemma('foreign_policy.n.01.foreign_policy'), ['foreign', 'policy']), ['in'], Tree(Lemma('capital.n.06.Washington'), ['Washington']), [','], ['it'], Tree(Lemma('follow.v.03.follow'), ['follows']), ['that'], ['the'], Tree(Lemma('ambassador.n.01.ambassador'), ['ambassador']), ['is'], Tree(Lemma('ask.v.04.expect'), ['expected']), ['to'], Tree(Lemma('take_hold.v.01.take_charge'), ['take', 'charge']), Tree(Lemma('overseas.r.02.overseas'), ['overseas']), ['.']] \n",
      "\n",
      "11 lemma-annotated tokens :\n",
      " [Tree(Lemma('department_of_state.n.01.Department_of_State'), ['Department', 'of', 'State']), Tree(Lemma('assume.v.05.take'), ['take']), Tree(Lemma('primary.a.01.primary'), ['primary']), Tree(Lemma('duty.n.01.responsibility'), ['responsibility']), Tree(Lemma('foreign_policy.n.01.foreign_policy'), ['foreign', 'policy']), Tree(Lemma('capital.n.06.Washington'), ['Washington']), Tree(Lemma('follow.v.03.follow'), ['follows']), Tree(Lemma('ambassador.n.01.ambassador'), ['ambassador']), Tree(Lemma('ask.v.04.expect'), ['expected']), Tree(Lemma('take_hold.v.01.take_charge'), ['take', 'charge']), Tree(Lemma('overseas.r.02.overseas'), ['overseas'])] \n",
      "\n",
      "\n",
      "sentence 40 : \n",
      " [['Selections'], ['from'], ['fifteen'], ['countries'], ['were'], Tree(Lemma('sing.v.01.sing'), ['sung']), ['as'], ['solos'], ['and'], ['duets'], ['in'], ['a'], ['broad'], ['range'], ['of'], ['languages'], ['.']] \n",
      "\n",
      "1 lemma-annotated tokens :\n",
      " [Tree(Lemma('sing.v.01.sing'), ['sung'])] \n",
      "\n",
      "\n",
      "sentence 41 : \n",
      " [['Anti-recession'], ['programs'], ['-'], ['aid'], ['for'], ['the'], ['unemployed'], [','], ['their'], ['children'], ['and'], ['for'], ['depressed'], ['areas'], ['-'], Tree(Lemma('account_for.v.01.account_for'), ['account', 'for']), ['only'], ['900'], ['million'], ['of'], ['the'], ['6.9'], ['billion'], ['dollar'], ['deficit'], ['.']] \n",
      "\n",
      "1 lemma-annotated tokens :\n",
      " [Tree(Lemma('account_for.v.01.account_for'), ['account', 'for'])] \n",
      "\n",
      "\n",
      "sentence 42 : \n",
      " [['Traffic'], ['frequently'], ['has'], Tree(Lemma('fail.v.01.fail'), ['failed']), ['to'], Tree(Lemma('qualify.v.01.measure_up'), ['measure', 'up']), ['to'], ['engineers'], [\"'\"], ['rosy'], ['estimates'], ['.']] \n",
      "\n",
      "2 lemma-annotated tokens :\n",
      " [Tree(Lemma('fail.v.01.fail'), ['failed']), Tree(Lemma('qualify.v.01.measure_up'), ['measure', 'up'])] \n",
      "\n",
      "\n",
      "sentence 43 : \n",
      " [['``'], ['I'], Tree(Lemma('try.v.01.try'), ['try']), [\"''\"], [','], Tree(Lemma('person.n.01.person'), [Tree('NE', ['Felix'])]), Tree(Lemma('state.v.01.say'), ['said']), Tree(Lemma('happily.r.01.blithely'), ['blithely']), ['.']] \n",
      "\n",
      "3 lemma-annotated tokens :\n",
      " [Tree(Lemma('try.v.01.try'), ['try']), Tree(Lemma('state.v.01.say'), ['said']), Tree(Lemma('happily.r.01.blithely'), ['blithely'])] \n",
      "\n",
      "\n",
      "sentence 44 : \n",
      " [['And', 'then'], ['he'], Tree(Lemma('hear.v.01.hear'), ['heard']), ['a'], ['car'], Tree(Lemma('come.v.01.come'), ['coming']), ['from'], ['the'], ['east'], [','], ['and'], ['he'], Tree(Lemma('feel.v.01.feel'), ['felt']), ['as'], ['if'], ['he'], ['would'], Tree(Lemma('break_down.v.03.break_down'), ['break', 'down']), ['and'], Tree(Lemma('cry.v.02.weep'), ['weep']), ['.']] \n",
      "\n",
      "5 lemma-annotated tokens :\n",
      " [Tree(Lemma('hear.v.01.hear'), ['heard']), Tree(Lemma('come.v.01.come'), ['coming']), Tree(Lemma('feel.v.01.feel'), ['felt']), Tree(Lemma('break_down.v.03.break_down'), ['break', 'down']), Tree(Lemma('cry.v.02.weep'), ['weep'])] \n",
      "\n",
      "\n",
      "sentence 45 : \n",
      " [['``'], Tree(Lemma('person.n.01.person'), [Tree('NE', ['Monsieur', 'Favre'])]), Tree(Lemma('indicate.v.03.indicate'), ['indicated']), ['that'], ['if'], ['I'], ['would'], Tree(Lemma('collaborate.v.01.cooperate'), ['co-operate']), [','], ['after'], ['you'], ['and'], Tree(Lemma('person.n.01.person'), [Tree('NE', ['William'])]), ['are'], Tree(Lemma('banish.v.01.banish'), ['banished']), [','], ['following'], ['the'], Tree(Lemma('debate.n.02.debate'), ['debate']), [','], ['I'], ['will'], ['be'], Tree(Lemma('give.v.03.give'), ['given']), ['a'], Tree(Lemma('position.n.06.place'), ['place']), ['of'], Tree(Lemma('influence.n.01.influence'), ['influence']), [\"''\"], ['.']] \n",
      "\n",
      "7 lemma-annotated tokens :\n",
      " [Tree(Lemma('indicate.v.03.indicate'), ['indicated']), Tree(Lemma('collaborate.v.01.cooperate'), ['co-operate']), Tree(Lemma('banish.v.01.banish'), ['banished']), Tree(Lemma('debate.n.02.debate'), ['debate']), Tree(Lemma('give.v.03.give'), ['given']), Tree(Lemma('position.n.06.place'), ['place']), Tree(Lemma('influence.n.01.influence'), ['influence'])] \n",
      "\n",
      "\n",
      "sentence 46 : \n",
      " [Tree(Lemma('thus.r.02.thus'), ['Thus']), ['the'], Tree(Lemma('image.n.01.image'), ['image']), ['of'], Tree('man.n.00', ['man']), ['has'], Tree(Lemma('suffer.v.01.suffer'), ['suffered']), Tree(Lemma('complete.a.01.complete'), ['complete']), Tree(Lemma('atomization.n.01.fragmentation'), ['fragmentation']), ['in'], Tree(Lemma('personal.a.01.personal'), ['personal']), ['and'], Tree(Lemma('spiritual.s.03.spiritual'), ['spiritual']), Tree(Lemma('quality.n.01.quality'), ['qualities']), [','], ['and'], Tree(Lemma('complete.a.01.complete'), ['complete']), Tree(Lemma('objectification.n.01.objectification'), ['objectification']), ['in'], Tree(Lemma('subhuman.a.01.subhuman'), ['sub-human']), ['and'], Tree(Lemma('quasi.s.01.quasi'), ['quasi']), Tree(Lemma('mechanistic.s.01.mechanistic'), ['mechanistic']), Tree(Lemma('ability.n.02.power'), ['powers']), ['.']] \n",
      "\n",
      "15 lemma-annotated tokens :\n",
      " [Tree(Lemma('thus.r.02.thus'), ['Thus']), Tree(Lemma('image.n.01.image'), ['image']), Tree('man.n.00', ['man']), Tree(Lemma('suffer.v.01.suffer'), ['suffered']), Tree(Lemma('complete.a.01.complete'), ['complete']), Tree(Lemma('atomization.n.01.fragmentation'), ['fragmentation']), Tree(Lemma('personal.a.01.personal'), ['personal']), Tree(Lemma('spiritual.s.03.spiritual'), ['spiritual']), Tree(Lemma('quality.n.01.quality'), ['qualities']), Tree(Lemma('complete.a.01.complete'), ['complete']), Tree(Lemma('objectification.n.01.objectification'), ['objectification']), Tree(Lemma('subhuman.a.01.subhuman'), ['sub-human']), Tree(Lemma('quasi.s.01.quasi'), ['quasi']), Tree(Lemma('mechanistic.s.01.mechanistic'), ['mechanistic']), Tree(Lemma('ability.n.02.power'), ['powers'])] \n",
      "\n",
      "\n",
      "sentence 47 : \n",
      " [['You'], ['should'], ['have'], Tree('go_to.v.00', ['gone', 'to']), Tree(Lemma('workplace.n.01.work'), ['work']), Tree(Lemma('today.r.02.today'), ['today']), [','], [\"'\"], ['stead'], ['of'], Tree(Lemma('sneak.v.01.sneak'), ['sneaking']), Tree(Lemma('about.r.04.around'), ['around']), Tree(Lemma('spy.v.02.spy'), ['spying']), ['on'], ['the'], Tree(Lemma('person.n.01.person'), [Tree('NE', ['Dronk'])]), Tree(Lemma('house.n.01.house'), ['house']), [\"''\"], ['.']] \n",
      "\n",
      "7 lemma-annotated tokens :\n",
      " [Tree('go_to.v.00', ['gone', 'to']), Tree(Lemma('workplace.n.01.work'), ['work']), Tree(Lemma('today.r.02.today'), ['today']), Tree(Lemma('sneak.v.01.sneak'), ['sneaking']), Tree(Lemma('about.r.04.around'), ['around']), Tree(Lemma('spy.v.02.spy'), ['spying']), Tree(Lemma('house.n.01.house'), ['house'])] \n",
      "\n",
      "\n",
      "sentence 48 : \n",
      " [['In'], ['a'], ['tense'], [','], ['closed-door'], ['session'], ['with'], ['Judge', 'Smith'], [','], ['Rayburn'], Tree(Lemma('try.v.01.attempt'), ['attempted']), ['to'], Tree(Lemma('work_out.v.01.work_out'), ['work', 'out']), ['a'], ['compromise'], [':'], ['to'], Tree(Lemma('add.v.01.add'), ['add']), ['three'], ['new'], ['members'], ['to'], ['the'], ['Rules', 'Committee'], ['('], ['two'], ['Democrats'], [','], Tree(Lemma('include.v.01.include'), ['including']), ['one'], ['Southerner'], [','], ['and'], ['one'], ['Republican'], [')'], ['.']] \n",
      "\n",
      "4 lemma-annotated tokens :\n",
      " [Tree(Lemma('try.v.01.attempt'), ['attempted']), Tree(Lemma('work_out.v.01.work_out'), ['work', 'out']), Tree(Lemma('add.v.01.add'), ['add']), Tree(Lemma('include.v.01.include'), ['including'])] \n",
      "\n",
      "\n",
      "sentence 49 : \n",
      " [['After'], Tree(Lemma('notch.v.01.notch'), ['notching']), ['it'], ['for'], ['the'], ['keelson'], [','], ['chines'], ['and'], ['battens'], [','], ['the'], ['half-inch'], ['plywood'], ['transom'], ['is'], Tree(Lemma('fasten.v.01.secure'), ['secured']), ['to', 'it'], ['with'], ['glue'], ['and'], ['the'], ['same'], ['type'], ['nails'], ['.']] \n",
      "\n",
      "2 lemma-annotated tokens :\n",
      " [Tree(Lemma('notch.v.01.notch'), ['notching']), Tree(Lemma('fasten.v.01.secure'), ['secured'])] \n",
      "\n",
      "\n",
      "sentence 50 : \n",
      " [['And'], ['I'], [\"'ll\"], Tree(Lemma('lead.v.01.take'), ['take']), ['you'], ['with'], ['me'], ['.']] \n",
      "\n",
      "1 lemma-annotated tokens :\n",
      " [Tree(Lemma('lead.v.01.take'), ['take'])] \n",
      "\n",
      "\n",
      "sentence 51 : \n",
      " [['A'], ['new'], ['radial'], ['drill'], ['press'], ['with'], ['a'], ['16'], ['inch'], ['capacity'], Tree(Lemma('have.v.02.have'), ['has']), ['a'], Tree(Lemma('lean.v.01.tilt'), ['tilting']), ['head'], ['that'], Tree(Lemma('let.v.01.allow'), ['allows']), ['drilling'], ['to'], ['be'], Tree(Lemma('make.v.01.do'), ['done']), ['at'], ['any'], ['angle'], ['.']] \n",
      "\n",
      "4 lemma-annotated tokens :\n",
      " [Tree(Lemma('have.v.02.have'), ['has']), Tree(Lemma('lean.v.01.tilt'), ['tilting']), Tree(Lemma('let.v.01.allow'), ['allows']), Tree(Lemma('make.v.01.do'), ['done'])] \n",
      "\n",
      "\n",
      "sentence 52 : \n",
      " [['I'], ['used', 'to'], Tree(Lemma('love.v.01.love'), ['love']), ['this'], Tree(Lemma('country.n.02.country'), ['country']), ['and'], Tree(Lemma('believe.v.03.believe'), ['believe']), ['that'], Tree(Lemma('someday.r.01.someday'), ['someday']), ['we'], [\"'d\"], Tree(Lemma('win.v.01.win'), ['win']), ['our'], Tree(Lemma('struggle.n.01.battle'), ['battle']), ['for'], Tree(Lemma('equality.n.01.equality'), ['equality']), ['.']] \n",
      "\n",
      "7 lemma-annotated tokens :\n",
      " [Tree(Lemma('love.v.01.love'), ['love']), Tree(Lemma('country.n.02.country'), ['country']), Tree(Lemma('believe.v.03.believe'), ['believe']), Tree(Lemma('someday.r.01.someday'), ['someday']), Tree(Lemma('win.v.01.win'), ['win']), Tree(Lemma('struggle.n.01.battle'), ['battle']), Tree(Lemma('equality.n.01.equality'), ['equality'])] \n",
      "\n",
      "\n",
      "sentence 53 : \n",
      " [['``'], ['Thanks'], [\"''\"], [','], ['Owen'], Tree(Lemma('state.v.01.say'), ['said']), [','], ['``'], ['but'], ['Friday'], Tree(Lemma('be.v.01.be'), ['is']), ['a'], ['long'], ['way'], ['off'], ['and'], ['anything'], ['can'], Tree(Lemma('happen.v.01.happen'), ['happen']), [\"''\"], ['.']] \n",
      "\n",
      "3 lemma-annotated tokens :\n",
      " [Tree(Lemma('state.v.01.say'), ['said']), Tree(Lemma('be.v.01.be'), ['is']), Tree(Lemma('happen.v.01.happen'), ['happen'])] \n",
      "\n",
      "\n",
      "sentence 54 : \n",
      " [['One'], Tree(Lemma('wonder.v.02.wonder'), ['wonders']), ['about'], ['its'], Tree(Lemma('applicability.n.01.applicability'), ['applicability']), ['to'], Tree(Lemma('people.n.01.people'), ['people']), ['.']] \n",
      "\n",
      "3 lemma-annotated tokens :\n",
      " [Tree(Lemma('wonder.v.02.wonder'), ['wonders']), Tree(Lemma('applicability.n.01.applicability'), ['applicability']), Tree(Lemma('people.n.01.people'), ['people'])] \n",
      "\n",
      "\n",
      "sentence 55 : \n",
      " [['Richard', 'J.', 'Hughes'], Tree(Lemma('make.v.01.make'), ['made']), ['his'], ['Morris', 'County'], ['debut'], ['in'], ['his'], ['bid'], ['for'], ['the'], ['Democratic'], ['gubernatorial'], ['nomination'], ['here'], ['last'], ['night'], ['with'], ['a'], ['pledge'], ['``'], ['to'], Tree(Lemma('transport.v.02.carry'), ['carry']), ['the'], ['issues'], ['to'], ['every'], ['corner'], ['of'], ['the'], ['state'], [\"''\"], ['.']] \n",
      "\n",
      "2 lemma-annotated tokens :\n",
      " [Tree(Lemma('make.v.01.make'), ['made']), Tree(Lemma('transport.v.02.carry'), ['carry'])] \n",
      "\n",
      "\n",
      "sentence 56 : \n",
      " [['But'], Tree(Lemma('person.n.01.person'), [Tree('NE', ['Duclos'])]), Tree('think.v.1;2', ['thought']), ['he'], Tree(Lemma('visualize.v.01.see'), ['saw']), ['a'], Tree(Lemma('exit.n.01.way_out'), ['way', 'out']), ['.']] \n",
      "\n",
      "3 lemma-annotated tokens :\n",
      " [Tree('think.v.1;2', ['thought']), Tree(Lemma('visualize.v.01.see'), ['saw']), Tree(Lemma('exit.n.01.way_out'), ['way', 'out'])] \n",
      "\n",
      "\n",
      "sentence 57 : \n",
      " [['Now'], ['he'], ['is'], ['apparently'], Tree(Lemma('ask.v.04.expect'), ['expected']), ['to'], Tree(Lemma('spare.v.03.give_up'), ['give', 'up']), ['his'], ['evenings'], ['-'], ['and'], ['Sundays'], [','], ['too'], [','], ['for'], ['this'], ['is'], Tree(Lemma('come.v.03.come'), ['coming']), ['.']] \n",
      "\n",
      "3 lemma-annotated tokens :\n",
      " [Tree(Lemma('ask.v.04.expect'), ['expected']), Tree(Lemma('spare.v.03.give_up'), ['give', 'up']), Tree(Lemma('come.v.03.come'), ['coming'])] \n",
      "\n",
      "\n",
      "sentence 58 : \n",
      " [['The'], ['striptease'], Tree(Lemma('be.v.01.be'), ['is']), ['crass'], [';'], ['the'], ['belly', 'dance'], Tree(Lemma('leave.v.06.leave'), ['leaves']), ['more'], ['to'], ['the'], ['imagination'], ['.']] \n",
      "\n",
      "2 lemma-annotated tokens :\n",
      " [Tree(Lemma('be.v.01.be'), ['is']), Tree(Lemma('leave.v.06.leave'), ['leaves'])] \n",
      "\n",
      "\n",
      "sentence 59 : \n",
      " [['But'], Tree(Lemma('person.n.01.person'), [Tree('NE', ['Michael', 'Sept'])]), ['had'], Tree(Lemma('unmask.v.01.unmask'), ['unmasked']), ['him'], [','], Tree(Lemma('uncover.v.01.reveal'), ['revealing']), ['he'], ['had'], Tree(Lemma('never.r.01.never'), ['never']), Tree(Lemma('be.v.02.be'), ['been']), ['a'], Tree(Lemma('bishop.n.01.bishop'), ['bishop']), [','], ['but'], Tree(Lemma('be.v.02.be'), ['was']), ['an'], Tree(Lemma('anabaptist.n.01.Anabaptist'), ['Anabaptist']), [','], Tree(Lemma('afraid.a.01.afraid'), ['afraid']), ['to'], Tree(Lemma('state.v.01.state'), ['state']), ['his'], Tree(Lemma('religion.n.01.faith'), ['faith']), [','], ['because'], ['he'], Tree(Lemma('know.v.01.know'), ['knew']), Tree(Lemma('person.n.01.person'), [Tree('NE', ['John', 'Calvin'])]), ['had'], Tree(Lemma('publish.v.03.write'), ['written']), ['a'], Tree(Lemma('book.n.01.book'), ['book']), ['against'], ['their'], Tree('belief.n.00', ['belief']), ['that'], ['the'], Tree(Lemma('soul.n.01.soul'), ['soul']), Tree(Lemma('sleep.v.01.sleep'), ['slept']), ['after'], Tree(Lemma('death.n.04.death'), ['death']), ['.']] \n",
      "\n",
      "17 lemma-annotated tokens :\n",
      " [Tree(Lemma('unmask.v.01.unmask'), ['unmasked']), Tree(Lemma('uncover.v.01.reveal'), ['revealing']), Tree(Lemma('never.r.01.never'), ['never']), Tree(Lemma('be.v.02.be'), ['been']), Tree(Lemma('bishop.n.01.bishop'), ['bishop']), Tree(Lemma('be.v.02.be'), ['was']), Tree(Lemma('anabaptist.n.01.Anabaptist'), ['Anabaptist']), Tree(Lemma('afraid.a.01.afraid'), ['afraid']), Tree(Lemma('state.v.01.state'), ['state']), Tree(Lemma('religion.n.01.faith'), ['faith']), Tree(Lemma('know.v.01.know'), ['knew']), Tree(Lemma('publish.v.03.write'), ['written']), Tree(Lemma('book.n.01.book'), ['book']), Tree('belief.n.00', ['belief']), Tree(Lemma('soul.n.01.soul'), ['soul']), Tree(Lemma('sleep.v.01.sleep'), ['slept']), Tree(Lemma('death.n.04.death'), ['death'])] \n",
      "\n",
      "\n",
      "sentence 60 : \n",
      " [['Since'], ['the'], Tree('circulating.s.00', ['circulating']), Tree(Lemma('thyroid_hormone.n.01.thyroid_hormone'), ['thyroid', 'hormones']), Tree(Lemma('be.v.02.be'), ['are']), ['the'], Tree(Lemma('amino_acid.n.01.amino_acid'), ['amino', 'acids']), Tree(Lemma('thyroxine.n.01.thyroxine'), ['thyroxine']), ['and'], Tree(Lemma('tri-iodothyronine.n.01.tri-iodothyronine'), ['tri-iodothyronine']), ['('], ['cf.'], Tree(Lemma('section.n.01.section'), ['Section']), ['C'], [')'], [','], ['it'], Tree(Lemma('be.v.01.be'), ['is']), Tree(Lemma('clear.a.01.clear'), ['clear']), ['that'], ['some'], Tree(Lemma('mechanism.n.03.mechanism'), ['mechanism']), ['must'], Tree(Lemma('exist.v.01.exist'), ['exist']), ['in'], ['the'], Tree(Lemma('thyroid_gland.n.01.thyroid_gland'), ['thyroid', 'gland']), ['for'], ['their'], Tree(Lemma('release.n.03.release'), ['release']), ['from'], Tree(Lemma('protein.n.01.protein'), ['proteins']), ['before'], Tree(Lemma('secretion.n.01.secretion'), ['secretion']), ['.']] \n",
      "\n",
      "15 lemma-annotated tokens :\n",
      " [Tree('circulating.s.00', ['circulating']), Tree(Lemma('thyroid_hormone.n.01.thyroid_hormone'), ['thyroid', 'hormones']), Tree(Lemma('be.v.02.be'), ['are']), Tree(Lemma('amino_acid.n.01.amino_acid'), ['amino', 'acids']), Tree(Lemma('thyroxine.n.01.thyroxine'), ['thyroxine']), Tree(Lemma('tri-iodothyronine.n.01.tri-iodothyronine'), ['tri-iodothyronine']), Tree(Lemma('section.n.01.section'), ['Section']), Tree(Lemma('be.v.01.be'), ['is']), Tree(Lemma('clear.a.01.clear'), ['clear']), Tree(Lemma('mechanism.n.03.mechanism'), ['mechanism']), Tree(Lemma('exist.v.01.exist'), ['exist']), Tree(Lemma('thyroid_gland.n.01.thyroid_gland'), ['thyroid', 'gland']), Tree(Lemma('release.n.03.release'), ['release']), Tree(Lemma('protein.n.01.protein'), ['proteins']), Tree(Lemma('secretion.n.01.secretion'), ['secretion'])] \n",
      "\n",
      "\n",
      "Total sentences in dict: 195\n",
      "Total items of Lemma('have.v.02.have') : 6\n",
      "Total items of Lemma('lean.v.01.tilt') : 4\n",
      "Total items of Lemma('let.v.01.allow') : 4\n",
      "Total items of Lemma('make.v.01.do') : 8\n",
      "Total items of Lemma('love.v.01.love') : 4\n",
      "Total items of Lemma('country.n.02.country') : 4\n",
      "Total items of Lemma('believe.v.03.believe') : 4\n",
      "Total items of Lemma('someday.r.01.someday') : 4\n",
      "Total items of Lemma('win.v.01.win') : 4\n",
      "Total items of Lemma('struggle.n.01.battle') : 4\n",
      "Total items of Lemma('equality.n.01.equality') : 4\n",
      "Total items of Lemma('state.v.01.say') : 6\n",
      "Total items of Lemma('be.v.01.be') : 50\n",
      "Total items of Lemma('happen.v.01.happen') : 4\n",
      "Total items of Lemma('wonder.v.02.wonder') : 4\n",
      "Total items of Lemma('applicability.n.01.applicability') : 4\n",
      "Total items of Lemma('people.n.01.people') : 4\n",
      "Total items of Lemma('make.v.01.make') : 4\n",
      "Total items of Lemma('transport.v.02.carry') : 4\n",
      "Total items of think.v.1;2 : 4\n",
      "Total items of Lemma('visualize.v.01.see') : 6\n",
      "Total items of Lemma('ask.v.04.expect') : 6\n",
      "Total items of Lemma('come.v.03.come') : 7\n",
      "Total items of Lemma('leave.v.06.leave') : 6\n",
      "Total items of Lemma('unmask.v.01.unmask') : 4\n",
      "Total items of Lemma('uncover.v.01.reveal') : 4\n",
      "Total items of Lemma('never.r.01.never') : 6\n",
      "Total items of Lemma('bishop.n.01.bishop') : 4\n",
      "Total items of Lemma('anabaptist.n.01.Anabaptist') : 4\n",
      "Total items of Lemma('afraid.a.01.afraid') : 4\n",
      "Total items of Lemma('state.v.01.state') : 4\n",
      "Total items of Lemma('religion.n.01.faith') : 4\n",
      "Total items of Lemma('know.v.01.know') : 8\n",
      "Total items of Lemma('publish.v.03.write') : 6\n",
      "Total items of Lemma('book.n.01.book') : 4\n",
      "Total items of belief.n.00 : 4\n",
      "Total items of Lemma('soul.n.01.soul') : 4\n",
      "Total items of Lemma('sleep.v.01.sleep') : 4\n",
      "Total items of Lemma('death.n.04.death') : 4\n",
      "Total items of circulating.s.00 : 4\n",
      "Total items of Lemma('thyroxine.n.01.thyroxine') : 4\n",
      "Total items of Lemma('tri-iodothyronine.n.01.tri-iodothyronine') : 5\n",
      "Total items of Lemma('section.n.01.section') : 4\n",
      "Total items of Lemma('clear.a.01.clear') : 4\n",
      "Total items of Lemma('mechanism.n.03.mechanism') : 4\n",
      "Total items of Lemma('exist.v.01.exist') : 4\n",
      "Total items of Lemma('release.n.03.release') : 4\n",
      "Total items of Lemma('protein.n.01.protein') : 4\n",
      "Total items of Lemma('secretion.n.01.secretion') : 5\n",
      "Total items of Lemma('reduce.v.01.cut') : 2\n",
      "Total items of Lemma('necessitate.v.01.need') : 6\n",
      "Total items of Lemma('pare.v.01.pare') : 2\n",
      "Total items of Lemma('yankee.n.01.Yankee') : 2\n",
      "Total items of Lemma('sergeant.n.01.sergeant') : 2\n",
      "Total items of Lemma('yield.v.01.give') : 6\n",
      "Total items of following.s.01 : 2\n",
      "Total items of Lemma('description.n.01.description') : 2\n",
      "Total items of Lemma('sweetheart.n.01.sweetheart') : 2\n",
      "Total items of Lemma('girlfriend.n.02.girl') : 6\n",
      "Total items of Lemma('jerkwater.s.01.one-horse') : 2\n",
      "Total items of Lemma('kill.v.01.kill') : 2\n",
      "Total items of Lemma('exist.v.02.live') : 2\n",
      "Total items of Lemma('name.n.01.name') : 2\n",
      "Total items of Lemma('use.v.01.employ') : 2\n",
      "Total items of Lemma('desire.v.01.want') : 2\n",
      "Total items of Lemma('help.v.01.help') : 2\n",
      "Total items of Lemma('prevent.v.01.prevent') : 2\n",
      "Total items of Lemma('originate.v.01.arise') : 2\n",
      "Total items of Lemma('play.v.10.play') : 4\n",
      "Total items of Lemma('arrange.v.01.arrange') : 2\n",
      "Total items of Lemma('earlier.r.01.before') : 2\n",
      "Total items of Lemma('start.v.06.start') : 4\n",
      "Total items of Lemma('aim.n.02.objective') : 2\n",
      "Total items of Lemma('organization.n.01.organization') : 2\n",
      "Total items of Lemma('list.n.01.list') : 2\n",
      "Total items of Lemma('necessity.n.02.requirement') : 2\n",
      "Total items of Lemma('type.n.01.type') : 2\n",
      "Total items of Lemma('site.n.01.site') : 2\n",
      "Total items of Lemma('most.a.01.most') : 4\n",
      "Total items of Lemma('belief.n.01.belief') : 2\n",
      "Total items of Lemma('still.r.01.still') : 2\n",
      "Total items of Lemma('unsettled.a.01.unsettled') : 2\n",
      "Total items of Lemma('not.r.01.not') : 2\n",
      "Total items of Lemma('killing.n.02.killing') : 3\n",
      "Total items of n't.r.00 : 4\n",
      "Total items of Lemma('jury.n.01.jury') : 2\n",
      "Total items of Lemma('praise.v.01.praise') : 2\n",
      "Total items of Lemma('administration.n.01.administration') : 2\n",
      "Total items of Lemma('operation.n.01.operation') : 2\n",
      "Total items of Lemma('atlanta.n.01.Atlanta') : 2\n",
      "Total items of Lemma('then.r.01.then') : 2\n",
      "Total items of Lemma('new.a.01.new') : 2\n",
      "Total items of Lemma('affluence.n.01.affluence') : 2\n",
      "Total items of Lemma('actually.r.01.actually') : 2\n",
      "Total items of Lemma('wide.a.01.wide') : 5\n",
      "Total items of Lemma('earth.n.01.world') : 2\n",
      "Total items of Lemma('precinct.n.01.precinct') : 2\n",
      "Total items of Lemma('slightly.r.01.somewhat') : 4\n",
      "Total items of Lemma('surprise.n.01.surprise') : 2\n",
      "Total items of Lemma('detect.v.01.find') : 6\n",
      "Total items of one.s.00 : 2\n",
      "Total items of Lemma('consider.v.03.consider') : 2\n",
      "Total items of Lemma('occupation.n.01.job') : 4\n",
      "Total items of Lemma('look.v.02.appear') : 2\n",
      "Total items of Lemma('mousy.s.01.mousy') : 2\n",
      "Total items of Lemma('besides.r.02.also') : 2\n",
      "Total items of Lemma('office.n.01.office') : 2\n",
      "Total items of Lemma('relatively.r.01.relatively') : 2\n",
      "Total items of Lemma('short.a.01.short') : 2\n",
      "Total items of Lemma('time.n.02.time') : 4\n",
      "Total items of Lemma('outstanding.s.01.outstanding') : 2\n",
      "Total items of role.n.2;1 : 2\n",
      "Total items of Lemma('receptionist.n.01.receptionist') : 2\n",
      "Total items of show.v.6;4 : 2\n",
      "Total items of Lemma('wit.n.01.wit') : 2\n",
      "Total items of Lemma('sparkle.n.01.sparkle') : 2\n",
      "Total items of Lemma('aplomb.n.01.aplomb') : 2\n",
      "Total items of Lemma('whole.a.01.whole') : 2\n",
      "Total items of Lemma('sky.n.01.sky') : 2\n",
      "Total items of Lemma('become.v.02.become') : 2\n",
      "Total items of Lemma('attach.v.03.attach') : 2\n",
      "Total items of Lemma('bottom.n.02.bottom') : 2\n",
      "Total items of Lemma('hill.n.01.hill') : 2\n",
      "Total items of Lemma('conduct.v.02.direct') : 2\n",
      "Total items of Lemma('support.n.04.reinforcement') : 2\n",
      "Total items of Lemma('battle.n.01.fight') : 2\n",
      "Total items of Lemma('tell.v.03.tell') : 2\n",
      "Total items of Lemma('spend.v.01.spend') : 2\n",
      "Total items of Lemma('get.v.01.get') : 2\n",
      "Total items of Lemma('like.v.02.like') : 2\n",
      "Total items of Lemma('same.a.01.same') : 2\n",
      "Total items of Lemma('room.n.01.room') : 2\n",
      "Total items of Lemma('buy.v.01.buy') : 2\n",
      "Total items of Lemma('feel.v.01.feel') : 4\n",
      "Total items of Lemma('travel.v.01.go') : 2\n",
      "Total items of Lemma('avoid.v.01.avoid') : 2\n",
      "Total items of Lemma('frame.v.04.put') : 2\n",
      "Total items of Lemma('keep.v.01.keep') : 2\n",
      "Total items of Lemma('end.v.01.end') : 2\n",
      "Total items of Lemma('catch.v.01.catch') : 2\n",
      "Total items of Lemma('one.s.01.one') : 2\n",
      "Total items of more.s.00 : 2\n",
      "Total items of Lemma('down.a.01.down') : 2\n",
      "Total items of Lemma('here.r.01.here') : 2\n",
      "Total items of Lemma('stocking.n.01.stocking') : 2\n",
      "Total items of Lemma('modern.a.01.modern') : 2\n",
      "Total items of Lemma('condition.n.01.condition') : 2\n",
      "Total items of Lemma('particularly.r.01.especially') : 2\n",
      "Total items of Lemma('true.a.01.true') : 2\n",
      "Total items of Lemma('ready.a.01.ready') : 2\n",
      "Total items of Lemma('militia.n.01.reserves') : 2\n",
      "Total items of Lemma('assume.v.05.take') : 6\n",
      "Total items of Lemma('primary.a.01.primary') : 2\n",
      "Total items of Lemma('duty.n.01.responsibility') : 2\n",
      "Total items of Lemma('capital.n.06.Washington') : 2\n",
      "Total items of Lemma('follow.v.03.follow') : 3\n",
      "Total items of Lemma('ambassador.n.01.ambassador') : 2\n",
      "Total items of Lemma('overseas.r.02.overseas') : 2\n",
      "Total items of Lemma('sing.v.01.sing') : 2\n",
      "Total items of Lemma('fail.v.01.fail') : 2\n",
      "Total items of Lemma('try.v.01.try') : 2\n",
      "Total items of Lemma('happily.r.01.blithely') : 2\n",
      "Total items of Lemma('hear.v.01.hear') : 2\n",
      "Total items of Lemma('cry.v.02.weep') : 3\n",
      "Total items of Lemma('indicate.v.03.indicate') : 2\n",
      "Total items of Lemma('collaborate.v.01.cooperate') : 2\n",
      "Total items of Lemma('banish.v.01.banish') : 2\n",
      "Total items of Lemma('debate.n.02.debate') : 2\n",
      "Total items of Lemma('position.n.06.place') : 2\n",
      "Total items of Lemma('influence.n.01.influence') : 2\n",
      "Total items of Lemma('thus.r.02.thus') : 2\n",
      "Total items of Lemma('image.n.01.image') : 2\n",
      "Total items of man.n.00 : 2\n",
      "Total items of Lemma('suffer.v.01.suffer') : 2\n",
      "Total items of Lemma('complete.a.01.complete') : 4\n",
      "Total items of Lemma('atomization.n.01.fragmentation') : 2\n",
      "Total items of Lemma('personal.a.01.personal') : 2\n",
      "Total items of Lemma('spiritual.s.03.spiritual') : 2\n",
      "Total items of Lemma('quality.n.01.quality') : 2\n",
      "Total items of Lemma('objectification.n.01.objectification') : 2\n",
      "Total items of Lemma('subhuman.a.01.subhuman') : 2\n",
      "Total items of Lemma('quasi.s.01.quasi') : 2\n",
      "Total items of Lemma('mechanistic.s.01.mechanistic') : 2\n",
      "Total items of Lemma('ability.n.02.power') : 2\n",
      "Total items of Lemma('workplace.n.01.work') : 3\n",
      "Total items of Lemma('today.r.02.today') : 2\n",
      "Total items of Lemma('sneak.v.01.sneak') : 2\n",
      "Total items of Lemma('about.r.04.around') : 2\n",
      "Total items of Lemma('spy.v.02.spy') : 2\n",
      "Total items of Lemma('house.n.01.house') : 2\n",
      "Total items of Lemma('try.v.01.attempt') : 2\n",
      "Total items of Lemma('add.v.01.add') : 3\n",
      "Total items of Lemma('include.v.01.include') : 2\n",
      "Total items of Lemma('notch.v.01.notch') : 2\n",
      "Total items of Lemma('fasten.v.01.secure') : 2\n"
     ]
    }
   ],
   "source": [
    "# test function: check multiple vectors per sense\n",
    "new_data = semcor_train[:50] + semcor_train[:10]\n",
    "len(new_data)\n",
    "\n",
    "store_training_set(new_data)\n",
    "\n",
    "\n",
    "# check total senses in dict\n",
    "print('Total sentences in dict:', len(Train_embeddings))\n",
    "\n",
    "\n",
    "# check total vectors in dict\n",
    "count = 1\n",
    "for key, value in Train_embeddings.items(): \n",
    "    print('Total items of', key, ':', len(Train_embeddings[key]))\n",
    "    count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([Lemma('have.v.02.have'), Lemma('lean.v.01.tilt'), Lemma('let.v.01.allow'), Lemma('make.v.01.do'), Lemma('love.v.01.love'), Lemma('country.n.02.country'), Lemma('believe.v.03.believe'), Lemma('someday.r.01.someday'), Lemma('win.v.01.win'), Lemma('struggle.n.01.battle'), Lemma('equality.n.01.equality'), Lemma('state.v.01.say'), Lemma('be.v.01.be'), Lemma('happen.v.01.happen'), Lemma('wonder.v.02.wonder'), Lemma('applicability.n.01.applicability'), Lemma('people.n.01.people'), Lemma('make.v.01.make'), Lemma('transport.v.02.carry'), 'think.v.1;2', Lemma('visualize.v.01.see'), Lemma('ask.v.04.expect'), Lemma('come.v.03.come'), Lemma('leave.v.06.leave'), Lemma('unmask.v.01.unmask'), Lemma('uncover.v.01.reveal'), Lemma('never.r.01.never'), Lemma('bishop.n.01.bishop'), Lemma('anabaptist.n.01.Anabaptist'), Lemma('afraid.a.01.afraid'), Lemma('state.v.01.state'), Lemma('religion.n.01.faith'), Lemma('know.v.01.know'), Lemma('publish.v.03.write'), Lemma('book.n.01.book'), 'belief.n.00', Lemma('soul.n.01.soul'), Lemma('sleep.v.01.sleep'), Lemma('death.n.04.death'), 'circulating.s.00', Lemma('thyroxine.n.01.thyroxine'), Lemma('tri-iodothyronine.n.01.tri-iodothyronine'), Lemma('section.n.01.section'), Lemma('clear.a.01.clear'), Lemma('mechanism.n.03.mechanism'), Lemma('exist.v.01.exist'), Lemma('release.n.03.release'), Lemma('protein.n.01.protein'), Lemma('secretion.n.01.secretion'), Lemma('reduce.v.01.cut'), Lemma('necessitate.v.01.need'), Lemma('pare.v.01.pare'), Lemma('yankee.n.01.Yankee'), Lemma('sergeant.n.01.sergeant'), Lemma('yield.v.01.give'), 'following.s.01', Lemma('description.n.01.description'), Lemma('sweetheart.n.01.sweetheart'), Lemma('girlfriend.n.02.girl'), Lemma('jerkwater.s.01.one-horse'), Lemma('kill.v.01.kill'), Lemma('exist.v.02.live'), Lemma('name.n.01.name'), Lemma('use.v.01.employ'), Lemma('desire.v.01.want'), Lemma('help.v.01.help'), Lemma('prevent.v.01.prevent'), Lemma('originate.v.01.arise'), Lemma('play.v.10.play'), Lemma('arrange.v.01.arrange'), Lemma('earlier.r.01.before'), Lemma('start.v.06.start'), Lemma('aim.n.02.objective'), Lemma('organization.n.01.organization'), Lemma('list.n.01.list'), Lemma('necessity.n.02.requirement'), Lemma('type.n.01.type'), Lemma('site.n.01.site'), Lemma('most.a.01.most'), Lemma('belief.n.01.belief'), Lemma('still.r.01.still'), Lemma('unsettled.a.01.unsettled'), Lemma('not.r.01.not'), Lemma('killing.n.02.killing'), \"n't.r.00\", Lemma('jury.n.01.jury'), Lemma('praise.v.01.praise'), Lemma('administration.n.01.administration'), Lemma('operation.n.01.operation'), Lemma('atlanta.n.01.Atlanta'), Lemma('then.r.01.then'), Lemma('new.a.01.new'), Lemma('affluence.n.01.affluence'), Lemma('actually.r.01.actually'), Lemma('wide.a.01.wide'), Lemma('earth.n.01.world'), Lemma('precinct.n.01.precinct'), Lemma('slightly.r.01.somewhat'), Lemma('surprise.n.01.surprise'), Lemma('detect.v.01.find'), 'one.s.00', Lemma('consider.v.03.consider'), Lemma('occupation.n.01.job'), Lemma('look.v.02.appear'), Lemma('mousy.s.01.mousy'), Lemma('besides.r.02.also'), Lemma('office.n.01.office'), Lemma('relatively.r.01.relatively'), Lemma('short.a.01.short'), Lemma('time.n.02.time'), Lemma('outstanding.s.01.outstanding'), 'role.n.2;1', Lemma('receptionist.n.01.receptionist'), 'show.v.6;4', Lemma('wit.n.01.wit'), Lemma('sparkle.n.01.sparkle'), Lemma('aplomb.n.01.aplomb'), Lemma('whole.a.01.whole'), Lemma('sky.n.01.sky'), Lemma('become.v.02.become'), Lemma('attach.v.03.attach'), Lemma('bottom.n.02.bottom'), Lemma('hill.n.01.hill'), Lemma('conduct.v.02.direct'), Lemma('support.n.04.reinforcement'), Lemma('battle.n.01.fight'), Lemma('tell.v.03.tell'), Lemma('spend.v.01.spend'), Lemma('get.v.01.get'), Lemma('like.v.02.like'), Lemma('same.a.01.same'), Lemma('room.n.01.room'), Lemma('buy.v.01.buy'), Lemma('feel.v.01.feel'), Lemma('travel.v.01.go'), Lemma('avoid.v.01.avoid'), Lemma('frame.v.04.put'), Lemma('keep.v.01.keep'), Lemma('end.v.01.end'), Lemma('catch.v.01.catch'), Lemma('one.s.01.one'), 'more.s.00', Lemma('down.a.01.down'), Lemma('here.r.01.here'), Lemma('stocking.n.01.stocking'), Lemma('modern.a.01.modern'), Lemma('condition.n.01.condition'), Lemma('particularly.r.01.especially'), Lemma('true.a.01.true'), Lemma('ready.a.01.ready'), Lemma('militia.n.01.reserves'), Lemma('assume.v.05.take'), Lemma('primary.a.01.primary'), Lemma('duty.n.01.responsibility'), Lemma('capital.n.06.Washington'), Lemma('follow.v.03.follow'), Lemma('ambassador.n.01.ambassador'), Lemma('overseas.r.02.overseas'), Lemma('sing.v.01.sing'), Lemma('fail.v.01.fail'), Lemma('try.v.01.try'), Lemma('happily.r.01.blithely'), Lemma('hear.v.01.hear'), Lemma('cry.v.02.weep'), Lemma('indicate.v.03.indicate'), Lemma('collaborate.v.01.cooperate'), Lemma('banish.v.01.banish'), Lemma('debate.n.02.debate'), Lemma('position.n.06.place'), Lemma('influence.n.01.influence'), Lemma('thus.r.02.thus'), Lemma('image.n.01.image'), 'man.n.00', Lemma('suffer.v.01.suffer'), Lemma('complete.a.01.complete'), Lemma('atomization.n.01.fragmentation'), Lemma('personal.a.01.personal'), Lemma('spiritual.s.03.spiritual'), Lemma('quality.n.01.quality'), Lemma('objectification.n.01.objectification'), Lemma('subhuman.a.01.subhuman'), Lemma('quasi.s.01.quasi'), Lemma('mechanistic.s.01.mechanistic'), Lemma('ability.n.02.power'), Lemma('workplace.n.01.work'), Lemma('today.r.02.today'), Lemma('sneak.v.01.sneak'), Lemma('about.r.04.around'), Lemma('spy.v.02.spy'), Lemma('house.n.01.house'), Lemma('try.v.01.attempt'), Lemma('add.v.01.add'), Lemma('include.v.01.include'), Lemma('notch.v.01.notch'), Lemma('fasten.v.01.secure')])"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check keys\n",
    "Train_embeddings.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4. Vector averaging (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can calculate the average ELMo vector for each word sense in the training corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process complete train dataset\n",
    "print(\"total sentences:\", len(semcor_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset dicts for testing\n",
    "Train_embeddings=defaultdict(list) \n",
    "Train_counter=defaultdict(list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 0 : \n",
      " [['A'], ['new'], ['radial'], ['drill'], ['press'], ['with'], ['a'], ['16'], ['inch'], ['capacity'], Tree(Lemma('have.v.02.have'), ['has']), ['a'], Tree(Lemma('lean.v.01.tilt'), ['tilting']), ['head'], ['that'], Tree(Lemma('let.v.01.allow'), ['allows']), ['drilling'], ['to'], ['be'], Tree(Lemma('make.v.01.do'), ['done']), ['at'], ['any'], ['angle'], ['.']] \n",
      "\n",
      "4 lemma-annotated tokens :\n",
      " [Tree(Lemma('have.v.02.have'), ['has']), Tree(Lemma('lean.v.01.tilt'), ['tilting']), Tree(Lemma('let.v.01.allow'), ['allows']), Tree(Lemma('make.v.01.do'), ['done'])] \n",
      "\n",
      "\n",
      "sentence 1 : \n",
      " [['I'], ['used', 'to'], Tree(Lemma('love.v.01.love'), ['love']), ['this'], Tree(Lemma('country.n.02.country'), ['country']), ['and'], Tree(Lemma('believe.v.03.believe'), ['believe']), ['that'], Tree(Lemma('someday.r.01.someday'), ['someday']), ['we'], [\"'d\"], Tree(Lemma('win.v.01.win'), ['win']), ['our'], Tree(Lemma('struggle.n.01.battle'), ['battle']), ['for'], Tree(Lemma('equality.n.01.equality'), ['equality']), ['.']] \n",
      "\n",
      "7 lemma-annotated tokens :\n",
      " [Tree(Lemma('love.v.01.love'), ['love']), Tree(Lemma('country.n.02.country'), ['country']), Tree(Lemma('believe.v.03.believe'), ['believe']), Tree(Lemma('someday.r.01.someday'), ['someday']), Tree(Lemma('win.v.01.win'), ['win']), Tree(Lemma('struggle.n.01.battle'), ['battle']), Tree(Lemma('equality.n.01.equality'), ['equality'])] \n",
      "\n",
      "\n",
      "sentence 2 : \n",
      " [['A'], ['new'], ['radial'], ['drill'], ['press'], ['with'], ['a'], ['16'], ['inch'], ['capacity'], Tree(Lemma('have.v.02.have'), ['has']), ['a'], Tree(Lemma('lean.v.01.tilt'), ['tilting']), ['head'], ['that'], Tree(Lemma('let.v.01.allow'), ['allows']), ['drilling'], ['to'], ['be'], Tree(Lemma('make.v.01.do'), ['done']), ['at'], ['any'], ['angle'], ['.']] \n",
      "\n",
      "4 lemma-annotated tokens :\n",
      " [Tree(Lemma('have.v.02.have'), ['has']), Tree(Lemma('lean.v.01.tilt'), ['tilting']), Tree(Lemma('let.v.01.allow'), ['allows']), Tree(Lemma('make.v.01.do'), ['done'])] \n",
      "\n",
      "\n",
      "sentence 3 : \n",
      " [['I'], ['used', 'to'], Tree(Lemma('love.v.01.love'), ['love']), ['this'], Tree(Lemma('country.n.02.country'), ['country']), ['and'], Tree(Lemma('believe.v.03.believe'), ['believe']), ['that'], Tree(Lemma('someday.r.01.someday'), ['someday']), ['we'], [\"'d\"], Tree(Lemma('win.v.01.win'), ['win']), ['our'], Tree(Lemma('struggle.n.01.battle'), ['battle']), ['for'], Tree(Lemma('equality.n.01.equality'), ['equality']), ['.']] \n",
      "\n",
      "7 lemma-annotated tokens :\n",
      " [Tree(Lemma('love.v.01.love'), ['love']), Tree(Lemma('country.n.02.country'), ['country']), Tree(Lemma('believe.v.03.believe'), ['believe']), Tree(Lemma('someday.r.01.someday'), ['someday']), Tree(Lemma('win.v.01.win'), ['win']), Tree(Lemma('struggle.n.01.battle'), ['battle']), Tree(Lemma('equality.n.01.equality'), ['equality'])] \n",
      "\n",
      "\n",
      "ADD\n",
      "ADD\n",
      "ADD\n",
      "ADD\n",
      "ADD\n",
      "ADD\n",
      "ADD\n",
      "ADD\n",
      "ADD\n",
      "ADD\n",
      "ADD\n",
      "ADD\n",
      "ADD\n",
      "ADD\n",
      "ADD\n",
      "ADD\n",
      "ADD\n",
      "ADD\n",
      "ADD\n",
      "ADD\n",
      "ADD\n",
      "ADD\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {Lemma('have.v.02.have'): [tensor([ 0.0837,  0.3644, -0.0595,  ...,  0.3183,  0.1400,  0.7171],\n",
       "                     grad_fn=<SelectBackward>),\n",
       "              [tensor([ 0.0837,  0.3644, -0.0595,  ...,  0.3183,  0.1400,  0.7171],\n",
       "                      grad_fn=<SelectBackward>)],\n",
       "              [tensor([ 0.0837,  0.3644, -0.0595,  ...,  0.3183,  0.1400,  0.7171],\n",
       "                      grad_fn=<SelectBackward>)],\n",
       "              [tensor([ 0.0837,  0.3644, -0.0595,  ...,  0.3183,  0.1400,  0.7171],\n",
       "                      grad_fn=<SelectBackward>)]],\n",
       "             Lemma('lean.v.01.tilt'): [tensor([-0.0495,  0.4396,  0.4416,  ...,  0.4333,  0.3886,  0.3021],\n",
       "                     grad_fn=<SelectBackward>),\n",
       "              [tensor([-0.0495,  0.4396,  0.4416,  ...,  0.4333,  0.3886,  0.3021],\n",
       "                      grad_fn=<SelectBackward>)],\n",
       "              [tensor([-0.0495,  0.4396,  0.4416,  ...,  0.4333,  0.3886,  0.3021],\n",
       "                      grad_fn=<SelectBackward>)],\n",
       "              [tensor([-0.0495,  0.4396,  0.4416,  ...,  0.4333,  0.3886,  0.3021],\n",
       "                      grad_fn=<SelectBackward>)]],\n",
       "             Lemma('let.v.01.allow'): [tensor([0.2064, 0.3203, 0.0098,  ..., 0.1969, 0.2769, 0.4351],\n",
       "                     grad_fn=<SelectBackward>),\n",
       "              [tensor([0.2064, 0.3203, 0.0098,  ..., 0.1969, 0.2769, 0.4351],\n",
       "                      grad_fn=<SelectBackward>)],\n",
       "              [tensor([0.2064, 0.3203, 0.0098,  ..., 0.1969, 0.2769, 0.4351],\n",
       "                      grad_fn=<SelectBackward>)],\n",
       "              [tensor([0.2064, 0.3203, 0.0098,  ..., 0.1969, 0.2769, 0.4351],\n",
       "                      grad_fn=<SelectBackward>)]],\n",
       "             Lemma('make.v.01.do'): [tensor([ 0.3732, -0.1597,  0.0891,  ...,  0.0447, -0.0036,  0.2702],\n",
       "                     grad_fn=<SelectBackward>),\n",
       "              [tensor([ 0.3732, -0.1597,  0.0891,  ...,  0.0447, -0.0036,  0.2702],\n",
       "                      grad_fn=<SelectBackward>)],\n",
       "              [tensor([ 0.3732, -0.1597,  0.0891,  ...,  0.0447, -0.0036,  0.2702],\n",
       "                      grad_fn=<SelectBackward>)],\n",
       "              [tensor([ 0.3732, -0.1597,  0.0891,  ...,  0.0447, -0.0036,  0.2702],\n",
       "                      grad_fn=<SelectBackward>)]],\n",
       "             Lemma('love.v.01.love'): [tensor([-0.2136, -0.1949,  0.3466,  ...,  0.2277,  0.2227,  0.4123],\n",
       "                     grad_fn=<SelectBackward>),\n",
       "              [tensor([-0.2136, -0.1949,  0.3466,  ...,  0.2277,  0.2227,  0.4123],\n",
       "                      grad_fn=<SelectBackward>)],\n",
       "              [tensor([-0.2136, -0.1949,  0.3466,  ...,  0.2277,  0.2227,  0.4123],\n",
       "                      grad_fn=<SelectBackward>)],\n",
       "              [tensor([-0.2136, -0.1949,  0.3466,  ...,  0.2277,  0.2227,  0.4123],\n",
       "                      grad_fn=<SelectBackward>)]],\n",
       "             Lemma('country.n.02.country'): [tensor([ 0.9978,  0.0687, -0.1779,  ...,  0.3488,  0.4417,  0.6735],\n",
       "                     grad_fn=<SelectBackward>),\n",
       "              [tensor([ 0.9978,  0.0687, -0.1779,  ...,  0.3488,  0.4417,  0.6735],\n",
       "                      grad_fn=<SelectBackward>)],\n",
       "              [tensor([ 0.9978,  0.0687, -0.1779,  ...,  0.3488,  0.4417,  0.6735],\n",
       "                      grad_fn=<SelectBackward>)],\n",
       "              [tensor([ 0.9978,  0.0687, -0.1779,  ...,  0.3488,  0.4417,  0.6735],\n",
       "                      grad_fn=<SelectBackward>)]],\n",
       "             Lemma('believe.v.03.believe'): [tensor([-0.1353,  0.3340,  0.2477,  ...,  0.2441,  0.0528,  0.0603],\n",
       "                     grad_fn=<SelectBackward>),\n",
       "              [tensor([-0.1353,  0.3340,  0.2477,  ...,  0.2441,  0.0528,  0.0603],\n",
       "                      grad_fn=<SelectBackward>)],\n",
       "              [tensor([-0.1353,  0.3340,  0.2477,  ...,  0.2441,  0.0528,  0.0603],\n",
       "                      grad_fn=<SelectBackward>)],\n",
       "              [tensor([-0.1353,  0.3340,  0.2477,  ...,  0.2441,  0.0528,  0.0603],\n",
       "                      grad_fn=<SelectBackward>)]],\n",
       "             Lemma('someday.r.01.someday'): [tensor([ 0.2206,  0.3389, -0.2494,  ...,  0.2325,  0.2580,  0.1345],\n",
       "                     grad_fn=<SelectBackward>),\n",
       "              [tensor([ 0.2206,  0.3389, -0.2494,  ...,  0.2325,  0.2580,  0.1345],\n",
       "                      grad_fn=<SelectBackward>)],\n",
       "              [tensor([ 0.2206,  0.3389, -0.2494,  ...,  0.2325,  0.2580,  0.1345],\n",
       "                      grad_fn=<SelectBackward>)],\n",
       "              [tensor([ 0.2206,  0.3389, -0.2494,  ...,  0.2325,  0.2580,  0.1345],\n",
       "                      grad_fn=<SelectBackward>)]],\n",
       "             Lemma('win.v.01.win'): [tensor([-0.0446, -0.4263,  0.2908,  ...,  0.3318,  0.4306,  0.5099],\n",
       "                     grad_fn=<SelectBackward>),\n",
       "              [tensor([-0.0446, -0.4263,  0.2908,  ...,  0.3318,  0.4306,  0.5099],\n",
       "                      grad_fn=<SelectBackward>)],\n",
       "              [tensor([-0.0446, -0.4263,  0.2908,  ...,  0.3318,  0.4306,  0.5099],\n",
       "                      grad_fn=<SelectBackward>)],\n",
       "              [tensor([-0.0446, -0.4263,  0.2908,  ...,  0.3318,  0.4306,  0.5099],\n",
       "                      grad_fn=<SelectBackward>)]],\n",
       "             Lemma('struggle.n.01.battle'): [tensor([ 0.3646, -0.6239, -0.1404,  ..., -0.2407,  0.1340, -0.1959],\n",
       "                     grad_fn=<SelectBackward>),\n",
       "              [tensor([ 0.3646, -0.6239, -0.1404,  ..., -0.2407,  0.1340, -0.1959],\n",
       "                      grad_fn=<SelectBackward>)],\n",
       "              [tensor([ 0.3646, -0.6239, -0.1404,  ..., -0.2407,  0.1340, -0.1959],\n",
       "                      grad_fn=<SelectBackward>)],\n",
       "              [tensor([ 0.3646, -0.6239, -0.1404,  ..., -0.2407,  0.1340, -0.1959],\n",
       "                      grad_fn=<SelectBackward>)]],\n",
       "             Lemma('equality.n.01.equality'): [tensor([-0.6281,  0.4977,  0.3059,  ...,  0.1842,  0.2236,  0.2044],\n",
       "                     grad_fn=<SelectBackward>),\n",
       "              [tensor([-0.6281,  0.4977,  0.3059,  ...,  0.1842,  0.2236,  0.2044],\n",
       "                      grad_fn=<SelectBackward>)],\n",
       "              [tensor([-0.6281,  0.4977,  0.3059,  ...,  0.1842,  0.2236,  0.2044],\n",
       "                      grad_fn=<SelectBackward>)],\n",
       "              [tensor([-0.6281,  0.4977,  0.3059,  ...,  0.1842,  0.2236,  0.2044],\n",
       "                      grad_fn=<SelectBackward>)]]})"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check addition of multiple vectors\n",
    "new_data = semcor_train[:2] + semcor_train[:2]\n",
    "len(new_data)\n",
    "\n",
    "store_training_set(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('have.v.02.have'), Lemma('lean.v.01.tilt'), Lemma('let.v.01.allow'), Lemma('make.v.01.do'), Lemma('love.v.01.love'), Lemma('country.n.02.country'), Lemma('believe.v.03.believe'), Lemma('someday.r.01.someday'), Lemma('win.v.01.win'), Lemma('struggle.n.01.battle'), Lemma('equality.n.01.equality'), 0]\n",
      "Lemma('have.v.02.have')\n",
      "Lemma('lean.v.01.tilt')\n",
      "Lemma('let.v.01.allow')\n",
      "Lemma('make.v.01.do')\n",
      "Lemma('love.v.01.love')\n",
      "Lemma('country.n.02.country')\n",
      "Lemma('believe.v.03.believe')\n",
      "Lemma('someday.r.01.someday')\n",
      "Lemma('win.v.01.win')\n",
      "Lemma('struggle.n.01.battle')\n",
      "Lemma('equality.n.01.equality')\n"
     ]
    }
   ],
   "source": [
    "list_keys = list(Train_embeddings.keys())\n",
    "print(list_keys)\n",
    "\n",
    "for sense in range(0, len(list_keys)-1):\n",
    "    print(list_keys[sense])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Train_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5c89d25a5bfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mavg_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0maveraging_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Train_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# for each sense, calculate average\n",
    "import torch \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from torch import unsqueeze \n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "Train_avg=defaultdict(list) \n",
    "\n",
    "def averaging_vectors(Train_embeddings):\n",
    "    for sense in range(0, len(list_keys)-1):\n",
    "        \n",
    "        key = list_keys[sense]\n",
    "        total_items = len(Train_embeddings[key])\n",
    "\n",
    "        for i in range(0, total_items):\n",
    "            added_tensor = Train_embeddings[key][i]\n",
    "            current_tensor = torch.empty(1, 1024)\n",
    "            \n",
    "            # extend dimension\n",
    "            addedd_tensor = torch.unsqueeze(added_tensor[0], 0)\n",
    "            \n",
    "            # append at tensor\n",
    "            new_tensor = torch.cat((added_tensor, current_tensor), dim=1)\n",
    "\n",
    "        avg_vector = torch.mean(new_tensor, 0)\n",
    "\n",
    "averaging_vectors(Train_embeddings)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys Train_counter:\n",
      "\n",
      "\n",
      "\n",
      "Keys Train_embeddings:\n",
      "\n",
      "Lemma('have.v.02.have') [tensor([ 0.0837,  0.3644, -0.0595,  ...,  0.3183,  0.1400,  0.7171],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.5255,  0.0992, -0.4613,  ...,  0.3177, -0.0881,  0.3643],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.0888,  0.3886, -0.0399,  ...,  0.3210,  0.1381,  0.7103],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.0964,  0.3704, -0.0487,  ...,  0.3199,  0.1402,  0.7152],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.5319,  0.0918, -0.4677,  ...,  0.3177, -0.0881,  0.3643],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.0888,  0.3886, -0.0399,  ...,  0.3210,  0.1381,  0.7103],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('lean.v.01.tilt') [tensor([-0.0495,  0.4396,  0.4416,  ...,  0.4333,  0.3886,  0.3021],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.0506,  0.4356,  0.4227,  ...,  0.4280,  0.3890,  0.3081],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.0605,  0.4861,  0.4689,  ...,  0.4351,  0.3883,  0.3006],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.0506,  0.4356,  0.4227,  ...,  0.4280,  0.3890,  0.3081],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('let.v.01.allow') [tensor([0.2064, 0.3203, 0.0098,  ..., 0.1969, 0.2769, 0.4351],\n",
      "       grad_fn=<SelectBackward>), [tensor([0.2166, 0.3398, 0.0344,  ..., 0.1947, 0.2791, 0.4380],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.2893,  0.3644, -0.0018,  ...,  0.1977,  0.2756,  0.4353],\n",
      "       grad_fn=<SelectBackward>)], [tensor([0.2166, 0.3398, 0.0344,  ..., 0.1947, 0.2791, 0.4380],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('make.v.01.do') [tensor([ 0.3732, -0.1597,  0.0891,  ...,  0.0447, -0.0036,  0.2702],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.5579,  0.2850, -0.8128,  ...,  0.0746, -0.0704,  0.2711],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.3620,  0.3398, -0.3972,  ..., -0.0672,  0.1984,  0.4396],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.3936, -0.1575,  0.0841,  ...,  0.0474, -0.0042,  0.2704],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.3918, -0.0909,  0.0616,  ...,  0.0602, -0.0043,  0.2708],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.5578,  0.2846, -0.8129,  ...,  0.0746, -0.0704,  0.2711],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.4227,  0.3246, -0.4068,  ..., -0.0735,  0.1952,  0.4388],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.3936, -0.1575,  0.0841,  ...,  0.0474, -0.0042,  0.2704],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('love.v.01.love') [tensor([-0.2136, -0.1949,  0.3466,  ...,  0.2277,  0.2227,  0.4123],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.2100, -0.1833,  0.3570,  ...,  0.2277,  0.2220,  0.4121],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.2035, -0.1869,  0.3516,  ...,  0.2276,  0.2226,  0.4123],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.2100, -0.1833,  0.3570,  ...,  0.2277,  0.2220,  0.4121],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('country.n.02.country') [tensor([ 0.9978,  0.0687, -0.1779,  ...,  0.3488,  0.4417,  0.6735],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.9997,  0.0635, -0.1797,  ...,  0.3488,  0.4416,  0.6737],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.9991,  0.0653, -0.1755,  ...,  0.3494,  0.4425,  0.6734],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.9997,  0.0635, -0.1797,  ...,  0.3488,  0.4416,  0.6737],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('believe.v.03.believe') [tensor([-0.1353,  0.3340,  0.2477,  ...,  0.2441,  0.0528,  0.0603],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.1423,  0.3528,  0.2539,  ...,  0.2439,  0.0531,  0.0609],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1338,  0.3488,  0.2524,  ...,  0.2441,  0.0526,  0.0601],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1423,  0.3528,  0.2539,  ...,  0.2439,  0.0531,  0.0609],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('someday.r.01.someday') [tensor([ 0.2206,  0.3389, -0.2494,  ...,  0.2325,  0.2580,  0.1345],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.2253,  0.3352, -0.2470,  ...,  0.2325,  0.2578,  0.1344],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.2202,  0.3545, -0.2519,  ...,  0.2329,  0.2583,  0.1343],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.2253,  0.3352, -0.2470,  ...,  0.2325,  0.2578,  0.1344],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('win.v.01.win') [tensor([-0.0446, -0.4263,  0.2908,  ...,  0.3318,  0.4306,  0.5099],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.0499, -0.4384,  0.3023,  ...,  0.3370,  0.4259,  0.5096],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.0844, -0.4791,  0.3866,  ...,  0.3325,  0.4240,  0.5039],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.0499, -0.4384,  0.3023,  ...,  0.3370,  0.4259,  0.5096],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('struggle.n.01.battle') [tensor([ 0.3646, -0.6239, -0.1404,  ..., -0.2407,  0.1340, -0.1959],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.3719, -0.5944, -0.1626,  ..., -0.2400,  0.1354, -0.2021],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.3541, -0.5702, -0.2158,  ..., -0.2445,  0.1288, -0.2005],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.3719, -0.5944, -0.1626,  ..., -0.2400,  0.1354, -0.2021],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('equality.n.01.equality') [tensor([-0.6281,  0.4977,  0.3059,  ...,  0.1842,  0.2236,  0.2044],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.6233,  0.4599,  0.3064,  ...,  0.1880,  0.2203,  0.2056],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.6384,  0.4862,  0.3056,  ...,  0.1892,  0.2178,  0.1998],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.6233,  0.4599,  0.3064,  ...,  0.1880,  0.2203,  0.2056],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('state.v.01.say') [tensor([ 0.1546,  0.2591, -0.3077,  ...,  0.4450,  0.1323,  0.2646],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.1310,  0.2609, -0.2910,  ...,  0.4477,  0.1238,  0.2578],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.1537,  0.2606, -0.3015,  ...,  0.4450,  0.1323,  0.2646],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.1545,  0.2603, -0.3046,  ...,  0.4450,  0.1323,  0.2646],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.1310,  0.2609, -0.2910,  ...,  0.4477,  0.1238,  0.2578],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.1537,  0.2606, -0.3015,  ...,  0.4450,  0.1323,  0.2646],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('be.v.01.be') [tensor([-0.0954,  0.1683, -0.0890,  ...,  0.2918,  0.3115,  0.6509],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.1078,  0.0977, -0.1217,  ...,  0.2919,  0.3089,  0.6472],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1439, -0.2165,  0.1552,  ..., -0.0192,  0.4459,  0.7338],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.2049, -0.2543,  0.1271,  ...,  0.1504,  0.3582,  0.6824],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.3899,  0.5283, -0.0260,  ...,  0.3783,  0.3653,  0.2709],\n",
      "       grad_fn=<SelectBackward>), tensor([-0.3747,  1.3241, -0.1249,  ...,  0.0421,  0.0795,  0.2922],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1693, -0.0242,  0.1079,  ...,  0.1660, -0.3251, -0.2945],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1106,  0.1634, -0.1086,  ...,  0.2911,  0.3070,  0.6469],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.2150, -0.1996,  0.1213,  ...,  0.1485,  0.3539,  0.6800],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.2158, -0.2245,  0.1071,  ...,  0.1497,  0.3486,  0.6731],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.1701,  0.2939,  0.3377,  ...,  0.4541, -0.3026,  0.3369],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.2228,  0.1467, -0.5747,  ...,  0.2010,  0.1301,  0.4561],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1818, -0.1938,  0.1119,  ...,  0.1501,  0.3491,  0.6717],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.2205,  0.1840, -0.5597,  ...,  0.1980,  0.1209,  0.4571],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.0369, -0.2135, -0.2669,  ...,  0.0090,  0.3188,  0.5861],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1310, -0.2086,  0.1463,  ..., -0.0175,  0.4477,  0.7318],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.0804,  0.1784, -0.1248,  ...,  0.2902,  0.3077,  0.6540],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1221,  0.1605, -0.0885,  ...,  0.2882,  0.3055,  0.6468],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1672, -0.2741,  0.0886,  ...,  0.1492,  0.3515,  0.6751],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.0954,  0.1219, -0.1254,  ...,  0.2892,  0.3080,  0.6515],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.0961,  0.1666, -0.0893,  ...,  0.2918,  0.3115,  0.6509],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1078,  0.0979, -0.1217,  ...,  0.2919,  0.3089,  0.6472],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1437, -0.2186,  0.1551,  ..., -0.0192,  0.4459,  0.7337],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.2020, -0.2494,  0.1266,  ...,  0.1504,  0.3583,  0.6824],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.3899,  0.5283, -0.0260,  ...,  0.3783,  0.3653,  0.2709],\n",
      "       grad_fn=<SelectBackward>), tensor([-0.3747,  1.3241, -0.1249,  ...,  0.0421,  0.0795,  0.2922],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1677, -0.0270,  0.1090,  ...,  0.1660, -0.3251, -0.2945],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.0984,  0.1660, -0.0882,  ...,  0.2918,  0.3116,  0.6509],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1078,  0.0978, -0.1217,  ...,  0.2919,  0.3089,  0.6472],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1426, -0.2151,  0.1551,  ..., -0.0192,  0.4459,  0.7338],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.2011, -0.2492,  0.1279,  ...,  0.1505,  0.3581,  0.6823],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.3900,  0.5283, -0.0260,  ...,  0.3783,  0.3653,  0.2709],\n",
      "       grad_fn=<SelectBackward>), tensor([-0.3746,  1.3241, -0.1249,  ...,  0.0421,  0.0795,  0.2922],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1684, -0.0266,  0.1102,  ...,  0.1660, -0.3251, -0.2945],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.0940,  0.1641, -0.1123,  ...,  0.2911,  0.3070,  0.6469],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.2150, -0.1996,  0.1213,  ...,  0.1485,  0.3539,  0.6800],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.2158, -0.2245,  0.1071,  ...,  0.1497,  0.3486,  0.6731],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.1701,  0.2939,  0.3377,  ...,  0.4541, -0.3026,  0.3369],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.2228,  0.1467, -0.5747,  ...,  0.2010,  0.1301,  0.4561],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1818, -0.1938,  0.1119,  ...,  0.1501,  0.3491,  0.6717],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.2205,  0.1838, -0.5599,  ...,  0.1980,  0.1209,  0.4571],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.0369, -0.2135, -0.2669,  ...,  0.0090,  0.3188,  0.5861],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1282, -0.2047,  0.1466,  ..., -0.0175,  0.4477,  0.7318],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.0804,  0.1784, -0.1248,  ...,  0.2902,  0.3077,  0.6540],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1221,  0.1605, -0.0885,  ...,  0.2882,  0.3055,  0.6468],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1672, -0.2741,  0.0886,  ...,  0.1492,  0.3515,  0.6751],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.0954,  0.1219, -0.1254,  ...,  0.2892,  0.3080,  0.6515],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.0961,  0.1666, -0.0893,  ...,  0.2918,  0.3115,  0.6509],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1078,  0.0979, -0.1217,  ...,  0.2919,  0.3089,  0.6472],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1437, -0.2186,  0.1551,  ..., -0.0192,  0.4459,  0.7337],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.2020, -0.2494,  0.1266,  ...,  0.1504,  0.3583,  0.6824],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.3899,  0.5283, -0.0260,  ...,  0.3783,  0.3653,  0.2709],\n",
      "       grad_fn=<SelectBackward>), tensor([-0.3747,  1.3241, -0.1249,  ...,  0.0421,  0.0795,  0.2922],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1677, -0.0270,  0.1090,  ...,  0.1660, -0.3251, -0.2945],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('happen.v.01.happen') [tensor([ 0.5875, -0.5491,  0.2992,  ...,  0.1901,  0.3229,  0.2736],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.5839, -0.5367,  0.3016,  ...,  0.1900,  0.3229,  0.2737],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.5847, -0.5402,  0.3013,  ...,  0.1901,  0.3229,  0.2736],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.5839, -0.5367,  0.3016,  ...,  0.1900,  0.3229,  0.2737],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('wonder.v.02.wonder') [tensor([ 0.1614,  0.1324,  0.5064,  ...,  0.1054, -0.0732,  0.5340],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.1598,  0.1361,  0.5051,  ...,  0.1054, -0.0732,  0.5340],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.1606,  0.1345,  0.5056,  ...,  0.1054, -0.0732,  0.5340],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.1598,  0.1361,  0.5051,  ...,  0.1054, -0.0732,  0.5340],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('applicability.n.01.applicability') [tensor([-0.1631,  0.7445,  0.0782,  ...,  0.5830,  0.6875,  0.0325],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.1635,  0.7438,  0.0782,  ...,  0.5830,  0.6875,  0.0325],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1619,  0.7446,  0.0786,  ...,  0.5830,  0.6875,  0.0325],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1635,  0.7438,  0.0782,  ...,  0.5830,  0.6875,  0.0325],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('people.n.01.people') [tensor([-0.2547,  0.0807,  0.0202,  ...,  0.1843,  0.2956,  0.3211],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.2534,  0.0903,  0.0219,  ...,  0.1843,  0.2957,  0.3211],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.2545,  0.0874,  0.0215,  ...,  0.1843,  0.2956,  0.3211],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.2534,  0.0903,  0.0219,  ...,  0.1843,  0.2957,  0.3211],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('make.v.01.make') [tensor([-0.0174,  0.2843, -0.1446,  ...,  0.3221,  0.2353,  0.4381],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.0169,  0.2853, -0.1439,  ...,  0.3221,  0.2353,  0.4381],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.0171,  0.2848, -0.1442,  ...,  0.3221,  0.2353,  0.4381],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.0169,  0.2853, -0.1439,  ...,  0.3221,  0.2353,  0.4381],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('transport.v.02.carry') [tensor([ 0.7139, -0.0221, -0.1033,  ...,  0.1769,  0.0237,  0.3742],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.7140, -0.0220, -0.1036,  ...,  0.1769,  0.0237,  0.3742],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.7148, -0.0222, -0.1021,  ...,  0.1769,  0.0237,  0.3742],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.7140, -0.0220, -0.1036,  ...,  0.1769,  0.0237,  0.3742],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "think.v.1;2 [tensor([ 0.2161, -0.2979,  0.2951,  ...,  0.2348,  0.2348,  0.3163],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.2150, -0.2973,  0.2964,  ...,  0.2348,  0.2348,  0.3163],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.2155, -0.2976,  0.2958,  ...,  0.2348,  0.2348,  0.3163],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.2150, -0.2973,  0.2964,  ...,  0.2348,  0.2348,  0.3163],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('visualize.v.01.see') [tensor([ 0.1838,  0.3323, -0.2819,  ...,  0.0716,  0.1816,  0.2311],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.4400,  0.2770, -0.5024,  ...,  0.2310,  0.0276,  0.0801],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.1838,  0.3320, -0.2824,  ...,  0.0716,  0.1816,  0.2311],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.1834,  0.3319, -0.2827,  ...,  0.0716,  0.1816,  0.2311],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.4400,  0.2770, -0.5024,  ...,  0.2310,  0.0276,  0.0801],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.1838,  0.3320, -0.2824,  ...,  0.0716,  0.1816,  0.2311],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('ask.v.04.expect') [tensor([ 0.2636, -0.2305, -0.0079,  ...,  0.6100,  0.2130,  0.3285],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.0610, -0.1249,  0.7969,  ...,  0.2717,  0.3999,  0.7193],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.2633, -0.2302, -0.0076,  ...,  0.6100,  0.2130,  0.3285],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.2634, -0.2304, -0.0077,  ...,  0.6100,  0.2130,  0.3285],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.0610, -0.1249,  0.7969,  ...,  0.2717,  0.3999,  0.7193],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.2633, -0.2302, -0.0076,  ...,  0.6100,  0.2130,  0.3285],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('come.v.03.come') [tensor([ 0.3271, -0.1683, -0.6302,  ..., -0.0760,  0.0914,  0.5576],\n",
      "       grad_fn=<SelectBackward>), tensor([ 0.1161,  0.2499, -0.4158,  ..., -0.1669, -0.1781,  0.4948],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 1.0388, -0.1711,  0.0208,  ...,  0.2321,  0.3411,  0.4404],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.3269, -0.1686, -0.6302,  ..., -0.0760,  0.0914,  0.5576],\n",
      "       grad_fn=<SelectBackward>), tensor([ 0.1163,  0.2498, -0.4156,  ..., -0.1669, -0.1781,  0.4948],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.3264, -0.1685, -0.6300,  ..., -0.0760,  0.0914,  0.5576],\n",
      "       grad_fn=<SelectBackward>), tensor([ 0.1166,  0.2505, -0.4136,  ..., -0.1669, -0.1781,  0.4948],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 1.0388, -0.1711,  0.0208,  ...,  0.2321,  0.3411,  0.4404],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.3269, -0.1686, -0.6302,  ..., -0.0760,  0.0914,  0.5576],\n",
      "       grad_fn=<SelectBackward>), tensor([ 0.1163,  0.2498, -0.4156,  ..., -0.1669, -0.1781,  0.4948],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('leave.v.06.leave') [tensor([0.1273, 0.0655, 0.2379,  ..., 0.0421, 0.2144, 0.6271],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.3425, -0.4942, -0.1758,  ..., -0.1719,  0.0522,  0.6361],\n",
      "       grad_fn=<SelectBackward>)], [tensor([0.1273, 0.0654, 0.2378,  ..., 0.0421, 0.2144, 0.6271],\n",
      "       grad_fn=<SelectBackward>)], [tensor([0.1268, 0.0653, 0.2376,  ..., 0.0421, 0.2144, 0.6271],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.3425, -0.4942, -0.1758,  ..., -0.1719,  0.0522,  0.6361],\n",
      "       grad_fn=<SelectBackward>)], [tensor([0.1273, 0.0654, 0.2378,  ..., 0.0421, 0.2144, 0.6271],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('unmask.v.01.unmask') [tensor([ 0.7684, -0.0349, -0.2804,  ...,  0.3847, -0.4311, -0.0347],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.7684, -0.0348, -0.2803,  ...,  0.3847, -0.4311, -0.0347],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.7684, -0.0348, -0.2803,  ...,  0.3847, -0.4311, -0.0347],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.7684, -0.0348, -0.2803,  ...,  0.3847, -0.4311, -0.0347],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('uncover.v.01.reveal') [tensor([ 0.3010,  0.3350, -0.4996,  ...,  0.3750, -0.1626,  0.4114],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.3010,  0.3350, -0.4996,  ...,  0.3750, -0.1626,  0.4114],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.3010,  0.3350, -0.4995,  ...,  0.3750, -0.1626,  0.4114],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.3010,  0.3350, -0.4996,  ...,  0.3750, -0.1626,  0.4114],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('never.r.01.never') [tensor([-0.7896,  0.1447,  0.5910,  ..., -0.0747,  0.3451,  0.6256],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.8000,  0.1607,  0.5546,  ..., -0.0807,  0.3552,  0.6256],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.7892,  0.1459,  0.5912,  ..., -0.0747,  0.3451,  0.6256],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.7893,  0.1456,  0.5912,  ..., -0.0747,  0.3451,  0.6256],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.8001,  0.1607,  0.5545,  ..., -0.0807,  0.3552,  0.6256],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.7892,  0.1459,  0.5912,  ..., -0.0747,  0.3451,  0.6256],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('bishop.n.01.bishop') [tensor([-0.3370, -0.0751,  0.0179,  ...,  0.1786, -0.4439, -0.1187],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.3282, -0.0786,  0.0066,  ...,  0.1786, -0.4441, -0.1189],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.3312, -0.0763,  0.0156,  ...,  0.1785, -0.4442, -0.1188],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.3282, -0.0786,  0.0066,  ...,  0.1786, -0.4441, -0.1189],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('anabaptist.n.01.Anabaptist') [tensor([ 0.2021, -0.3492, -0.2702,  ...,  0.5189, -0.1465,  0.0361],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.2006, -0.3507, -0.2727,  ...,  0.5190, -0.1465,  0.0359],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.2070, -0.3518, -0.2709,  ...,  0.5188, -0.1465,  0.0362],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.2006, -0.3507, -0.2727,  ...,  0.5190, -0.1465,  0.0359],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('afraid.a.01.afraid') [tensor([-0.0977,  0.0762, -0.0479,  ...,  0.1750, -0.0711,  0.3431],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.0724,  0.0795, -0.1347,  ...,  0.1800, -0.0688,  0.3447],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.0805,  0.0539, -0.1437,  ...,  0.1893, -0.0714,  0.3389],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.0725,  0.0795, -0.1349,  ...,  0.1800, -0.0688,  0.3447],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('state.v.01.state') [tensor([0.8387, 0.4358, 0.2751,  ..., 0.1154, 0.3765, 0.0181],\n",
      "       grad_fn=<SelectBackward>), [tensor([0.8316, 0.4248, 0.2283,  ..., 0.1164, 0.3849, 0.0211],\n",
      "       grad_fn=<SelectBackward>)], [tensor([0.8700, 0.4185, 0.2103,  ..., 0.1123, 0.3770, 0.0251],\n",
      "       grad_fn=<SelectBackward>)], [tensor([0.8316, 0.4249, 0.2280,  ..., 0.1164, 0.3849, 0.0211],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('religion.n.01.faith') [tensor([-0.1222,  0.1537,  0.2968,  ...,  0.1237, -0.0720,  0.1113],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.0615,  0.1139,  0.3426,  ...,  0.1360, -0.0624,  0.1054],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1603,  0.1627,  0.3167,  ...,  0.1387, -0.0579,  0.1127],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.0615,  0.1139,  0.3426,  ...,  0.1360, -0.0624,  0.1054],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('know.v.01.know') [tensor([ 4.7676e-01, -4.7552e-02,  3.5792e-01,  ...,  3.4681e-04,\n",
      "        -4.1676e-02,  7.4980e-01], grad_fn=<SelectBackward>), [tensor([ 0.4749,  0.2162,  0.0091,  ...,  0.1555, -0.0309,  0.6517],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.5065, -0.0416,  0.3607,  ...,  0.0027, -0.0362,  0.7476],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.5059, -0.0597,  0.3509,  ..., -0.0016, -0.0349,  0.7509],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.5142, -0.0540,  0.3352,  ...,  0.0061, -0.0400,  0.7463],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.4749,  0.2162,  0.0091,  ...,  0.1555, -0.0309,  0.6517],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.5073, -0.0421,  0.3594,  ...,  0.0027, -0.0362,  0.7476],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.5061, -0.0600,  0.3510,  ..., -0.0016, -0.0349,  0.7509],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('publish.v.03.write') [tensor([ 0.0539, -0.0976, -0.6054,  ...,  0.4349,  0.0689,  0.1603],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.1713, -0.1521, -0.2949,  ...,  0.5627, -0.3090,  0.0864],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.0514, -0.1738, -0.6538,  ...,  0.4302,  0.0704,  0.1643],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.0693, -0.1139, -0.6507,  ...,  0.4398,  0.0729,  0.1657],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.1713, -0.1521, -0.2949,  ...,  0.5627, -0.3090,  0.0864],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.0518, -0.1741, -0.6535,  ...,  0.4302,  0.0704,  0.1643],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('book.n.01.book') [tensor([ 0.0330, -0.8939,  0.1641,  ..., -0.1009, -0.0780,  0.1031],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.0131, -0.9006,  0.1140,  ..., -0.1013, -0.0820,  0.1259],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.0295, -0.8779,  0.0767,  ..., -0.1000, -0.0806,  0.1340],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.0168, -0.9003,  0.1140,  ..., -0.1013, -0.0820,  0.1259],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "belief.n.00 [tensor([-0.5578,  0.3632,  0.6845,  ...,  0.2812,  0.1882,  0.3536],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.6100,  0.3152,  0.7215,  ...,  0.2843,  0.2061,  0.3542],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.5790,  0.3142,  0.7343,  ...,  0.2847,  0.2079,  0.3573],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.6132,  0.3123,  0.7231,  ...,  0.2843,  0.2061,  0.3542],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('soul.n.01.soul') [tensor([0.3314, 0.0373, 0.1876,  ..., 0.6192, 0.5436, 0.7054],\n",
      "       grad_fn=<SelectBackward>), [tensor([0.3237, 0.0265, 0.1653,  ..., 0.6260, 0.5490, 0.7101],\n",
      "       grad_fn=<SelectBackward>)], [tensor([0.2979, 0.0403, 0.1573,  ..., 0.6255, 0.5424, 0.7124],\n",
      "       grad_fn=<SelectBackward>)], [tensor([0.3199, 0.0262, 0.1647,  ..., 0.6260, 0.5490, 0.7101],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('sleep.v.01.sleep') [tensor([ 1.1375,  0.5654,  0.6001,  ...,  0.1671, -0.1656,  0.5706],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 1.1117,  0.6021,  0.4940,  ...,  0.1720, -0.1639,  0.5603],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 1.1199,  0.6198,  0.4726,  ...,  0.1717, -0.1602,  0.5659],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 1.1125,  0.6084,  0.4775,  ...,  0.1717, -0.1641,  0.5601],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('death.n.04.death') [tensor([-0.2281,  0.1603,  0.1752,  ..., -0.0748,  0.3178,  0.3503],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.2184,  0.0390,  0.1429,  ..., -0.0775,  0.3153,  0.3463],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.2542,  0.0206,  0.1502,  ..., -0.0706,  0.3217,  0.3453],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.2230,  0.0260,  0.1397,  ..., -0.0775,  0.3153,  0.3463],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "circulating.s.00 [tensor([ 0.7005,  0.2045, -0.3752,  ...,  0.3996, -0.1575,  0.2086],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.7006,  0.2045, -0.3752,  ...,  0.3996, -0.1575,  0.2086],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.7005,  0.2045, -0.3752,  ...,  0.3996, -0.1575,  0.2086],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.7006,  0.2045, -0.3752,  ...,  0.3996, -0.1575,  0.2086],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('thyroxine.n.01.thyroxine') [tensor([ 0.2686,  0.1080, -0.6229,  ...,  0.2006,  0.1216,  0.4525],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.2684,  0.1097, -0.6222,  ...,  0.2006,  0.1216,  0.4525],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.2685,  0.1093, -0.6224,  ...,  0.2006,  0.1216,  0.4525],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.2684,  0.1097, -0.6222,  ...,  0.2006,  0.1216,  0.4525],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('tri-iodothyronine.n.01.tri-iodothyronine') [tensor([ 0.9726, -0.1112, -0.0156,  ...,  0.0168,  0.1743, -0.0271],\n",
      "       grad_fn=<SelectBackward>), tensor([ 0.0387,  0.9761,  0.4486,  ...,  0.4176,  0.5916, -0.1566],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.9725, -0.1118, -0.0153,  ...,  0.0168,  0.1743, -0.0271],\n",
      "       grad_fn=<SelectBackward>), tensor([ 0.0389,  0.9762,  0.4487,  ...,  0.4176,  0.5916, -0.1566],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.9732, -0.1080, -0.0148,  ...,  0.0168,  0.1743, -0.0271],\n",
      "       grad_fn=<SelectBackward>), tensor([ 0.0384,  0.9792,  0.4479,  ...,  0.4176,  0.5916, -0.1566],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.9725, -0.1118, -0.0153,  ...,  0.0168,  0.1743, -0.0271],\n",
      "       grad_fn=<SelectBackward>), tensor([ 0.0389,  0.9762,  0.4487,  ...,  0.4176,  0.5916, -0.1566],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('section.n.01.section') [tensor([-0.0112,  0.1875,  0.1940,  ...,  0.1883,  0.3381, -0.0298],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.0151,  0.1733,  0.1926,  ...,  0.1882,  0.3381, -0.0298],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.0116,  0.1807,  0.1931,  ...,  0.1884,  0.3381, -0.0297],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.0151,  0.1733,  0.1926,  ...,  0.1882,  0.3381, -0.0298],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('clear.a.01.clear') [tensor([ 0.2065,  0.2701, -0.5400,  ...,  0.0924, -0.0626,  0.2219],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.2062,  0.2700, -0.5408,  ...,  0.0924, -0.0626,  0.2219],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.2066,  0.2702, -0.5401,  ...,  0.0924, -0.0626,  0.2219],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.2062,  0.2700, -0.5408,  ...,  0.0924, -0.0626,  0.2219],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('mechanism.n.03.mechanism') [tensor([-0.1271,  0.1403, -0.1103,  ...,  0.2893,  0.3139,  0.6500],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.1116,  0.1581, -0.1160,  ...,  0.2891,  0.3137,  0.6499],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1059,  0.1537, -0.1174,  ...,  0.2892,  0.3139,  0.6499],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1114,  0.1583, -0.1160,  ...,  0.2891,  0.3137,  0.6499],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('exist.v.01.exist') [tensor([ 0.1996,  0.1280, -0.2200,  ..., -0.0121, -0.2101, -0.0418],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.1684,  0.1608, -0.2815,  ..., -0.0140, -0.2113, -0.0422],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.1747,  0.1598, -0.2811,  ..., -0.0133, -0.2109, -0.0421],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.1683,  0.1609, -0.2819,  ..., -0.0140, -0.2113, -0.0422],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('release.n.03.release') [tensor([-0.5097,  0.4027, -0.2705,  ...,  0.0207,  1.0081,  0.1240],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.5243,  0.4015, -0.2841,  ...,  0.0210,  1.0086,  0.1240],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.5283,  0.4096, -0.2891,  ...,  0.0211,  1.0087,  0.1240],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.5243,  0.4015, -0.2841,  ...,  0.0210,  1.0086,  0.1240],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('protein.n.01.protein') [tensor([ 0.6945, -0.2514,  0.2080,  ..., -0.0547,  0.7280,  0.0709],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.6944, -0.2422,  0.2016,  ..., -0.0549,  0.7278,  0.0709],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.6951, -0.2445,  0.2028,  ..., -0.0547,  0.7278,  0.0708],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.6944, -0.2420,  0.2014,  ..., -0.0549,  0.7278,  0.0709],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('secretion.n.01.secretion') [tensor([ 0.4048,  0.5151, -0.0386,  ...,  0.4266,  0.2980,  0.7066],\n",
      "       grad_fn=<SelectBackward>), tensor([ 0.2390,  0.2254, -0.5615,  ...,  0.1760,  1.1301,  0.2009],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.3823,  0.5258, -0.0438,  ...,  0.4269,  0.2982,  0.7069],\n",
      "       grad_fn=<SelectBackward>), tensor([ 0.2062,  0.2106, -0.5915,  ...,  0.1760,  1.1302,  0.2012],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.3943,  0.5424, -0.0399,  ...,  0.4270,  0.2982,  0.7068],\n",
      "       grad_fn=<SelectBackward>), tensor([ 0.2165,  0.2172, -0.5883,  ...,  0.1762,  1.1303,  0.2011],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.3820,  0.5262, -0.0438,  ...,  0.4269,  0.2982,  0.7069],\n",
      "       grad_fn=<SelectBackward>), tensor([ 0.2055,  0.2104, -0.5925,  ...,  0.1760,  1.1302,  0.2012],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('reduce.v.01.cut') [tensor([-0.1910, -0.0484,  0.4966,  ...,  0.1884, -0.1489,  0.5567],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.1910, -0.0484,  0.4966,  ...,  0.1884, -0.1489,  0.5567],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('necessitate.v.01.need') [tensor([ 0.2691, -0.6744, -0.9967,  ...,  0.3814,  0.4395,  0.3681],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.1519, -0.4757, -0.4798,  ...,  0.4533,  0.5921,  0.3948],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.1874, -0.5120, -0.7582,  ...,  0.2774,  0.3279,  0.0894],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.2691, -0.6745, -0.9966,  ...,  0.3814,  0.4395,  0.3681],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.1520, -0.4759, -0.4792,  ...,  0.4533,  0.5921,  0.3948],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.1948, -0.5108, -0.7798,  ...,  0.2774,  0.3279,  0.0894],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('pare.v.01.pare') [tensor([ 0.2688,  0.2548,  0.3962,  ..., -0.0837, -0.1791,  0.2818],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.2689,  0.2551,  0.3963,  ..., -0.0837, -0.1791,  0.2818],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('yankee.n.01.Yankee') [tensor([ 0.0487,  0.1887, -0.5258,  ..., -0.1000, -0.2005,  0.1742],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.0487,  0.1887, -0.5258,  ..., -0.1000, -0.2005,  0.1742],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('sergeant.n.01.sergeant') [tensor([ 0.1102, -0.3080, -0.0606,  ..., -0.0755, -0.3110,  0.1635],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.1102, -0.3080, -0.0606,  ..., -0.0755, -0.3110,  0.1635],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('yield.v.01.give') [tensor([ 0.1215, -0.1269, -0.6761,  ...,  0.1622,  0.0822,  0.4665],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.1165, -0.5305, -0.7389,  ...,  0.4220, -0.2699,  0.0748],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.2884, -0.1573, -0.6851,  ...,  0.5012,  0.1407,  0.2323],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.1215, -0.1267, -0.6761,  ...,  0.1622,  0.0822,  0.4665],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.1165, -0.5305, -0.7389,  ...,  0.4220, -0.2699,  0.0748],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.2884, -0.1573, -0.6851,  ...,  0.5012,  0.1407,  0.2323],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "following.s.01 [tensor([0.1542, 0.0151, 0.1186,  ..., 0.1227, 0.2771, 0.6339],\n",
      "       grad_fn=<SelectBackward>), [tensor([0.1544, 0.0153, 0.1187,  ..., 0.1227, 0.2771, 0.6339],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('description.n.01.description') [tensor([-0.2532,  0.1231, -0.0673,  ..., -0.1950,  0.1289,  0.6079],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.2508,  0.1236, -0.0673,  ..., -0.1950,  0.1289,  0.6079],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('sweetheart.n.01.sweetheart') [tensor([-0.5772, -0.7077, -0.2340,  ...,  0.0219,  0.0794, -0.0013],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.5745, -0.7081, -0.2335,  ...,  0.0219,  0.0794, -0.0013],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('girlfriend.n.02.girl') [tensor([ 0.0363, -0.0574, -0.4273,  ...,  0.1094,  0.0156,  0.2172],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.2555,  0.7202, -0.2075,  ...,  0.5274,  0.1527,  0.2183],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.0402, -0.1079, -0.4395,  ...,  0.1078,  0.0177,  0.2194],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.0362, -0.0576, -0.4275,  ...,  0.1094,  0.0156,  0.2172],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.2691,  0.7296, -0.2199,  ...,  0.5273,  0.1524,  0.2183],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.0403, -0.1079, -0.4395,  ...,  0.1078,  0.0177,  0.2194],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('jerkwater.s.01.one-horse') [tensor([ 0.2914, -0.8561,  0.8416,  ..., -0.0133, -0.2238,  0.3855],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.2806, -0.8492,  0.8292,  ..., -0.0161, -0.2258,  0.3851],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('kill.v.01.kill') [tensor([ 0.1327, -0.4110,  0.0589,  ...,  0.0881, -0.0900,  0.3696],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.1327, -0.4110,  0.0589,  ...,  0.0881, -0.0900,  0.3696],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('exist.v.02.live') [tensor([ 0.9471, -0.6270, -0.2623,  ...,  0.1888, -0.0368,  0.5731],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.9471, -0.6270, -0.2623,  ...,  0.1888, -0.0368,  0.5731],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('name.n.01.name') [tensor([-0.0226, -0.4012, -0.4365,  ...,  0.0113,  0.3200,  0.2110],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.0226, -0.4012, -0.4365,  ...,  0.0113,  0.3200,  0.2110],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('use.v.01.employ') [tensor([ 0.0915, -0.1074,  0.2997,  ...,  0.1995,  0.3322,  0.5498],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.0915, -0.1074,  0.2997,  ...,  0.1995,  0.3322,  0.5498],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('desire.v.01.want') [tensor([-0.3839, -0.2196, -0.2165,  ...,  0.2952,  0.3554,  0.4817],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.3839, -0.2196, -0.2165,  ...,  0.2952,  0.3554,  0.4817],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('help.v.01.help') [tensor([-0.4461,  0.1618,  0.1480,  ..., -0.0861,  0.1709,  0.4218],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.4461,  0.1618,  0.1480,  ..., -0.0861,  0.1709,  0.4218],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('prevent.v.01.prevent') [tensor([-0.2299, -0.0137,  0.2201,  ...,  0.1593, -0.0238,  0.3915],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.2299, -0.0137,  0.2201,  ...,  0.1593, -0.0238,  0.3915],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('originate.v.01.arise') [tensor([ 0.4417,  0.1512,  0.4558,  ..., -0.1965,  0.1583,  0.6234],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.4417,  0.1512,  0.4558,  ..., -0.1965,  0.1583,  0.6234],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('play.v.10.play') [tensor([ 0.4945,  0.2285,  0.2772,  ...,  0.1787, -0.2508,  0.3382],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.4766,  0.2277,  0.3167,  ...,  0.1813, -0.2451,  0.3371],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.4945,  0.2285,  0.2772,  ...,  0.1787, -0.2508,  0.3382],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.5038,  0.2110,  0.2803,  ...,  0.1753, -0.2499,  0.3392],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('arrange.v.01.arrange') [tensor([-0.6831,  0.0851, -0.3688,  ...,  0.6461,  0.0364,  0.2425],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.6830,  0.0852, -0.3686,  ...,  0.6461,  0.0364,  0.2425],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('earlier.r.01.before') [tensor([ 0.3783,  0.3938, -0.6807,  ..., -0.2930,  0.0137,  0.0940],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.3783,  0.3938, -0.6807,  ..., -0.2930,  0.0137,  0.0940],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('start.v.06.start') [tensor([ 0.6968, -0.1477,  0.5481,  ...,  0.0377, -0.0366,  0.2468],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.8626,  0.3370,  0.5269,  ...,  0.1370, -0.2170,  0.1631],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.6968, -0.1477,  0.5481,  ...,  0.0377, -0.0366,  0.2468],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.8626,  0.3370,  0.5269,  ...,  0.1370, -0.2170,  0.1631],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('aim.n.02.objective') [tensor([-0.6043,  0.4456,  0.0795,  ...,  0.1279,  0.4280,  0.6039],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.6021,  0.4461,  0.0801,  ...,  0.1279,  0.4280,  0.6039],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('organization.n.01.organization') [tensor([-0.2445,  0.0544, -0.0413,  ...,  0.1781, -0.1846,  0.1871],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.2444,  0.0539, -0.0411,  ...,  0.1781, -0.1846,  0.1871],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('list.n.01.list') [tensor([-0.0053, -0.1548, -0.3599,  ..., -0.3039, -0.1512,  0.4471],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.0097, -0.1492, -0.3668,  ..., -0.3041, -0.1513,  0.4471],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('necessity.n.02.requirement') [tensor([-0.6129,  0.3319, -0.2568,  ...,  0.1072,  0.6205,  0.2650],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.6261,  0.3272, -0.2580,  ...,  0.1072,  0.6205,  0.2650],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('type.n.01.type') [tensor([0.8913, 0.2308, 0.3772,  ..., 0.2194, 0.2588, 0.0356],\n",
      "       grad_fn=<SelectBackward>), [tensor([0.8771, 0.2319, 0.3628,  ..., 0.2194, 0.2588, 0.0356],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('site.n.01.site') [tensor([0.4827, 0.4849, 0.2146,  ..., 0.0914, 0.0627, 0.1315],\n",
      "       grad_fn=<SelectBackward>), [tensor([0.4738, 0.4896, 0.2098,  ..., 0.0914, 0.0627, 0.1315],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('most.a.01.most') [tensor([ 0.3259,  0.3855,  0.0989,  ...,  0.3137, -0.1776,  0.6632],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.2907,  0.4453,  0.2433,  ...,  0.3187, -0.1662,  0.6677],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.3259,  0.3855,  0.0989,  ...,  0.3137, -0.1776,  0.6632],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.3451,  0.3926,  0.1080,  ...,  0.3219, -0.1723,  0.6630],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('belief.n.01.belief') [tensor([-0.6378,  0.2682,  0.4856,  ...,  0.3830,  0.3166,  0.3487],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.6378,  0.2682,  0.4856,  ...,  0.3830,  0.3166,  0.3487],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('still.r.01.still') [tensor([-0.2030,  0.0123,  0.4205,  ...,  0.2868,  0.2945,  0.7936],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.2030,  0.0123,  0.4207,  ...,  0.2868,  0.2945,  0.7936],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('unsettled.a.01.unsettled') [tensor([-0.0728, -0.3082,  0.5252,  ...,  0.1803,  0.0959,  0.3980],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.0715, -0.3087,  0.5257,  ...,  0.1803,  0.0959,  0.3980],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('not.r.01.not') [tensor([ 0.1373, -0.3391,  0.2728,  ...,  0.0833,  0.2333,  0.3423],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.1373, -0.3392,  0.2727,  ...,  0.0833,  0.2333,  0.3423],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('killing.n.02.killing') [tensor([-0.1148,  0.3662,  0.2716,  ...,  0.1857,  0.3736, -0.0358],\n",
      "       grad_fn=<SelectBackward>), tensor([ 0.2068,  0.3711, -0.1197,  ..., -0.0687,  0.1015,  0.5055],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.1091,  0.3587,  0.2690,  ...,  0.1857,  0.3736, -0.0358],\n",
      "       grad_fn=<SelectBackward>), tensor([ 0.2056,  0.3720, -0.1191,  ..., -0.0687,  0.1015,  0.5055],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "n't.r.00 [tensor([ 0.0175, -0.0601, -0.2983,  ..., -0.1312,  0.1364,  0.3176],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.0156, -0.0181, -0.2651,  ..., -0.1350,  0.1355,  0.3236],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.0175, -0.0601, -0.2983,  ..., -0.1312,  0.1364,  0.3176],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.0156, -0.0181, -0.2651,  ..., -0.1350,  0.1355,  0.3236],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('jury.n.01.jury') [tensor([ 0.9118, -0.8232, -0.1999,  ...,  0.3673,  0.5845,  0.1678],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.9118, -0.8232, -0.1999,  ...,  0.3673,  0.5845,  0.1678],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('praise.v.01.praise') [tensor([-0.1530, -0.0438, -0.0089,  ...,  0.3843,  0.0566,  0.4542],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.1530, -0.0438, -0.0089,  ...,  0.3843,  0.0566,  0.4542],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('administration.n.01.administration') [tensor([-0.7947,  0.2230,  0.6036,  ..., -0.1320, -0.0732,  0.4468],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.7947,  0.2230,  0.6036,  ..., -0.1320, -0.0732,  0.4468],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('operation.n.01.operation') [tensor([-0.0747, -0.3269,  0.2578,  ..., -0.1668,  0.5563,  0.4282],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.0746, -0.3269,  0.2578,  ..., -0.1668,  0.5563,  0.4282],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('atlanta.n.01.Atlanta') [tensor([ 0.3510,  0.0803,  0.3959,  ..., -0.2973,  0.0698,  0.2124],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.3510,  0.0805,  0.3958,  ..., -0.2973,  0.0698,  0.2124],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('then.r.01.then') [tensor([ 0.3319,  0.1080, -0.0140,  ...,  0.3456, -0.1671,  0.3828],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.3319,  0.1080, -0.0140,  ...,  0.3456, -0.1671,  0.3828],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('new.a.01.new') [tensor([ 0.4110, -0.0238, -0.7315,  ...,  0.0599,  0.0430,  0.2580],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.4110, -0.0238, -0.7315,  ...,  0.0599,  0.0430,  0.2580],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('affluence.n.01.affluence') [tensor([-0.4906, -0.1620,  0.5904,  ...,  0.2285,  0.4666,  0.2445],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.4906, -0.1620,  0.5904,  ...,  0.2285,  0.4666,  0.2445],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('actually.r.01.actually') [tensor([0.0221, 0.0990, 0.0438,  ..., 0.1863, 0.3863, 0.6814],\n",
      "       grad_fn=<SelectBackward>), [tensor([0.0221, 0.0990, 0.0438,  ..., 0.1863, 0.3863, 0.6814],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('wide.a.01.wide') [tensor([-0.0852, -0.4658,  0.5287,  ...,  0.1599,  0.0305,  0.0954],\n",
      "       grad_fn=<SelectBackward>), tensor([-0.1203,  0.4844,  0.4654,  ...,  0.3354,  0.2276,  0.5456],\n",
      "       grad_fn=<SelectBackward>), [tensor([0.1341, 0.1111, 0.5573,  ..., 0.3314, 0.1549, 0.4119],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.0852, -0.4657,  0.5286,  ...,  0.1599,  0.0305,  0.0954],\n",
      "       grad_fn=<SelectBackward>), tensor([-0.1202,  0.4848,  0.4655,  ...,  0.3354,  0.2276,  0.5456],\n",
      "       grad_fn=<SelectBackward>)], [tensor([0.1341, 0.1111, 0.5573,  ..., 0.3314, 0.1549, 0.4119],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('earth.n.01.world') [tensor([0.0809, 0.1609, 0.5084,  ..., 0.3384, 0.1510, 0.4101],\n",
      "       grad_fn=<SelectBackward>), [tensor([0.0809, 0.1609, 0.5084,  ..., 0.3384, 0.1510, 0.4101],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('precinct.n.01.precinct') [tensor([1.2577, 0.0018, 0.4984,  ..., 0.1067, 0.6541, 0.6242],\n",
      "       grad_fn=<SelectBackward>), [tensor([1.2569e+00, 9.6773e-04, 4.9834e-01,  ..., 1.0673e-01, 6.5407e-01,\n",
      "        6.2418e-01], grad_fn=<SelectBackward>)]]\n",
      "Lemma('slightly.r.01.somewhat') [tensor([0.1305, 0.4292, 0.6076,  ..., 0.1354, 0.2337, 0.2962],\n",
      "       grad_fn=<SelectBackward>), [tensor([0.4269, 0.6047, 0.2931,  ..., 0.3462, 0.5119, 0.5649],\n",
      "       grad_fn=<SelectBackward>)], [tensor([0.1305, 0.4292, 0.6076,  ..., 0.1354, 0.2337, 0.2962],\n",
      "       grad_fn=<SelectBackward>)], [tensor([0.4269, 0.6048, 0.2930,  ..., 0.3462, 0.5119, 0.5649],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('surprise.n.01.surprise') [tensor([ 0.5855, -0.3667,  0.6699,  ...,  0.1241,  0.3775,  0.5903],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.5855, -0.3667,  0.6699,  ...,  0.1241,  0.3775,  0.5903],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('detect.v.01.find') [tensor([ 0.2632,  0.3860,  0.1644,  ...,  0.4897, -0.1960,  0.2021],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.4677,  0.3975, -0.0882,  ...,  0.4382,  0.0233,  0.4594],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.2831,  0.3914,  0.1067,  ...,  0.4927, -0.1952,  0.2022],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.2632,  0.3860,  0.1644,  ...,  0.4897, -0.1960,  0.2021],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.4677,  0.3975, -0.0882,  ...,  0.4382,  0.0233,  0.4594],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.2831,  0.3914,  0.1067,  ...,  0.4927, -0.1952,  0.2022],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "one.s.00 [tensor([ 0.6516, -0.1395,  0.1974,  ..., -0.1990, -0.4265,  0.3527],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.6516, -0.1395,  0.1974,  ..., -0.1990, -0.4265,  0.3527],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('consider.v.03.consider') [tensor([ 0.3560,  0.0844, -0.4574,  ...,  0.6190,  0.1604,  0.5366],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.3560,  0.0844, -0.4574,  ...,  0.6190,  0.1604,  0.5366],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('occupation.n.01.job') [tensor([ 0.4443, -0.3749,  0.3288,  ...,  0.1424, -0.0701,  0.1104],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.4288, -0.3564,  0.3907,  ...,  0.1370, -0.0620,  0.1009],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.4435, -0.3767,  0.3287,  ...,  0.1424, -0.0701,  0.1104],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.4535, -0.3383,  0.3289,  ...,  0.1410, -0.0692,  0.1090],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('look.v.02.appear') [tensor([ 0.4762,  0.2238,  0.4932,  ..., -0.1778,  0.5121,  0.5027],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.4768,  0.2244,  0.4927,  ..., -0.1778,  0.5121,  0.5027],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('mousy.s.01.mousy') [tensor([ 0.4405, -0.3143,  0.1692,  ...,  0.2233,  0.1403,  0.1386],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.4392, -0.3163,  0.1679,  ...,  0.2233,  0.1403,  0.1386],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('besides.r.02.also') [tensor([0.1062, 0.2841, 0.2980,  ..., 0.1550, 0.2130, 0.5200],\n",
      "       grad_fn=<SelectBackward>), [tensor([0.1067, 0.2839, 0.2963,  ..., 0.1550, 0.2130, 0.5200],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('office.n.01.office') [tensor([ 0.6694,  0.2425, -0.2003,  ...,  0.1062,  0.3112,  0.2456],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.6586,  0.2443, -0.2177,  ...,  0.1061,  0.3112,  0.2456],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('relatively.r.01.relatively') [tensor([1.2791, 0.6255, 0.7782,  ..., 0.4417, 0.4946, 0.6010],\n",
      "       grad_fn=<SelectBackward>), [tensor([1.2807, 0.6187, 0.7755,  ..., 0.4417, 0.4946, 0.6010],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('short.a.01.short') [tensor([-0.0183,  0.2106,  0.0017,  ...,  0.4983, -0.0475,  0.4866],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.0585,  0.2254, -0.0466,  ...,  0.4978, -0.0488,  0.4877],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('time.n.02.time') [tensor([-0.0323, -0.2759, -0.0201,  ...,  0.1604,  0.4043,  0.3377],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.0424, -0.2883, -0.0481,  ...,  0.1554,  0.4002,  0.3400],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.0607, -0.2757, -0.0073,  ...,  0.1608,  0.4046,  0.3374],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.0424, -0.2883, -0.0481,  ...,  0.1554,  0.4002,  0.3400],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('outstanding.s.01.outstanding') [tensor([ 0.4469,  0.5603,  0.2010,  ...,  0.1258, -0.0051, -0.0737],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.4277,  0.5177,  0.1626,  ...,  0.1197, -0.0032, -0.0762],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "role.n.2;1 [tensor([ 0.4362,  0.2107, -0.0051,  ...,  0.1558,  0.4465, -0.1280],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.4534,  0.1789, -0.0582,  ...,  0.1657,  0.4488, -0.1198],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('receptionist.n.01.receptionist') [tensor([-0.0585, -0.3340,  0.1389,  ..., -0.0477, -0.4930,  0.4933],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.0696, -0.2972,  0.1045,  ..., -0.0422, -0.4886,  0.4975],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "show.v.6;4 [tensor([ 0.1160,  0.4331, -0.5621,  ...,  0.3461,  0.0269,  0.3653],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.1190,  0.4288, -0.6072,  ...,  0.3552,  0.0345,  0.3585],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('wit.n.01.wit') [tensor([-0.3579, -0.2193,  0.0724,  ...,  0.2102,  0.1088,  0.5293],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.3825, -0.2346,  0.0539,  ...,  0.2236,  0.1064,  0.5254],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('sparkle.n.01.sparkle') [tensor([0.3098, 0.1670, 0.7143,  ..., 0.5590, 0.2992, 0.1931],\n",
      "       grad_fn=<SelectBackward>), [tensor([0.2878, 0.1701, 0.6718,  ..., 0.5586, 0.3012, 0.1884],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('aplomb.n.01.aplomb') [tensor([-0.7233,  0.0021,  0.5679,  ...,  0.2451,  0.5015,  0.4619],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.7411,  0.0113,  0.5874,  ...,  0.2450,  0.5044,  0.4451],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('whole.a.01.whole') [tensor([ 0.4526, -0.2982,  0.1108,  ...,  0.2198, -0.0582,  0.0247],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.4526, -0.2982,  0.1108,  ...,  0.2198, -0.0582,  0.0247],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('sky.n.01.sky') [tensor([ 0.4961, -0.2019, -0.6149,  ...,  0.1292,  0.2577,  0.6639],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.4961, -0.2019, -0.6149,  ...,  0.1292,  0.2577,  0.6639],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('become.v.02.become') [tensor([-0.0900,  0.0595, -0.1845,  ...,  0.1221,  0.4675, -0.0515],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.0900,  0.0595, -0.1845,  ...,  0.1221,  0.4675, -0.0515],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('attach.v.03.attach') [tensor([ 0.0987,  0.7231, -0.9814,  ...,  0.0246,  0.0787, -0.0382],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.0987,  0.7231, -0.9814,  ...,  0.0246,  0.0787, -0.0382],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('bottom.n.02.bottom') [tensor([ 0.1394, -0.3603,  0.4767,  ..., -0.1764,  0.1128,  0.3330],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.1394, -0.3603,  0.4767,  ..., -0.1764,  0.1128,  0.3330],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('hill.n.01.hill') [tensor([ 0.2589, -0.3977, -0.5112,  ...,  0.1942,  0.5736,  0.3340],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.2589, -0.3977, -0.5112,  ...,  0.1942,  0.5736,  0.3340],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('conduct.v.02.direct') [tensor([ 0.2747, -0.1027, -0.0057,  ...,  0.6615, -0.0300,  0.4541],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.2747, -0.1027, -0.0057,  ...,  0.6615, -0.0300,  0.4541],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('support.n.04.reinforcement') [tensor([-0.3582, -0.0466, -0.4474,  ...,  0.4310,  0.3538,  0.0867],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.3582, -0.0466, -0.4474,  ...,  0.4310,  0.3538,  0.0867],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('battle.n.01.fight') [tensor([ 0.6906, -0.6505, -0.0454,  ...,  0.0085,  0.0988, -0.0254],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.6906, -0.6505, -0.0454,  ...,  0.0085,  0.0988, -0.0254],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('tell.v.03.tell') [tensor([ 0.5839, -0.4322, -0.5007,  ...,  0.3219,  0.2475,  0.4155],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.5839, -0.4322, -0.5007,  ...,  0.3219,  0.2475,  0.4155],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('spend.v.01.spend') [tensor([-0.5431,  0.2029,  0.0073,  ...,  0.3341, -0.1542,  0.1484],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.5431,  0.2029,  0.0073,  ...,  0.3341, -0.1542,  0.1484],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('get.v.01.get') [tensor([ 0.2028, -0.2360,  0.2011,  ...,  0.6088,  0.2446, -0.0181],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.2028, -0.2360,  0.2011,  ...,  0.6088,  0.2446, -0.0181],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('like.v.02.like') [tensor([-0.0659, -0.0875, -0.0562,  ..., -0.2331,  0.1871,  0.5633],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.0659, -0.0875, -0.0562,  ..., -0.2331,  0.1871,  0.5633],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('same.a.01.same') [tensor([ 0.7568,  0.1614, -0.0481,  ...,  0.1875,  0.4457,  0.8542],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.7568,  0.1614, -0.0481,  ...,  0.1875,  0.4457,  0.8542],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('room.n.01.room') [tensor([ 0.4628, -0.8477,  0.0100,  ..., -0.1311,  0.0084,  0.4247],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.4628, -0.8477,  0.0100,  ..., -0.1311,  0.0084,  0.4247],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('buy.v.01.buy') [tensor([-0.1219, -0.2326,  0.6117,  ...,  0.3729,  0.3049,  0.2833],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.1219, -0.2326,  0.6117,  ...,  0.3729,  0.3049,  0.2833],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('feel.v.01.feel') [tensor([-0.4258,  0.4747, -0.1999,  ...,  0.6945,  0.1345,  0.3667],\n",
      "       grad_fn=<SelectBackward>), [tensor([0.2639, 0.3643, 0.2316,  ..., 0.4198, 0.4835, 0.6011],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.4258,  0.4747, -0.1999,  ...,  0.6945,  0.1345,  0.3667],\n",
      "       grad_fn=<SelectBackward>)], [tensor([0.2639, 0.3643, 0.2316,  ..., 0.4198, 0.4835, 0.6011],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('travel.v.01.go') [tensor([ 0.5893, -0.5139,  0.3836,  ...,  0.3498, -0.0781,  0.3581],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.5893, -0.5139,  0.3836,  ...,  0.3498, -0.0781,  0.3581],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('avoid.v.01.avoid') [tensor([-0.1239,  0.3099, -0.0076,  ..., -0.0542, -0.0885,  0.3781],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.1239,  0.3099, -0.0076,  ..., -0.0542, -0.0885,  0.3781],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('frame.v.04.put') [tensor([-0.2509,  0.5472,  0.1207,  ...,  0.2999, -0.1159,  0.3688],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.2509,  0.5472,  0.1207,  ...,  0.2999, -0.1159,  0.3688],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('keep.v.01.keep') [tensor([ 0.0296, -0.5565,  0.0874,  ...,  0.2977,  0.3011,  0.2163],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.0296, -0.5565,  0.0874,  ...,  0.2977,  0.3011,  0.2163],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('end.v.01.end') [tensor([ 0.1385,  0.1128,  0.3813,  ..., -0.0468,  0.1989,  0.4349],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.1385,  0.1128,  0.3813,  ..., -0.0468,  0.1989,  0.4349],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('catch.v.01.catch') [tensor([ 0.3476, -0.5531,  0.1464,  ...,  0.2803,  0.2926,  0.7657],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.3476, -0.5531,  0.1464,  ...,  0.2803,  0.2926,  0.7657],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('one.s.01.one') [tensor([ 0.6548, -0.1304,  0.1973,  ..., -0.1922, -0.4261,  0.3484],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.6548, -0.1304,  0.1973,  ..., -0.1922, -0.4261,  0.3484],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "more.s.00 [tensor([ 0.6770,  0.0578, -0.5204,  ...,  0.1446, -0.0421,  0.2041],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.6770,  0.0578, -0.5204,  ...,  0.1446, -0.0421,  0.2041],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('down.a.01.down') [tensor([-0.4673, -0.0410,  0.4436,  ..., -0.2819, -0.4849,  0.3101],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.4673, -0.0410,  0.4436,  ..., -0.2819, -0.4849,  0.3101],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('here.r.01.here') [tensor([ 0.3388, -0.1678, -0.0194,  ..., -0.0788,  0.1660,  0.0934],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.3388, -0.1678, -0.0194,  ..., -0.0788,  0.1660,  0.0934],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('stocking.n.01.stocking') [tensor([-0.1753,  0.1875,  0.0588,  ...,  0.7710,  0.1422,  0.5864],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.1753,  0.1875,  0.0588,  ...,  0.7710,  0.1422,  0.5864],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('modern.a.01.modern') [tensor([ 0.3415,  0.0284, -0.2161,  ..., -0.1679,  0.0142, -0.1643],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.3415,  0.0284, -0.2161,  ..., -0.1679,  0.0142, -0.1643],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('condition.n.01.condition') [tensor([0.0321, 0.8330, 0.1121,  ..., 0.0889, 0.1199, 0.3379],\n",
      "       grad_fn=<SelectBackward>), [tensor([0.0321, 0.8330, 0.1121,  ..., 0.0889, 0.1199, 0.3379],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('particularly.r.01.especially') [tensor([ 0.4982,  0.9940, -0.0161,  ...,  0.2712,  0.0205,  0.2745],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.4982,  0.9940, -0.0161,  ...,  0.2712,  0.0205,  0.2745],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('true.a.01.true') [tensor([ 0.4554, -0.3386, -0.1604,  ...,  0.3024,  0.7492,  0.2143],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.4554, -0.3386, -0.1604,  ...,  0.3024,  0.7492,  0.2143],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('ready.a.01.ready') [tensor([ 0.3226, -0.3474, -0.7261,  ...,  0.2594, -0.3335,  0.1429],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.3226, -0.3474, -0.7261,  ...,  0.2594, -0.3335,  0.1429],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('militia.n.01.reserves') [tensor([ 0.0873,  0.3505,  0.0456,  ...,  0.1237,  0.3524, -0.1132],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.0873,  0.3505,  0.0456,  ...,  0.1237,  0.3524, -0.1132],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('assume.v.05.take') [tensor([-0.0559,  0.7569,  0.1406,  ..., -0.1956,  0.1691,  0.3284],\n",
      "       grad_fn=<SelectBackward>), tensor([ 0.4829,  0.1243,  0.1024,  ..., -1.1432,  0.3058, -0.5590],\n",
      "       grad_fn=<SelectBackward>), tensor([-0.1382,  0.3305, -0.3113,  ...,  0.4525,  0.1953, -0.2551],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.4173, -0.2302,  0.2659,  ...,  0.3613,  0.1001,  0.6142],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.0559,  0.7569,  0.1406,  ..., -0.1956,  0.1691,  0.3284],\n",
      "       grad_fn=<SelectBackward>), tensor([ 0.4829,  0.1243,  0.1024,  ..., -1.1432,  0.3058, -0.5590],\n",
      "       grad_fn=<SelectBackward>), tensor([-0.1382,  0.3305, -0.3113,  ...,  0.4525,  0.1953, -0.2551],\n",
      "       grad_fn=<SelectBackward>)], [tensor([ 0.4173, -0.2302,  0.2659,  ...,  0.3613,  0.1001,  0.6142],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('primary.a.01.primary') [tensor([ 0.4607, -0.2156,  0.2448,  ...,  0.3644,  0.0960,  0.6205],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.4607, -0.2156,  0.2448,  ...,  0.3644,  0.0960,  0.6205],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('duty.n.01.responsibility') [tensor([ 0.4112,  0.0816, -0.1494,  ...,  0.4021,  0.1247,  0.2581],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.4112,  0.0816, -0.1494,  ...,  0.4021,  0.1247,  0.2581],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('capital.n.06.Washington') [tensor([-0.5102,  0.0536, -0.1289,  ..., -0.1873,  0.1186, -0.0074],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.5102,  0.0536, -0.1289,  ..., -0.1873,  0.1186, -0.0074],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('follow.v.03.follow') [tensor([ 0.3238, -0.4692,  0.4246,  ...,  0.6375,  0.6170,  0.0176],\n",
      "       grad_fn=<SelectBackward>), tensor([-0.9642, -0.2667, -0.0537,  ..., -0.1928,  0.3904,  0.7736],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.3238, -0.4692,  0.4246,  ...,  0.6375,  0.6170,  0.0176],\n",
      "       grad_fn=<SelectBackward>), tensor([-0.9642, -0.2667, -0.0537,  ..., -0.1928,  0.3904,  0.7736],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('ambassador.n.01.ambassador') [tensor([-0.7639, -0.7577,  0.4615,  ..., -0.1207,  0.3780, -0.0878],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.7639, -0.7577,  0.4615,  ..., -0.1207,  0.3780, -0.0878],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('overseas.r.02.overseas') [tensor([-0.4636, -0.5490, -0.2680,  ..., -0.1119, -0.2495,  0.1561],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.4635, -0.5486, -0.2685,  ..., -0.1119, -0.2495,  0.1561],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('sing.v.01.sing') [tensor([ 0.1003,  0.1281,  0.1936,  ...,  0.5488, -0.1629,  0.0620],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.1003,  0.1281,  0.1936,  ...,  0.5488, -0.1629,  0.0620],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('fail.v.01.fail') [tensor([-0.0302, -0.5183,  0.1138,  ...,  0.1055,  0.1971,  0.2045],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.0302, -0.5183,  0.1138,  ...,  0.1055,  0.1971,  0.2045],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('try.v.01.try') [tensor([ 0.5387, -0.1786, -0.1446,  ...,  0.3521,  0.0604,  0.1571],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.5387, -0.1786, -0.1446,  ...,  0.3521,  0.0604,  0.1571],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('happily.r.01.blithely') [tensor([-0.8342, -0.4041,  0.3751,  ..., -0.2717,  0.1953,  0.4701],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.8342, -0.4041,  0.3751,  ..., -0.2717,  0.1953,  0.4701],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('hear.v.01.hear') [tensor([ 0.6187, -0.7214, -0.4408,  ...,  0.3987,  0.3583, -0.1316],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.6187, -0.7214, -0.4408,  ...,  0.3987,  0.3583, -0.1316],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('cry.v.02.weep') [tensor([ 0.2191, -0.1302,  0.0294,  ..., -0.0486,  0.5298,  0.3152],\n",
      "       grad_fn=<SelectBackward>), tensor([ 0.0330,  0.6169, -0.2457,  ..., -0.2742, -0.4868,  0.3148],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.2191, -0.1302,  0.0294,  ..., -0.0486,  0.5298,  0.3152],\n",
      "       grad_fn=<SelectBackward>), tensor([ 0.0330,  0.6169, -0.2457,  ..., -0.2742, -0.4868,  0.3148],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('indicate.v.03.indicate') [tensor([ 0.1006,  0.5614, -0.5263,  ...,  0.2367,  0.0819,  0.1268],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.1006,  0.5614, -0.5263,  ...,  0.2367,  0.0819,  0.1268],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('collaborate.v.01.cooperate') [tensor([-0.3634,  0.4411, -0.0744,  ...,  0.3693,  0.1916,  0.1195],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.3634,  0.4411, -0.0744,  ...,  0.3693,  0.1916,  0.1195],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('banish.v.01.banish') [tensor([ 0.1320,  0.0269,  0.8388,  ...,  0.1229, -0.0670,  0.4984],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.1320,  0.0269,  0.8388,  ...,  0.1229, -0.0670,  0.4984],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('debate.n.02.debate') [tensor([0.0918, 0.1519, 0.4892,  ..., 0.4295, 0.0421, 0.2204],\n",
      "       grad_fn=<SelectBackward>), [tensor([0.0918, 0.1519, 0.4892,  ..., 0.4295, 0.0421, 0.2204],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('position.n.06.place') [tensor([ 0.9115, -0.4007,  0.2699,  ..., -0.1505,  0.1345,  0.1851],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.9115, -0.4007,  0.2699,  ..., -0.1505,  0.1345,  0.1851],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('influence.n.01.influence') [tensor([-0.3325,  0.0187,  0.5222,  ...,  0.3485,  0.7129,  0.1520],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.3325,  0.0187,  0.5222,  ...,  0.3485,  0.7129,  0.1520],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('thus.r.02.thus') [tensor([-0.0123,  0.4762, -0.3228,  ...,  0.5592, -0.1582, -0.0060],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.0123,  0.4762, -0.3228,  ...,  0.5592, -0.1582, -0.0060],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('image.n.01.image') [tensor([ 0.1778,  0.0150, -0.3423,  ..., -0.0235,  0.1938,  0.2806],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.1778,  0.0150, -0.3423,  ..., -0.0235,  0.1938,  0.2806],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "man.n.00 [tensor([ 0.0329, -0.1469, -0.2107,  ..., -0.1619,  0.1634,  0.2565],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.0329, -0.1469, -0.2107,  ..., -0.1619,  0.1634,  0.2565],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('suffer.v.01.suffer') [tensor([ 0.1975, -0.4997,  0.2139,  ...,  0.0456,  0.3949,  0.5821],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.1975, -0.4997,  0.2139,  ...,  0.0456,  0.3949,  0.5821],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('complete.a.01.complete') [tensor([-0.1003,  0.0361,  0.1431,  ...,  0.1580,  0.0887,  0.3194],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.0482,  0.0245,  0.1787,  ...,  0.1613,  0.0784,  0.3218],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.1003,  0.0361,  0.1431,  ...,  0.1580,  0.0887,  0.3194],\n",
      "       grad_fn=<SelectBackward>)], [tensor([-0.0482,  0.0245,  0.1787,  ...,  0.1613,  0.0784,  0.3218],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('atomization.n.01.fragmentation') [tensor([-0.2738,  0.4035,  0.4432,  ...,  0.4382,  0.5479,  0.2085],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.2738,  0.4035,  0.4432,  ...,  0.4382,  0.5479,  0.2085],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('personal.a.01.personal') [tensor([-0.1965,  0.4162, -0.3043,  ...,  0.3102, -0.1840,  0.1221],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.1965,  0.4162, -0.3043,  ...,  0.3102, -0.1840,  0.1221],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('spiritual.s.03.spiritual') [tensor([-0.0906, -0.2441,  0.1166,  ...,  0.0344, -0.1006,  0.4008],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.0912, -0.2444,  0.1166,  ...,  0.0344, -0.1006,  0.4008],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('quality.n.01.quality') [tensor([0.2425, 1.0852, 0.5881,  ..., 0.3423, 1.1718, 0.2879],\n",
      "       grad_fn=<SelectBackward>), [tensor([0.2420, 1.0853, 0.5880,  ..., 0.3423, 1.1718, 0.2879],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('objectification.n.01.objectification') [tensor([-0.5808, -0.1430,  0.5990,  ...,  0.4968,  0.6537,  0.3837],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.5811, -0.1446,  0.5999,  ...,  0.4968,  0.6537,  0.3837],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('subhuman.a.01.subhuman') [tensor([-0.1944, -0.3898,  0.0506,  ..., -0.1709,  0.1139,  0.1347],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.1941, -0.3897,  0.0494,  ..., -0.1709,  0.1139,  0.1347],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('quasi.s.01.quasi') [tensor([ 0.0501, -0.4217,  0.6160,  ...,  0.2692,  0.3081,  0.2127],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.0477, -0.4235,  0.6159,  ...,  0.2692,  0.3081,  0.2127],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('mechanistic.s.01.mechanistic') [tensor([-0.1623, -0.3453, -0.1597,  ...,  0.1715, -0.0280,  0.1692],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.1675, -0.3462, -0.1621,  ...,  0.1715, -0.0280,  0.1692],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('ability.n.02.power') [tensor([-0.3468,  0.2858,  0.1933,  ...,  0.4822,  0.5101,  0.1613],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.3498,  0.2780,  0.1874,  ...,  0.4822,  0.5101,  0.1613],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('workplace.n.01.work') [tensor([ 0.3527, -0.4869,  0.2588,  ..., -0.2401,  0.4835,  0.0448],\n",
      "       grad_fn=<SelectBackward>), tensor([ 0.2407, -0.0210,  0.0306,  ...,  0.4846,  0.2198,  0.4470],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.3527, -0.4869,  0.2588,  ..., -0.2401,  0.4835,  0.0448],\n",
      "       grad_fn=<SelectBackward>), tensor([ 0.2407, -0.0210,  0.0306,  ...,  0.4846,  0.2198,  0.4470],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('today.r.02.today') [tensor([ 0.7066,  0.0631,  0.1058,  ...,  0.1726, -0.2602,  0.0358],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.7066,  0.0631,  0.1058,  ...,  0.1726, -0.2602,  0.0358],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('sneak.v.01.sneak') [tensor([-0.4739,  0.3467, -0.1230,  ...,  0.0771, -0.0635,  0.4038],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.4739,  0.3467, -0.1230,  ...,  0.0771, -0.0635,  0.4038],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('about.r.04.around') [tensor([ 0.2992, -0.3000,  0.3436,  ...,  0.4973,  0.2801, -0.1083],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.2992, -0.3000,  0.3436,  ...,  0.4973,  0.2801, -0.1083],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('spy.v.02.spy') [tensor([ 0.3932, -0.0730,  0.0797,  ...,  0.0187, -0.2635,  0.5169],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.3932, -0.0730,  0.0797,  ...,  0.0187, -0.2635,  0.5169],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('house.n.01.house') [tensor([-0.0952,  0.3911,  0.3885,  ...,  0.2383, -0.0806,  0.0096],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.0952,  0.3911,  0.3885,  ...,  0.2383, -0.0806,  0.0096],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('try.v.01.attempt') [tensor([ 0.1854, -0.5285, -0.0869,  ...,  0.3340,  0.0269,  0.3432],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.1854, -0.5285, -0.0869,  ...,  0.3340,  0.0269,  0.3432],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('add.v.01.add') [tensor([ 0.6843,  0.0612,  0.1353,  ...,  0.1726, -0.2817, -0.2291],\n",
      "       grad_fn=<SelectBackward>), tensor([-0.1465,  0.8424,  0.1881,  ...,  0.1022, -0.3781,  0.4454],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.6843,  0.0612,  0.1353,  ...,  0.1726, -0.2817, -0.2291],\n",
      "       grad_fn=<SelectBackward>), tensor([-0.1465,  0.8424,  0.1881,  ...,  0.1022, -0.3781,  0.4454],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('include.v.01.include') [tensor([ 0.5794,  0.5820, -0.4239,  ...,  0.3790, -0.4270,  0.2023],\n",
      "       grad_fn=<SelectBackward>), [tensor([ 0.5794,  0.5820, -0.4239,  ...,  0.3790, -0.4270,  0.2023],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('notch.v.01.notch') [tensor([-0.1132, -0.1006, -0.4756,  ...,  0.4190,  0.1216,  0.1098],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.1132, -0.1006, -0.4756,  ...,  0.4190,  0.1216,  0.1098],\n",
      "       grad_fn=<SelectBackward>)]]\n",
      "Lemma('fasten.v.01.secure') [tensor([-0.3726, -0.0510, -0.0943,  ..., -0.0594,  0.0416, -0.2374],\n",
      "       grad_fn=<SelectBackward>), [tensor([-0.3726, -0.0510, -0.0943,  ..., -0.0594,  0.0416, -0.2374],\n",
      "       grad_fn=<SelectBackward>)]]\n"
     ]
    }
   ],
   "source": [
    "# check dict\n",
    "\n",
    "print(\"Keys Train_counter:\\n\")\n",
    "for sense, count in Train_counter.items():\n",
    "    print(sense, count)\n",
    "print(\"\\n\")\n",
    "print(\"Keys Train_embeddings:\\n\")\n",
    "for sense, embeddings in Train_embeddings.items():\n",
    "    print(sense, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5. Testing the sense vectors (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your sense embeddings on your test data, which is a subset of the SemCor corpus. Use the strategy outlined above, with 1st WordNet sense as a fallback: \n",
    "\n",
    "- rely on mean sense vectors for each word sense in the training partition of the corpus\n",
    "- for each sense-annotated token <i>t</i> (e.g. the verb \"run\") in the test partition of the corpus, assign it to the sense of the word \"Lemma(*.v*.run)\" to which ithe ELMo vector <i>t</i> is the closest according to the cosine distance metric\n",
    "- as a backup strategy, use the 1st sense of the word (e.g. <code>Lemma(run.v.01.run)</code>) by default.\n",
    "\n",
    "Report WSD accuracy in percentage points on your test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end\n",
    "Congratulations! this is the end of Lab 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acknowledgements** Tejaswini Deoskar has given valuable comments that helped improve this lab assignment. Timothee Mickus helped to test this assignment and gave extensive feedback on the instructions. Many thanks to both."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
